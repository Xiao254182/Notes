#### kubeadm搭建k8s高可用集群(三主两从一VIP)

架构方案: 采用三主两从一 VIP 的 K8S 高可用集群架构，通过多个主节点实现控制平面的高可用性，从节点负责承载工作负载。VIP（虚拟 IP）用于提供统一的访问入口，实现主节点的故障切换。   

适用场景: 适用于对业务连续性要求较高，负载较大，需要保证系统稳定运行的中大型企业级应用场景。   

优势:1. 提高系统的可用性：多个主节点避免了单点故障，当一个主节点出现问题时，其他主节点可以继续提供服务，确保集群的正常运行。

​        2.增强负载能力：能够更好地应对大规模的工作负载，通过合理分配资源，提高系统的整体性能。

​        3.便于扩展：可以方便地添加更多的从节点来扩展集群的计算能力。

​        4.提供更好的容错性：即使部分节点出现故障，也能保证业务的持续运行，减少故障对业务的影响。

节点配置

|   高可用集群    |    主机名    |       IP       |            所需的服务             |
| :-------------: | :----------: | :------------: | :-------------------------------: |
|       主        | k8s-master-1 | 192.168.100.31 | kubeadm,docker,keepalived,haproxy |
|       主        | k8s-master-2 | 192.168.100.32 | kubeadm,docker,keepalived,haproxy |
|       主        | k8s-master-3 | 192.168.100.33 | kubeadm,docker,keepalived,haproxy |
|       从        |  k8s-node-1  | 192.168.100.34 |          kubeadm,docker           |
|       从        |  k8s-node-2  | 192.168.100.35 |          kubeadm,docker           |
| VIP（可有可无） |   k8s-vip    | 192.168.100.30 |                无                 |

一：准备工作

1.所有节点关闭防火墙

```
systemctl stop firewalld && systemctl disable firewalld
```

PS:线上服务器之类的不能关闭防火墙，就按照以下配置开启对应的端口（这里说的是k8所用到的端口）

master节点：

| 规则 | 端口范围  | 作用                    | 使用者               |
| ---- | --------- | ----------------------- | -------------------- |
| TCP  | 6443*     | Kubernetes API server   | All                  |
| TCP  | 2379-2380 | etcd server client API  | kube-apiserver, etcd |
| TCP  | 10250     | Kubelet API             | Self, Control plane  |
| TCP  | 10251     | kube-scheduler          | Self                 |
| TCP  | 10252     | kube-controller-manager | Self                 |

node节点：

| 规则 | 端口范围    | 作用               | 使用者              |
| ---- | ----------- | ------------------ | ------------------- |
| TCP  | 10252       | Kubelet API        | Self, Control plane |
| TCP  | 30000-32767 | NodePort Services* | All                 |

2.所有节点关闭selinux

```shell
#临时关闭selinux
setenforce 0

#永久关闭selinux
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
```

3.所有节点关闭交换分区

```shell
#临时关闭所有的交换分区
swapoff -a

#永久关闭所有的交换分区
sed -i '/swap/s/^\(.*\)$/#\1/g' /etc/fstab
```

4.修改六台高可用集群的主机名

![](.\13\1.png)

5.所有节点都添加集群ip与主机名到hosts中

```
cat >> /etc/hosts << EOF 
192.168.100.31 k8s-master-1
192.168.100.32 k8s-master-2
192.168.100.33 k8s-master-3
192.168.100.34 k8s-node-1
192.168.100.35 k8s-node-2
192.168.100.30 k8s-vip
EOF
```

6.所有节点进行时间同步

```shell
#安裝同步时间命令
yum install ntpdate -y

#同步时间
ntpdate cn.pool.ntp.org

#设置定时任务每五分钟同步一次时间
echo "*/5 * * * * root /usr/sbin/ntpdate cn.pool.ntp.org &>/dev/null" >> /etc/crontab
```

PS:如果是克隆虚拟机建议执行rm -rf /etc/udev/* 保证网卡UUID不同

7.所有节点安装需要的一些命令

- 添加centos源并将下载地址更换为阿里云地址

```shell
#添加centos源
curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo

#将下载地址更换为阿里云地址
sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo
```

- 添加epel扩展源

```
curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
```

- 清除缓存并重新加载源缓存

```
yum clean all && yum makecache
```

- 升级yum并安装一些会用到的命令

```
yum -y update && yum -y install lrzsz wget conntrack ipvsadm ipset jq psmisc sysstat curl iptables net-tools libseccomp gcc gcc-c++ yum-utils device-mapper-persistent-data lvm2 bash-completion sshpass git
```

8.所有节点调整能打开文件数大小

```shell
#将当前用户会话中允许打开的文件描述符的数量增加到 65535，提高文件句柄限制
ulimit -SHn 65535

#确保系统能够处理较高的负载而不会因为资源限制而导致错误
cat >> /etc/security/limits.conf << EOF 
* soft nofile 655360
* hard nofile 131072
* soft nproc 65535
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
EOF
```

9.所有节点安装配置ipvsadm

```shell
#加载ipvs相关模块
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4

#指定了在系统启动时需要加载的内核模块,这些配置可以启用和优化 Linux 系统的负载均衡及网络功能，尤其是在构建高可用性和高性能的网络服务环境
cat > /etc/modules-load.d/ipvs.conf << EOF
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack_ipv4
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
EOF

#确保系统能够在启动时自动加载配置的内核模块并立即应用这些配置
systemctl enable --now systemd-modules-load.service
```

10.高可用集群设置免密登录（在k8s-master-1节点上操作）

```shell
#生成密钥
[root@k8s-master-1 ~]# ssh-keygen -t rsa -P '' -f /root/.ssh/id_rsa &> /dev/null

#循环给高可用集群进行免密设置(只有当所有主机密码均一致时才能使用)
[root@k8s-master-1 ~]# for i in k8s-master-1 k8s-master-2 k8s-master-3 k8s-node-1 k8s-node-2;do sshpass -p '000000' ssh-copy-id -o StrictHostKeyChecking='no' -i .ssh/id_rsa.pub $i;done
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: ".ssh/id_rsa.pub"
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'StrictHostKeyChecking=no' 'k8s-master-1'"
and check to make sure that only the key(s) you wanted were added.

/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: ".ssh/id_rsa.pub"
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'StrictHostKeyChecking=no' 'k8s-master-2'"
and check to make sure that only the key(s) you wanted were added.

/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: ".ssh/id_rsa.pub"
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'StrictHostKeyChecking=no' 'k8s-master-3'"
and check to make sure that only the key(s) you wanted were added.

/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: ".ssh/id_rsa.pub"
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'StrictHostKeyChecking=no' 'k8s-node-1'"
and check to make sure that only the key(s) you wanted were added.

/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: ".ssh/id_rsa.pub"
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'StrictHostKeyChecking=no' 'k8s-node-2'"
and check to make sure that only the key(s) you wanted were added.
```

二：所有节点部署 docker

```shell
#添加阿里云的docker镜像地址
[root@docker ~]# sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

#更新缓存，只处理新添加的yum源缓存
[root@docker ~]# yum makecache fast

#部署docker，默认安装最新版本
[root@docker ~]# yum install -y docker-ce-20.10.14 docker-ce-cli-20.10.14 containerd.io

#查看安装docker版本
[root@docker ~]# docker --version(或者使用docker version)
Docker version 20.10.14, build a224086

#加载docker配置,启动docker服务并设置开机自启
[root@docker ~]# systemctl daemon-reload && systemctl start docker && systemctl enable docker
```

三：所有节点部署 kubernetes

```shell
#将桥接的IPv4 流量传递到iptables 的链
cat <<EOF >> /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF

#让其生效
sysctl --system
```

添加 k8s yum源

```shell
cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

#重新加载缓存
yum makecache fast
```

安装 kubeadm,kubelet,kubectl

```shell
yum install -y kubeadm-1.20.0-0 kubelet-1.20.0-0 kubectl-1.20.0-0

#查看kubeadm版本
kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.0", GitCommit:"af46c47ce925f4c4ad5cc8d1fca46c7b77d13b38", GitTreeState:"clean", BuildDate:"2020-12-08T17:57:36Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"linux/amd64"}

#启动kubelet并设置开机自启
systemctl enable kubelet && systemctl start kubelet
```

kubernetes强化tab（安装之后会tab可以补全命令及参数）

```shell
echo 'source  <(kubectl  completion  bash)' >> ~/.bashrc
#执行完上述命令后执行bash命令更新环境
```

四：在所有主节点(k8s-master-1,k8s-master-2,k8s-master-3)部署keepalived和haproxy来配置高可用

1.安装 keepalived 和 haproxy

```
yum -y install keepalived haproxy
```

2.所有主节点修改haproxy配置文件（/etc/haproxy/haproxy.cfg）

```shell
global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

frontend k8s-master
  bind 0.0.0.0:16443
  bind 127.0.0.1:16443
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-master

backend k8s-master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-master-1	192.168.100.31:6443  check
  server k8s-master-2	192.168.100.32:6443  check
  server k8s-master-3	192.168.100.33:6443  check
```

3.所有主节点修改keepalived配置文件（/etc/keepalived/keepalived.conf ）

- k8s-master-1节点配置

```shell
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -5
    fall 2  
    rise 1
}
vrrp_instance VI_1 {
    state MASTER					#高可用主1
    interface eth0					#网卡名称
    mcast_src_ip 172.16.11.215		#该节点 IP
    virtual_router_id 51
    priority 101				#设置最高级优先级
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    }
    virtual_ipaddress {
        172.16.11.220			#vip地址
    }
    track_script {
       chk_apiserver
    }
}
```

- k8s-master2节点配置

```shell
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -5
    fall 2  
    rise 1
}
vrrp_instance VI_1 {
    state BACKUP					#高可用从1
    interface eth0					#网卡名称
    mcast_src_ip 172.16.11.216		#该节点 IP
    virtual_router_id 51
    priority 100				#设置优先级
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    }
    virtual_ipaddress {
        172.16.11.220			#vip地址
    }
    track_script {
       chk_apiserver
    }
}
```

- k8s-master3节点配置

```shell
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -5
    fall 2  
    rise 1
}
vrrp_instance VI_1 {
    state BACKUP					#高可用从2
    interface ens33					#网卡名称
    mcast_src_ip 172.16.11.217		#该节点 IP
    virtual_router_id 51
    priority 99				#设置优先级
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    }
    virtual_ipaddress {
        172.16.11.220			#vip地址
    }
    track_script {
       chk_apiserver
    }
}
```

4.所有主节点编写健康检测脚本（/etc/keepalived/check_apiserver.sh ）

```shell
#!/bin/bash
err=0
for k in $(seq 1 3);do
    check_code=$(pgrep haproxy)
    if [[ $check_code == "" ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != "0" ]]; then
    echo "systemctl stop keepalived"
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi

#给监测脚本添加执行权限
chmod +x /etc/keepalived/check_apiserver.sh
```

5.所有主节点启动 keepalived和haproxy

```shell
#生效配置文件
systemctl daemon-reload

#启动并设置开机自启haproxy
systemctl enable --now haproxy

#启动并设置开机自启keepalived
systemctl enable --now keepalived
```

3.k8s-master1主节点查看VIP

```shell
#查看IP与vip的IP
[root@k8s-master1 ~]# hostname -I
192.168.100.31 192.168.100.30 172.17.0.1

#测试vip的16443端口是否通
[root@k8s-master1 ~]# telnet 192.168.100.30 16443
Trying 192.168.100.30...
Connected to 192.168.100.30.
Escape character is '^]'.
Connection closed by foreign host.
```

五：部署K8S集群

1.在k8s-master-1节点创建kubeadm-config.yaml配置文件

```yaml
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: 7t2weq.bjbawausm0jaxury  #初始化集群使用的token
  ttl: 24h0m0s    #token有效期
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.100.31    #k8s-master-1节点IP
  bindPort: 6443
nodeRegistration:   #集群节点的信息
  criSocket: /var/run/dockershim.sock
  name: k8s-master-1    #k8s-master-1节点的名称
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  certSANs:
  - 192.168.100.30    #vip地址
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: 192.168.100.30:16443  #连接apiserver的地址
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.20.0  #与kubernets版本对应 
networking:
  dnsDomain: cluster.local
  podSubnet: 192.168.5.1/18
  serviceSubnet: 172.17.0.1/16  #pod,service与宿主机都不在同一个网段
scheduler: {}
```

2.更新配置文件

```shell
[root@k8s-master-1 ~]# kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml
```

3.将new.yaml文件复制到其他master节点

```shell
[root@k8s-master-1 ~]# scp new.yaml 192.168.100.32:/root
new.yaml                                         100%  983   188.4KB/s   00:00
[root@k8s-master-1 ~]# scp new.yaml 192.168.100.33:/root
new.yaml                                         100%  983   407.5KB/s   00:00
```

4.查看需要的镜像文件，并在所有master节点提前下载镜像，节省初始化时间

```shell
#在所有节点执行，此处以k8s-master-1节点做演示
[root@k8s-master-1 ~]# kubeadm config images list --config /root/new.yaml
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.20.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.20.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.20.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.20.0
registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.4.13-0
registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.7.0

#在所有节点执行，此处以k8s-master-1节点做演示
[root@k8s-master-1 ~]# kubeadm config images pull --config /root/new.yaml

#在所有节点执行，此处以k8s-master-1节点做演示
[root@k8s-master-1 ~]# docker images
REPOSITORY                                                                    TAG        IMAGE ID       CREATED       SIZE
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy                v1.20.0    10cc881966cf   3 years ago   118MB
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager   v1.20.0    b9fa1895dcaa   3 years ago   116MB
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler            v1.20.0    3138b6e3d471   3 years ago   46.4MB
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver            v1.20.0    ca9843d3b545   3 years ago   122MB
registry.cn-hangzhou.aliyuncs.com/google_containers/etcd                      3.4.13-0   0369cf4303ff   3 years ago   253MB
registry.cn-hangzhou.aliyuncs.com/google_containers/coredns                   1.7.0      bfe3a36ebd25   4 years ago   45.2MB
registry.cn-hangzhou.aliyuncs.com/google_containers/pause                     3.2        80d28bedfe5d   4 years ago   683kB
```

5.在k8s-master-1节点初始化，初始化后生成对应的证书

```shell
[root@k8s-master-1 ~]# kubeadm init --config /root/new.yaml  --upload-certs
```

出现下图样式表示生成成功

![](.\13\2.png)

6.在k8s-master-1节点配置环境变量，用于访问Kubernetes集群

```shell
[root@k8s-master-1 ~]# cat <<EOF >> /root/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf
EOF

#让其生效
[root@k8s-master-1 ~]# source /root/.bashrc
```

7.在 k8s-master-1节点查看集群节点状态

```shell
[root@k8s-master-1 ~]# kubectl get nodes
NAME           STATUS     ROLES                  AGE    VERSION
k8s-master-1   NotReady   control-plane,master   104s   v1.20.0

#采用初始化安装方式，所有的系统组件均以容器的方式运行并且在kube-system命名空间内，此时可以查看Pod状态：
[root@k8s-master-1 ~]# kubectl get pods -n kube-system -o wide
NAME                                   READY   STATUS    RESTARTS   AGE    IP               NODE           NOMINATED NODE   READINESS GATES
coredns-54d67798b7-4kbfn               0/1     Pending   0          106s   <none>           <none>         <none>           <none>
coredns-54d67798b7-vsb8d               0/1     Pending   0          106s   <none>           <none>         <none>           <none>
etcd-k8s-master-1                      1/1     Running   0          114s   192.168.100.31   k8s-master-1   <none>           <none>
kube-apiserver-k8s-master-1            1/1     Running   0          113s   192.168.100.31   k8s-master-1   <none>           <none>
kube-controller-manager-k8s-master-1   1/1     Running   0          113s   192.168.100.31   k8s-master-1   <none>           <none>
kube-proxy-tlrzg                       1/1     Running   0          106s   192.168.100.31   k8s-master-1   <none>           <none>
kube-scheduler-k8s-master-1            1/1     Running   0          113s   192.168.100.31   k8s-master-1   <none>           <none>
```

8.其他master节点加入集群（即在k8s-master-2和k8s-master-3节点操作）

```shell
#该命令为上述初始化成功后生成的master加入集群命令，即上图第一个框中的命令
kubeadm join 192.168.100.30:16443 --token 7t2weq.bjbawausm0jaxury \
     --discovery-token-ca-cert-hash sha256:9964272470701e4dfff29fe33e29ed59dcf2948e4a4a2e44448b86e708982416 \
     --control-plane --certificate-key 748d76848855816ff973fa28dc587057ffd2d0be63f7e6bd88305d07c81770e7
```

出现下图样式表示其他master节点加入集群成功

![](.\13\3.png)

9.Node节点配置（即在k8s-node-1和k8s-node-2节点操作）

```shell
#该命令为上述初始化成功后生成的node加入集群命令，即上图第二个框中的命令
kubeadm join 192.168.100.30:16443 --token 7t2weq.bjbawausm0jaxury \
     --discovery-token-ca-cert-hash sha256:9964272470701e4dfff29fe33e29ed59dcf2948e4a4a2e44448b86e708982416
```

出现下图样式表示node节点加入集群成功

![](.\13\4.png)

10.在k8s-master-1上查看集群中的所有节点

```shell
[root@k8s-master-1 ~]# kubectl get nodes
NAME           STATUS     ROLES                  AGE     VERSION
k8s-master-1   NotReady   control-plane,master   4m53s   v1.20.0
k8s-master-2   NotReady   control-plane,master   2m31s   v1.20.0
k8s-master-3   NotReady   control-plane,master   104s    v1.20.0
k8s-node-1     NotReady   <none>                 47s     v1.20.0
k8s-node-2     NotReady   <none>                 43s     v1.20.0
```

现在的集群状态都是NotReady表示不可达；这是因为还没有安装网络插件，下面我们来安装一下网络插件（caclico）

六：部署Calico网络插件，用于连接其他节点（只在k8s-master-1节点 操作)

1.下载地址

```shell
[root@k8s-master-1 ~]# curl https://docs.projectcalico.org/archive/v3.21/manifests/calico.yaml -O
```

2.yaml文件基于集群安装

国内集群，建议修改yaml文件对应替换成下列表格中的镜像

| 原始镜像地址                                | 国内镜像地址                                                 |
| ------------------------------------------- | ------------------------------------------------------------ |
| docker.io/calico/cni:v3.21.6                | swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/cni:v3.22.5 |
| docker.io/calico/pod2daemon-flexvol:v3.21.6 | swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/pod2daemon-flexvol:v3.22.5 |
| docker.io/calico/node:v3.21.6               | swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/node:v3.22.5 |
| docker.io/calico/kube-controllers:v3.21.6   | swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/kube-controllers:v3.22.5 |

（镜像来源地址：https://docker.aityp.com/i/search?search=calico）

```shell
[root@k8s-master-1 ~]# kubectl apply -f calico.yaml
```

（PS:calico内置IP为192.168.0.0/16，此IP作为所有pod的分配网段使用，在初始化时如果和本网段相同，要修改为其他网段）

3.查看创建的容器的状态

```shell
[root@k8s-master-1 ~]# kubectl get pod -n kube-system
NAME                                      READY   STATUS              RESTARTS   AGE
calico-kube-controllers-c7bbb5848-drx9w   1/1     Running             0          7m4s
calico-node-dvwzr                         1/1     Running             0          7m3s
calico-node-jb5kj                         1/1     Running             0          7m3s
calico-node-nmm8w                         1/1     Running             0          7m3s
calico-node-ptm6t                         1/1     Running             3          7m3s
calico-node-x2fms                         1/1     Running             0          7m3s
coredns-54d67798b7-4kbfn                  1/1     Running             0          4h16m
coredns-54d67798b7-vsb8d                  1/1     Running             0          4h16m
etcd-k8s-master-1                         1/1     Running             8          4h16m
etcd-k8s-master-2                         1/1     Running             0          4h14m
etcd-k8s-master-3                         1/1     Running             0          4h11m
kube-apiserver-k8s-master-1               1/1     Running             6          4h16m
kube-apiserver-k8s-master-2               1/1     Running             0          4h14m
kube-apiserver-k8s-master-3               1/1     Running             2          4h13m
kube-controller-manager-k8s-master-1      1/1     Running             6          4h16m
kube-controller-manager-k8s-master-2      1/1     Running             1          4h14m
kube-controller-manager-k8s-master-3      1/1     Running             0          4h12m
kube-proxy-gkql2                          1/1     Running             0          4h12m
kube-proxy-gsdbr                          1/1     Running             0          4h14m
kube-proxy-mdz67                          1/1     Running             0          4h13m
kube-proxy-tlrzg                          1/1     Running             5          4h16m
kube-proxy-tsxw7                          1/1     Running             0          30s
kube-scheduler-k8s-master-1               1/1     Running             7          4h16m
kube-scheduler-k8s-master-2               1/1     Running             1          4h14m
kube-scheduler-k8s-master-3               1/1     Running             0          4h12m
```

4.查看节点网络连接状态

```shell
[root@k8s-master-1 ~]# kubectl get nodes
NAME           STATUS   ROLES                  AGE     VERSION
k8s-master-1   Ready    control-plane,master   4h18m   v1.20.0
k8s-master-2   Ready    control-plane,master   4h16m   v1.20.0
k8s-master-3   Ready    control-plane,master   4h15m   v1.20.0
k8s-node-1     Ready    <none>                 4h14m   v1.20.0
k8s-node-2     Ready    <none>                 4h14m   v1.20.0
```

5.将主节点（k8s-master-1）中的`/etc/kubernetes/admin.conf`文件拷贝到其他所有节点的相同目录下

```shell
[root@k8s-master-1 ~]# scp /etc/kubernetes/admin.conf 192.168.100.32:/etc/kubernetes/admin.conf
[root@k8s-master-1 ~]# scp /etc/kubernetes/admin.conf 192.168.100.33:/etc/kubernetes/admin.conf
[root@k8s-master-1 ~]# scp /etc/kubernetes/admin.conf 192.168.100.34:/etc/kubernetes/admin.conf
[root@k8s-master-1 ~]# scp /etc/kubernetes/admin.conf 192.168.100.35:/etc/kubernetes/admin.conf
```

6.在其他所有节点配置环境变量并立即生效

```shell
echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> ~/.bash_profile

source ~/.bash_profile
```

七：Metrics部署用于查看其他节点的资源使用率（只在k8s-master-1节点 操作)

1.下载 metrics-server 官方的部署 yaml:

```shell
[root@k8s-master-1 ~]# wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.0/components.yaml
```

2.修改 metrics-server 启动参数

metrics-server 会请求每台节点的 kubelet 接口来获取监控数据，接口通过 HTTPS 暴露，但 Kubernetes 节点的 kubelet 使用的是自签证书，若 metrics-server 直接请求 kubelet 接口，将产生证书校验失败的错误，因此需要在 components.yaml 文件中加上 --kubelet-insecure-tls 启动参数

```yaml
containers:
- args:
  - --cert-dir=/tmp
  - --secure-port=443
  - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
  - --kubelet-use-node-status-port
  - --metric-resolution=15s
  - --kubelet-insecure-tls # 加上该启动参数
  image: ccr.ccs.tencentyun.com/mirrors/metrics-server:v0.5.0 # 国内集群，请替换成这个镜像
```

![](.\13\5.png)

3.部署到集群

```shell
[root@k8s-master-1 ~]# kubectl apply -f components.yaml
```

4.检查运行状态

```shell
[root@k8s-master-1 ~]# kubectl get pod -n kube-system | grep metrics-server
metrics-server-5f485465c7-cm442           1/1     Running            0          3m16s
```

5.查看节点占用性能情况

```shell
[root@k8s-master-1 ~]# kubectl top node
NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
k8s-master-1   226m         5%     1792Mi          48%
k8s-master-2   238m         5%     2088Mi          56%
k8s-master-3   202m         5%     2065Mi          56%
k8s-node-1     97m          2%     1721Mi          46%
k8s-node-2     36m          0%     1605Mi          43%
```

