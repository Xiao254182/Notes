### 20.利用operator部署Elasticserach集群和kibana

#### 1.部署eck operator

[Elastic Cloud Kubernetes](https://www.elastic.co/guide/en/cloud-on-k8s/current/index.html)（ECK）是Elastic官方推出的，基于k8s operator的插件，其扩展了k8s的基础编排功能，可以轻松地在k8s中安装、管理 Elasticsearch, Kibana 和 APM集群。
要理解ECK，首先需要了解CRD、Controller、Operator这三个基础概念。

什么是CRD?

```
CRD(Custom Resource Definitions)也就是自定义K8S资源类型。当内置的POD、Deployment、Configmap等资源类型不满足需求时，我们就需要扩展k8s，常用方式有三种：
1.使用CRD自定义资源类型
2.开发自定义的APIServer（例如HPA）
3.定制扩展二次开发Kubernetes源码（例如阿里云ACK、腾讯云TKE）

在 Kubernetes中，资源是 Kubernetes API中的一个端点，用于存储一堆特定类型的API对象。它允许我们通过向集群添加更多种类的对象来扩展Kubernetes。添加新种类的对象之后，我们可以像其他任何内置对象一样，使用 kubectl 来访问我们自定义的 API 对象，CRD无须修改Kubernetes源代码就能扩展它支持使用API资源类型。
```

什么是Controller

```
Kubernetes 的所有控制器，都有一个控制循环，负责监控集群中特定资源的更改，并确保特定资源在集群里的当前状态与控制器自身定义的期望状态保持一致。

Controller是需要CRD配套开发的程序，它通过Apiserver监听相应类型的资源对象事件，例如：创建、删除、更新等等，然后做出相应的动作，例如一个 Deployment 控制器管控着集群里的一组 Pod ，当你 Kill 掉一个 Pod 。控制器发现定义中期望的Pod数量与当前的数量不匹配，它就会马上创建一个 Pod 让当前状态与期望状态匹配。
```

什么是Operator

```
Kubernetes Operator 是一种用于自动化 Kubernetes 应用程序管理的设计模式。它利用 Kubernetes 的原生 API 和控制器来封装和管理复杂的应用程序和服务的生命周期。它的主要功能包括:
1.自动化操作 - Operator 可以自动执行诸如部署、扩缩容、升级、备份等常见操作。用户不需要手动一个个 Pod 去执行这些任务。
2.无状态应用管理 - Operator 很适合管理那些无状态的应用,比如数据库、缓存等。它通过控制器模式让应用始终处于预期的状态。
3.减少重复工作 - Operator 让用户从重复的日常工作中解脱出来,这些工作可以交给 Operator 来自动完成。
4.基于 CRD 扩展 - Operator 通过自定义资源定义(CRD)扩展 Kubernetes 的 API,并基于这些 CRD 来实现自定义控制循环。
5.结合服务目录 - Operator 可以与服务目录(如 Service Catalog)集成,来启用基于权限的资源控制和分发。

总的来说,Kubernetes Operator 的核心思想是通过程序化和自动化的方式来管理和扩展 Kubernetes 集群。它极大地简化了在 Kubernetes 上安装和运行复杂应用的过程。

Operator 的工作原理基于 Kubernetes 的控制器模式。它会不断地监测 Kubernetes 集群的状态,一旦发现自定义资源(CR)的实际状态与预期状态不符,Operator 就会执行相应的操作以使其达到预期状态。这种模式使得 Operator 可以实现自我修复和自动恢复的功能。

常见的 Kubernetes Operator 包括 Rook 提供存储解决方案的 Operator,Prometheus Operator 用于监控集群的 Operator,Istio Operator 用于服务网格的 Operator 等。这些 Operator 为 Kubernetes 生态带来了很大的便利。
```

什么是eck

```
Elastic Cloud on Kubernetes（ECK） 是一种 Kubernetes Operator，为了方便我们管理Elastic Stack全家桶中的各种组件，例如 Elasticsearch，Kibana，APM，Beats 等。通过Operator我们可以快速部署一套Elasticsearch集群，并大大简化日常运维工作。
```

ECK功能

```
1.快速部署、管理和监控多个集群
2.快速扩展集群规模和存储空间
3.通过滚动升级完成配置更改
4.使用 TLS 证书保护集群安全
5.设置具有可用性区域感知功能的热-温-冷体系结构
```

##### (1)创建ECK所需CRD

```shell
# kubectl create -f https://download.elastic.co/downloads/eck/2.10.0/crds.yaml
customresourcedefinition.apiextensions.k8s.io/agents.agent.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/apmservers.apm.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/beats.beat.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/elasticmapsservers.maps.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/elasticsearchautoscalers.autoscaling.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/elasticsearches.elasticsearch.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/enterprisesearches.enterprisesearch.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/kibanas.kibana.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/logstashes.logstash.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/stackconfigpolicies.stackconfigpolicy.k8s.elastic.co created
```

##### (2)创建ECK opeartor

```shell
# kubectl apply -f https://download.elastic.co/downloads/eck/2.10.0/operator.yaml
namespace/elastic-system created
serviceaccount/elastic-operator created
secret/elastic-webhook-server-cert created
configmap/elastic-operator created
clusterrole.rbac.authorization.k8s.io/elastic-operator created
clusterrole.rbac.authorization.k8s.io/elastic-operator-view created
clusterrole.rbac.authorization.k8s.io/elastic-operator-edit created
clusterrolebinding.rbac.authorization.k8s.io/elastic-operator created
service/elastic-webhook-server created
statefulset.apps/elastic-operator created
validatingwebhookconfiguration.admissionregistration.k8s.io/elastic-webhook.k8s.elastic.co created
```

##### (3)查看ECK operator

```shell
# kubectl -n elastic-system get pod
NAME                 READY   STATUS    RESTARTS   AGE
elastic-operator-0   1/1     Running   0          2m58s

# kubectl -n elastic-system logs -f statefulset.apps/elastic-operator
# 日志无报错即可
```

#### 2.使用持久卷

本文使用`NFS`实现持久化；生产环境中应该使用分布式存储,如`ceph`

##### (1)安装配置nfs(master节点)

```shell
# yum install -y nfs-utils rpcbind
# mkdir -p /data/nfs && chmod 755 /data/nfs
# echo "/data/nfs  *(rw,sync,no_root_squash)" > /etc/exports
# systemctl start rpcbind.service && systemctl enable rpcbind && systemctl start nfs.service && systemctl enable nfs
```

##### (2)为nfs创建rabc

```yaml
# vim nfs-rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
  - apiGroups: [""]
    resources: ["services", "endpoints"]
    verbs: ["get"]
  - apiGroups: ["extensions"]
    resources: ["podsecuritypolicies"]
    resourceNames: ["nfs-provisioner"]
    verbs: ["use"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-provisioner
     # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-provisioner
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-provisioner
  apiGroup: rbac.authorization.k8s.io

# kubectl apply -f nfs-rbac.yaml
clusterrole.rbac.authorization.k8s.io/nfs-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-provisioner created
```

##### (3)创建nfs provisioner

```yaml
# vim nfs-provisioner.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-provisioner
---
kind: Service
apiVersion: v1
metadata:
  name: nfs-provisioner
  labels:
    app: nfs-provisioner
spec:
  ports:
    - name: nfs
      port: 2049
    - name: nfs-udp
      port: 2049
      protocol: UDP
    - name: nlockmgr
      port: 32803
    - name: nlockmgr-udp
      port: 32803
      protocol: UDP
    - name: mountd
      port: 20048
    - name: mountd-udp
      port: 20048
      protocol: UDP
    - name: rquotad
      port: 875
    - name: rquotad-udp
      port: 875
      protocol: UDP
    - name: rpcbind
      port: 111
    - name: rpcbind-udp
      port: 111
      protocol: UDP
    - name: statd
      port: 662
    - name: statd-udp
      port: 662
      protocol: UDP
  selector:
    app: nfs-provisioner
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nfs-provisioner
spec:
  selector:
    matchLabels:
      app: nfs-provisioner
  replicas: 1
  strategy:
    type: Recreate 
  template:
    metadata:
      labels:
        app: nfs-provisioner
    spec:
      serviceAccount: nfs-provisioner
      containers:
        - name: nfs-provisioner
          image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/sig-storage/nfs-provisioner:v4.0.8
          ports:
            - name: nfs
              containerPort: 2049
            - name: nfs-udp
              containerPort: 2049
              protocol: UDP
            - name: nlockmgr
              containerPort: 32803
            - name: nlockmgr-udp
              containerPort: 32803
              protocol: UDP
            - name: mountd
              containerPort: 20048
            - name: mountd-udp
              containerPort: 20048
              protocol: UDP
            - name: rquotad
              containerPort: 875
            - name: rquotad-udp
              containerPort: 875
              protocol: UDP
            - name: rpcbind
              containerPort: 111
            - name: rpcbind-udp
              containerPort: 111
              protocol: UDP
            - name: statd
              containerPort: 662
            - name: statd-udp
              containerPort: 662
              protocol: UDP
          securityContext:
            capabilities:
              add:
                - DAC_READ_SEARCH
                - SYS_RESOURCE
          args:
            - "-provisioner=sc.cc/nfs"
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: SERVICE_NAME
              value: nfs-provisioner
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          imagePullPolicy: "IfNotPresent"
          volumeMounts:
            - name: export-volume
              mountPath: /export
      volumes:
        - name: export-volume
          hostPath:
            path: /data/nfs
            
# kubectl apply -f nfs-provisioner.yaml
serviceaccount/nfs-provisioner created
service/nfs-provisioner created
deployment.apps/nfs-provisioner created

# kubectl get pods --selector='app=nfs-provisioner'
NAME                               READY   STATUS    RESTARTS   AGE
nfs-provisioner-7cdfdfccfc-7rj5m   1/1     Running   0          22s
```

##### (4)创建StorageClass

```yaml
# vim nfs-class.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: nfs-sc
provisioner: sc.cc/nfs
mountOptions:
  - vers=4.1

# kubectl apply -f nfs-class.yaml
storageclass.storage.k8s.io/nfs-sc created
```

#### 3.部署es集群

```yaml
# vim es-cluster-nfs.yaml
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: quickstart-nfs
spec:
  version: 8.8.0
  nodeSets:
  - name: default
    count: 3
    config:
      node.store.allow_mmap: false
      node.roles: ["master", "data"] 
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
            memory: 4Gi
            cpu: 2
          limits:
            memory: 4Gi
        storageClassName: nfs-sc

# kubectl apply -f es-cluster-nfs.yaml
elasticsearch.elasticsearch.k8s.elastic.co/quickstart-nfs created
```

#### 4.验证es集群并访问

```shell
# kubectl get elasticsearch
NAME             HEALTH   NODES   VERSION   PHASE   AGE
quickstart-nfs   green    3       8.8.0     Ready   28m

# kubectl get service quickstart-nfs-es-http
NAME                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
quickstart-nfs-es-http   ClusterIP   172.17.67.13   <none>        9200/TCP   29m

# PASSWORD=$(kubectl get secret quickstart-nfs-es-elastic-user -o go-template='{{.data.elastic | base64decode}}')
# curl -u "elastic:$PASSWORD" -k "https://$(kubectl get svc|grep es-http|awk '{print $3}'):9200/_cat/health"
1725547893 14:51:33 quickstart-nfs green 3 3 0 0 0 0 0 0 - 100.0%

# curl -u "elastic:$PASSWORD" -k "https://$(kubectl get svc|grep es-http|awk '{print $3}'):9200"
{
  "name" : "quickstart-nfs-es-default-1",
  "cluster_name" : "quickstart-nfs",
  "cluster_uuid" : "UWx2Z9wyRnSd43hotfGpcg",
  "version" : {
    "number" : "8.8.0",
    "build_flavor" : "default",
    "build_type" : "docker",
    "build_hash" : "c01029875a091076ed42cdb3a41c10b1a9a5a20f",
    "build_date" : "2023-05-23T17:16:07.179039820Z",
    "build_snapshot" : false,
    "lucene_version" : "9.6.0",
    "minimum_wire_compatibility_version" : "7.17.0",
    "minimum_index_compatibility_version" : "7.0.0"
  },
  "tagline" : "You Know, for Search"
}
```

#### 5.部署kibana

```yaml
# vim kibana.yaml
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: quickstart
spec:
  version: 8.8.0
  count: 1
  elasticsearchRef:
    name: quickstart-nfs
  http:
    tls:
      selfSignedCertificate:
        disabled: true
    
# kubectl apply -f kibana.yaml
kibana.kibana.k8s.elastic.co/quickstart created
  
# kubectl get kibana
NAME         HEALTH   NODES   VERSION   AGE
quickstart   green    1       8.8.0     10m
  
# kubectl get pod --selector='kibana.k8s.elastic.co/name=quickstart'
NAME                            READY   STATUS    RESTARTS   AGE
quickstart-kb-f6c748d8d-5cp55   1/1     Running   0          69s 
```

#### 6.允许外部访问

```shell
# kubectl patch svc quickstart-kb-http -p '{"spec": {"type": "NodePort"}}'
service/quickstart-kb-http patched

# kubectl get svc | grep kb
quickstart-kb-http     NodePort    172.17.103.3    <none>        5601:31584/TCP     5m17s
```

#### 7.获取Kibana登陆密码并访问ui界面

```shell
# kubectl get secret quickstart-nfs-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode; echo
Rt96zm3O4695IAL9hoJn3v9f
```

用户名: elastic
密码: Rt96zm3O4695IAL9hoJn3v9f

![](https://github.com/Xiao254182/notes/blob/master/img/20/2.png)

![](https://github.com/Xiao254182/notes/blob/master/img/20/1.png)
