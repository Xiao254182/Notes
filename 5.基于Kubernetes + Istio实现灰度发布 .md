#### 5.基于Kubernetes + Istio实现灰度发布 

节点规划

|       IP       |   主机名   |    节点    |
| :------------: | :--------: | :--------: |
| 192.168.100.10 | k8s-master | master节点 |

一：基础环境准备

1.查看k8s集群状态

```shell
[root@k8s-master ~]# kubectl cluster-info
Kubernetes control plane is running at https://apiserver.cluster.local:6443
CoreDNS is running at https://apiserver.cluster.local:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

2.部署istio

```shell
[root@k8s-master ~]# wget https://github.com/istio/istio/releases/download/1.12.0/istio-1.12.0-linux-amd64.tar.gz
[root@k8s-master ~]# tar -zxvf istio-1.12.0-linux-amd64.tar.gz
[root@k8s-master ~]# mv istio-1.12.0/bin/istioctl /usr/bin/
```

安装Istio

```shell
[root@k8s-master ~]# istioctl install --set profile=demo -y
✔ Istio core installed
✔ Istiod installed
✔ Egress gateways installed
✔ Ingress gateways installed
✔ Installation complete                                                                            Making this installation the default for injection and validation.

Thank you for installing Istio 1.12.  Please take a few minutes to tell us about your install/upgrade experience!  https://forms.gle/FegQbc9UvePd4Z9z7
```

给命名空间添加标签，指示 Istio 在部署应用的时候，自动注入 Envoy 边车代理：

```shell
[root@k8s-master ~]# kubectl label namespace default istio-injection=enabled
namespace/default labeled
```

二：部署Bookinfo应用

1.部署应用程序

部署Bookinfo应用到Kubernetes集群：

```shell
[root@k8s-master ~]# mkdir bookinfo && cd bookinfo/
[root@k8s-master bookinfo]# vim bookinfo.yaml
# Details service
apiVersion: v1
kind: Service
metadata:
  name: details
  labels:
    app: details
    service: details
spec:
  ports:
  - port: 9080
    name: http
  selector:
    app: details
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: bookinfo-details
  labels:
    account: details
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: details-v1
  labels:
    app: details
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: details
      version: v1
  template:
    metadata:
      labels:
        app: details
        version: v1
    spec:
      serviceAccountName: bookinfo-details
      containers:
      - name: details
        image: docker.io/istio/examples-bookinfo-details-v1:1.16.2
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9080
        securityContext:
          runAsUser: 1000
---
# Ratings service
apiVersion: v1
kind: Service
metadata:
  name: ratings
  labels:
    app: ratings
    service: ratings
spec:
  ports:
  - port: 9080
    name: http
  selector:
    app: ratings
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: bookinfo-ratings
  labels:
    account: ratings
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ratings-v1
  labels:
    app: ratings
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ratings
      version: v1
  template:
    metadata:
      labels:
        app: ratings
        version: v1
    spec:
      serviceAccountName: bookinfo-ratings
      containers:
      - name: ratings
        image: docker.io/istio/examples-bookinfo-ratings-v1:1.16.2
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9080
        securityContext:
          runAsUser: 1000
---
# Reviews service
apiVersion: v1
kind: Service
metadata:
  name: reviews
  labels:
    app: reviews
    service: reviews
spec:
  ports:
  - port: 9080
    name: http
  selector:
    app: reviews
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: bookinfo-reviews
  labels:
    account: reviews
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: reviews-v1
  labels:
    app: reviews
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: reviews
      version: v1
  template:
    metadata:
      labels:
        app: reviews
        version: v1
    spec:
      serviceAccountName: bookinfo-reviews
      containers:
      - name: reviews
        image: docker.io/istio/examples-bookinfo-reviews-v1:1.16.2
        imagePullPolicy: IfNotPresent
        env:
        - name: LOG_DIR
          value: "/tmp/logs"
        ports:
        - containerPort: 9080
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: wlp-output
          mountPath: /opt/ibm/wlp/output
        securityContext:
          runAsUser: 1000
      volumes:
      - name: wlp-output
        emptyDir: {}
      - name: tmp
        emptyDir: {}
---
# Productpage services
apiVersion: v1
kind: Service
metadata:
  name: productpage
  labels:
    app: productpage
    service: productpage
spec:
  ports:
  - port: 9080
    name: http
  selector:
    app: productpage
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: bookinfo-productpage
  labels:
    account: productpage
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: productpage-v1
  labels:
    app: productpage
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: productpage
      version: v1
  template:
    metadata:
      labels:
        app: productpage
        version: v1
    spec:
      serviceAccountName: bookinfo-productpage
      containers:
      - name: productpage
        image: docker.io/istio/examples-bookinfo-productpage-v1:1.16.2
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9080
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        securityContext:
          runAsUser: 1000
      volumes:
      - name: tmp
        emptyDir: {}
---
```

(PS:该YAML文件修改自https://raw.githubusercontent.com/istio/istio/release-1.22/samples/bookinfo/platform/kube/bookinfo.yaml，可根据需求自行修改)

查看Pod状态：

```shell
[root@k8s-master bookinfo]# kubectl apply -f bookinfo.yaml
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
[root@k8s-master bookinfo]# kubectl get pod
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-59986f9769-m9qkt       1/1     Running   0          14s
productpage-v1-58c755bb6b-8fxvp   1/1     Running   0          13s
ratings-v1-684c977f7-r4twm        1/1     Running   0          14s
reviews-v1-5bd5f545c8-srhsg       1/1     Running   0          14s
```

2.启用对应用程序的外部访问

现在Bookinfo应用程序已成功运行，需要使应用程序可以从外部访问，可以用Istio Gateway来实现这个目标。

使用网关为网格来管理入站和出站流量，可以指定要进入或流出网格的流量。网关配置被用于运行在网格边界的独立Envoy代理，而不是服务工作负载的sidecar代理。

与Kubernetes Ingress API这种控制进入系统流量的其他机制不同，Istio网关充分利用了流量路由的强大能力和灵活性。Istio的网关资源可以配置4-6层的负载均衡属性，如对外暴露的端口、TLS设置等。作为替代应用层流量路由（L7）到相同的API资源，绑定一个常规的Istio虚拟服务到网关，这样就可以像管理网格中其他数据平面的流量一样去管理网关流量。

网关主要用于管理进入的流量，也可以配置出口网关。出口网关为流出网格的流量配置一个专用的出口节点，这可以限制哪些服务可以或应该访问外部网络，或者启用出口流量安全控制为网格添加安全性。

Gateway配置文件如下：

```shell
[root@k8s-master bookinfo]# vim bookinfo-gateway.yaml
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: bookinfo-gateway
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: bookinfo
spec:
  hosts:
  - "*"
  gateways:
  - bookinfo-gateway
  http:
  - match:
    - uri:
        exact: /productpage
    - uri:
        prefix: /static
    - uri:
        exact: /login
    - uri:
        exact: /logout
    - uri:
        prefix: /api/v1/products
    route:
    - destination:
        host: productpage
        port:
          number: 9080
```

(PS:该YAML文件修改自https://raw.githubusercontent.com/istio/istio/release-1.22/samples/bookinfo/networking/bookinfo-gateway.yaml,可根据需求自行修改)

这个网关指定所有HTTP流量通过80端口流入网格，然后把网关绑定到虚拟服务上。

为应用程序定义Ingress网关：

```shell
[root@k8s-master bookinfo]# kubectl apply -f bookinfo-gateway.yaml
gateway.networking.istio.io/bookinfo-gateway created
virtualservice.networking.istio.io/bookinfo created
```

确认网关创建完成：

```shell
[root@k8s-master bookinfo]# kubectl get gateway
NAME               AGE
bookinfo-gateway   7s
```

查看Ingress Gateway：

```shell
[root@k8s-master bookinfo]# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                     AGE
istio-egressgateway    ClusterIP      10.96.236.80    <none>        80/TCP,443/TCP                                                              21h
istio-ingressgateway   LoadBalancer   10.96.106.171   <pending>     15021:44913/TCP,80:7541/TCP,443:18948/TCP,31400:57642/TCP,15443:51913/TCP   21h 
```

可以看到Gateway 80端口对应的NodePort端口是7541，在浏览器上通过<ip地址>:7541/productpage访问Bookinfo应用，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/1.png)

3.生产测试

使用Curl工具每秒向Bookinfo应用发送1个请求，模拟用户流量：

```shell
[root@k8s-master bookinfo]# vim curl.sh
#!/bin/bash
while true
do
  curl http://192.168.100.10:7541/productpage >/dev/null 2>&1
  sleep 1
done
```

![](https://github.com/Xiao254182/notes/blob/master/img/5/2.png)

后台运行脚本：

```shell
[root@k8s-master bookinfo]# chmod +x curl.sh
[root@k8s-master bookinfo]# bash curl.sh &
[1] 65310
```

三：启用Istio

1.在productpage中启用Istio

在productpage微服务中，启用Istio。这个应用的其他部分会继续照原样运行。可以一个微服务一个微服务的逐步启用Istio。启用Istio在微服务中是无侵入的，不用修改微服务代码或者破坏应用，它也能够持续运行并且为用户请求服务。

在使用Istio控制Bookinfo版本路由之前，需要在目标规则中定义好可用的版本。目标规则是Istio流量路由功能的关键部分。可以将虚拟服务视为将流量如何路由到给定目标地址，然后使用目标规则来配置该目标的流量。在评估虚拟服务路由规则之后，目标规则将应用于流量的“真实”目标地址。

编写目标规则配置文件：

```shell
[root@k8s-master bookinfo]# vim destination-rule-all.yaml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: productpage
spec:
  host: productpage
  subsets:
  - name: v1
    labels:
      version: v1
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: reviews
spec:
  host: reviews
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
  - name: v3
    labels:
      version: v3
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: ratings
spec:
  host: ratings
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
  - name: v2-mysql
    labels:
      version: v2-mysql
  - name: v2-mysql-vm
    labels:
      version: v2-mysql-vm
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: details
spec:
  host: details
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

创建默认目标规则：

```shell
[root@k8s-master bookinfo]# kubectl apply -f destination-rule-all.yaml
destinationrule.networking.istio.io/productpage created
destinationrule.networking.istio.io/reviews created
destinationrule.networking.istio.io/ratings created
destinationrule.networking.istio.io/details created
[root@k8s-master bookinfo]# kubectl get destinationrule
NAME          HOST          AGE
details       details       5s
productpage   productpage   5s
ratings       ratings       5s
reviews       reviews       5s
```

重新部署productpage微服务，启用Istio：

```shell
[root@k8s-master bookinfo]# cat bookinfo.yaml | istioctl kube-inject -f - | kubectl apply -l app=productpage -f -
service/productpage unchanged
deployment.apps/productpage-v1 configured
```

检查productpage的Pod并且查看每个副本的两个容器。第一个容器是微服务本身的，第二个是连接到它的Sidecar代理：

```shell
[root@k8s-master bookinfo]# kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-59986f9769-m9qkt       1/1     Running   0          42m
productpage-v1-798669b7c7-xtbh5   2/2     Running   0          16s
ratings-v1-684c977f7-r4twm        1/1     Running   0          42m
reviews-v1-5bd5f545c8-srhsg       1/1     Running   0          42m
```

Kubernetes采取无侵入的和逐步的滚动更新方式用启用Istio的Pod替换了原有的Pod。Kubernetes只有在新的Pod开始运行的时候才会终止老的Pod，它透明地将流量一个一个地切换到新的Pod上。也就是说，它不会在声明一个新的Pod之前结束一个或者以上的Pod。这些操作都是为了防止破坏应用，因此在注入Istio的过程中应用能够持续工作。

部署Prometheus

```shell
[root@k8s-master bookinfo]# cd ../ && mkdir prometheus && cd prometheus
[root@k8s-master prometheus]# mv /root/istio-1.12.0/samples/addons/prometheus.yaml .
[root@k8s-master prometheus]# kubectl apply -f prometheus.yaml
serviceaccount/prometheus created
configmap/prometheus created
clusterrole.rbac.authorization.k8s.io/prometheus created
clusterrolebinding.rbac.authorization.k8s.io/prometheus created
service/prometheus created
deployment.apps/prometheus created
[root@k8s-master prometheus]# kubectl get deploy -n istio-system
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
istio-egressgateway    1/1     1            1           173m
istio-ingressgateway   1/1     1            1           173m
istiod                 1/1     1            1           173m
prometheus             1/1     1            1           10s
[root@k8s-master prometheus]# kubectl get pod -n istio-system
NAME                                   READY   STATUS    RESTARTS   AGE
istio-egressgateway-7f4864f59c-vhb5z   1/1     Running   0          173m
istio-ingressgateway-55d9fb9f-7q96k    1/1     Running   0          173m
istiod-555d47cb65-7bbbc                1/1     Running   0          173m
prometheus-64fd8ccd65-r28kz            2/2     Running   0          23s
[root@k8s-master prometheus]# kubectl get service -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                     AGE
istio-egressgateway    ClusterIP      10.96.143.20    <none>        80/TCP,443/TCP                                                              175m
istio-ingressgateway   LoadBalancer   10.96.215.98    <pending>     15021:44913/TCP,80:7541/TCP,443:18948/TCP,31400:57642/TCP,15443:51913/TCP   175m
istiod                 ClusterIP      10.96.227.52    <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP                                       175m
prometheus             ClusterIP      10.96.137.156   <none>        9090/TCP                                                                    2m40s
```

可以看到prometheus这个service的类型为ClusterIP，外部环境访问不了 

```shell
[root@k8s-master prometheus]# kubectl edit service prometheus -n istio-system
#修改prometheus这个service的类型为NodePort，这样外部环境就可以访问prometheus了
service/prometheus edited
```

![](https://github.com/Xiao254182/notes/blob/master/img/5/3.png)

![](https://github.com/Xiao254182/notes/blob/master/img/5/4.png)

```shell
[root@k8s-master prometheus]# kubectl get service -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                     AGE
istio-egressgateway    ClusterIP      10.96.143.20    <none>        80/TCP,443/TCP                                                              176m
istio-ingressgateway   LoadBalancer   10.96.215.98    <pending>     15021:44913/TCP,80:7541/TCP,443:18948/TCP,31400:57642/TCP,15443:51913/TCP   176m
istiod                 ClusterIP      10.96.227.52    <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP                                       176m
prometheus             NodePort       10.96.137.156   <none>        9090:29735/TCP                                                              3m14s
```

此时我们在浏览器上登录Prometheus（<ip地址>:port），如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/5.png)

部署Grafana

```shell
[root@k8s-master prometheus]# cd ../ && mkdir grafana && cd grafana
[root@k8s-master grafana]# mv /root/istio-1.12.0/samples/addons/grafana.yaml .
[root@k8s-master grafana]# kubectl apply -f grafana.yaml
serviceaccount/grafana created
configmap/grafana created
service/grafana created
deployment.apps/grafana created
configmap/istio-grafana-dashboards created
configmap/istio-services-grafana-dashboards created
[root@k8s-master grafana]# kubectl get pod -n istio-system
NAME                                   READY   STATUS    RESTARTS   AGE
grafana-6ccd56f4b6-m4bmb               1/1     Running   0          6s
istio-egressgateway-7f4864f59c-vhb5z   1/1     Running   0          3h11m
istio-ingressgateway-55d9fb9f-7q96k    1/1     Running   0          3h11m
istiod-555d47cb65-7bbbc                1/1     Running   0          3h11m
prometheus-64fd8ccd65-r28kz            2/2     Running   0          18m
[root@k8s-master grafana]# kubectl get service -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                     AGE
grafana                ClusterIP      10.96.64.221    <none>        3000/TCP                                                                    2m36s
istio-egressgateway    ClusterIP      10.96.143.20    <none>        80/TCP,443/TCP                                                              3h13m
istio-ingressgateway   LoadBalancer   10.96.215.98    <pending>     15021:44913/TCP,80:7541/TCP,443:18948/TCP,31400:57642/TCP,15443:51913/TCP   3h13m
istiod                 ClusterIP      10.96.227.52    <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP                                       3h14m
prometheus             NodePort       10.96.137.156   <none>        9090:29735/TCP                                                              20m
```

同样可以看到grafana这个service的类型为ClusterIP，外部环境访问不了

```shell
[root@k8s-master grafana]# kubectl edit service grafana -n istio-system
service/grafana edited
```

![](https://github.com/Xiao254182/notes/blob/master/img/5/6.png)

![](https://github.com/Xiao254182/notes/blob/master/img/5/7.png)

```shell
[root@k8s-master grafana]# kubectl get service -n istio-system                          NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                     AGE
grafana                NodePort       10.96.64.221    <none>        3000:57237/TCP                                                              5m19s
istio-egressgateway    ClusterIP      10.96.143.20    <none>        80/TCP,443/TCP                                                              3h16m
istio-ingressgateway   LoadBalancer   10.96.215.98    <pending>     15021:44913/TCP,80:7541/TCP,443:18948/TCP,31400:57642/TCP,15443:51913/TCP   3h16m
istiod                 ClusterIP      10.96.227.52    <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP                                       3h16m
prometheus             NodePort       10.96.137.156   <none>        9090:29735/TCP                                                              23m
```

在浏览器上登录Grafana（<ip地址>:port），如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/8.png)

依次点击左侧导航栏的“Dashboards” →“Manage”进入Dashboard管理界面，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/9.png)

选择istio文件夹后选择Istio Mesh Dashboard，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/10.png)

切换到Istio Service Dashboard仪表盘，在Service中选择productpage，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/11.png)

2.在所有微服务中启用Istio

所有服务启用Istio：

```shell
[root@k8s-master grafana]# cd ../bookinfo/
[root@k8s-master bookinfo]# cat bookinfo.yaml | istioctl kube-inject -f - | kubectl apply -l app!=productpage -f -
service/details unchanged
serviceaccount/bookinfo-details unchanged
deployment.apps/details-v1 configured
service/ratings unchanged
serviceaccount/bookinfo-ratings unchanged
deployment.apps/ratings-v1 configured
service/reviews unchanged
serviceaccount/bookinfo-reviews unchanged
deployment.apps/reviews-v1 configured
serviceaccount/bookinfo-productpage unchanged
```

查看应用程序Pod，现在每个Pod的两个容器，一个容器是微服务本身，另一个是连接到它的Sidecar代理：

```shell
[root@k8s-master bookinfo]# kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-6789b69495-cjc6t       2/2     Running   0          43s
productpage-v1-58c755bb6b-b9b72   2/2     Running   0          4m40s
ratings-v1-5b5897bb75-cvgb2       2/2     Running   0          43s
reviews-v1-bb46ccb7-t79q9         2/2     Running   0          43s
```

再次查看Istio Mesh Dashboard，会发现当前命名空间下所有服务都会出现在服务列表中，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/11.png)

部署Kiali

```shell
[root@k8s-master bookinfo]# cd ../ && mkdir kiali && cd kiali/
[root@k8s-master kiali]# mv /root/istio-1.12.0/samples/addons/kiali.yaml .
[root@k8s-master kiali]# kubectl apply -f kiali.yaml
serviceaccount/kiali created
configmap/kiali created
clusterrole.rbac.authorization.k8s.io/kiali-viewer created
clusterrole.rbac.authorization.k8s.io/kiali created
clusterrolebinding.rbac.authorization.k8s.io/kiali created
role.rbac.authorization.k8s.io/kiali-controlplane created
rolebinding.rbac.authorization.k8s.io/kiali-controlplane created
service/kiali created
deployment.apps/kiali created
[root@k8s-master kiali]# kubectl get pod -n istio-system
[root@k8s-master kiali]# kubectl get pod -n istio-system
NAME                                   READY   STATUS    RESTARTS   AGE
grafana-6ccd56f4b6-m4bmb               1/1     Running   0          24m
istio-egressgateway-7f4864f59c-vhb5z   1/1     Running   0          3h35m
istio-ingressgateway-55d9fb9f-7q96k    1/1     Running   0          3h35m
istiod-555d47cb65-7bbbc                1/1     Running   0          3h35m
kiali-79b86ff5bc-kn2ng                 1/1     Running   0          52s
prometheus-64fd8ccd65-r28kz            2/2     Running   0          42m
[root@k8s-master kiali]# kubectl get service -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                     AGE
grafana                NodePort       10.96.64.221    <none>        3000:57237/TCP                                                              24m
istio-egressgateway    ClusterIP      10.96.143.20    <none>        80/TCP,443/TCP                                                              3h36m
istio-ingressgateway   LoadBalancer   10.96.215.98    <pending>     15021:44913/TCP,80:7541/TCP,443:18948/TCP,31400:57642/TCP,15443:51913/TCP   3h36m
istiod                 ClusterIP      10.96.227.52    <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP                                       3h36m
kiali                  ClusterIP      10.96.191.214   <none>        20001/TCP,9090/TCP                                                          102s
prometheus             NodePort       10.96.137.156   <none>        9090:29735/TCP                                                              43m
```

可以看到kiali这个service的类型为ClusterIP，外部环境访问不了

```shell
[root@k8s-master kiali]# kubectl edit service kiali -n istio-system
service/kiali edited
```

![](https://github.com/Xiao254182/notes/blob/master/img/5/12.png)

```shell
[root@k8s-master kiali]# kubectl get service -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                     AGE
grafana                NodePort       10.96.64.221    <none>        3000:57237/TCP                                                              26m
istio-egressgateway    ClusterIP      10.96.143.20    <none>        80/TCP,443/TCP                                                              3h37m
istio-ingressgateway   LoadBalancer   10.96.215.98    <pending>     15021:44913/TCP,80:7541/TCP,443:18948/TCP,31400:57642/TCP,15443:51913/TCP   3h37m
istiod                 ClusterIP      10.96.227.52    <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP                                       3h37m
kiali                  NodePort       10.96.191.214   <none>        20001:25942/TCP,9090:38543/TCP                                              3m6s
prometheus             NodePort       10.96.137.156   <none>        9090:29735/TCP                                                              44m
```

访问Kiali控住台（<ip地址>:port），如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/13.png)

通过可视化界面来查看应用程序的拓扑结构，点击“Graph”按钮，在Namespace下拉菜单中选择命名空间default，然后在Display下拉菜单中选中“Traffic Animation”和“Idle Nodes”复选框，就可以看到实时流量动画。如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/14.png)

reviews微服务v1版本不会调用ratings服务，所以图中ratings服务无流量通过。

3.监控Istio

访问Prometheus控制台（<ip地址>:30090）,在Expression输入框中输入要查询的参数，然后点击Execute按钮即可在Console中查看查询结果。查询请求时采用istio_requests_total指标，这是一个标准的Istio指标

如查询命名空间的所有请求（istio_requests_total{destination_service_namespace=“default”,reporter=“destination”})

如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/15.png)

查询reviews微服务的请求（istio_requests_total{destination_service_namespace=“default”,reporter=“destination”,destination_service_name=“reviews”})

如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/16.png)

四：灰度发布

1.部署新版本服务

将v2、v3版本的reviews服务部署到集群中，均为单一副本。新版本的reviews可以正常工作后，实际生产流量将开始到达该服务。在当前的设置下，50%的流量将到达旧版本（1个旧版本的Pod），而另外50%的流量将到达新版本（1个新版本Pod）。

部署v2版本的reviews微服务并开启Istio：

```shell
[root@k8s-master bookinfo]# vim reviews-v2.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: reviews-v2
  labels:
    app: reviews
    version: v2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: reviews
      version: v2
  template:
    metadata:
      labels:
        app: reviews
        version: v2
    spec:
      containers:
      - name: reviews
        image: docker.io/istio/examples-bookinfo-reviews-v2:1.16.2
        imagePullPolicy: IfNotPresent
        env:
        - name: LOG_DIR
          value: "/tmp/logs"
        ports:
        - containerPort: 9080
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: wlp-output
          mountPath: /opt/ibm/wlp/output
        securityContext:
          runAsUser: 1000
      volumes:
      - name: wlp-output
        emptyDir: {}
      - name: tmp
        emptyDir: {}
[root@k8s-master bookinfo]# vim reviews-v3.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: reviews-v3
  labels:
    app: reviews
    version: v3
spec:
  replicas: 1
  selector:
    matchLabels:
      app: reviews
      version: v3
  template:
    metadata:
      labels:
        app: reviews
        version: v3
    spec:
      serviceAccountName: bookinfo-reviews
      containers:
      - name: reviews
        image: docker.io/istio/examples-bookinfo-reviews-v3:1.16.2
        imagePullPolicy: IfNotPresent
        env:
        - name: LOG_DIR
          value: "/tmp/logs"
        ports:
        - containerPort: 9080
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: wlp-output
          mountPath: /opt/ibm/wlp/output
        securityContext:
          runAsUser: 1000
      volumes:
      - name: wlp-output
        emptyDir: {}
      - name: tmp
        emptyDir: {}
[root@k8s-master bookinfo]# cat reviews-v2.yaml | istioctl kube-inject -f - | kubectl apply -f -
deployment.apps/reviews-v2 created
[root@k8s-master bookinfo]# cat reviews-v3.yaml | istioctl kube-inject -f - | kubectl apply -f -
deployment.apps/reviews-v3 created
```

查看Pod：

```shell
[root@k8s-master bookinfo]# kubectl get pods
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-6789b69495-cjc6t       2/2     Running   0          30m
productpage-v1-58c755bb6b-b9b72   2/2     Running   0          34m
ratings-v1-5b5897bb75-cvgb2       2/2     Running   0          30m
reviews-v1-bb46ccb7-t79q9         2/2     Running   0          30m
reviews-v2-7b96f464dc-2wd7p       2/2     Running   0          82s
reviews-v3-dc49d69bc-lzcc2        2/2     Running   0          21s
```

访问Bookinfo应用程序页面，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/17.png)

![](https://github.com/Xiao254182/notes/blob/master/img/5/18.png)

![](https://github.com/Xiao254182/notes/blob/master/img/5/19.png)

观察评级上的星标，发现有时返回的页面带有黑色星标（v2版本、大约三分之一的时间），有时带有红色星标（v3版本、大约三分之一的时间），有时不带星标（v1版本、大约三分之一的时间），这是因为没有明确的默认服务版本和路由，Istio将以轮询方式将请求路由到所有可用版本，所以三种评分结果出现的概率均为三分之一。

查看实时流量监控，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/20.png)

可以看到，v2和v3版本的reviews微服务已正常工作，因v2和v3版本会调用ratings服务，所以图中ratings服务也有流量通过。

2.路由请求

Bookinfo应用程序包含四个独立的微服务，每个微服务都有多个版本。其中一个微服务reviews的3个不同版本已经部署并同时运行。因为没有明确的默认服务版本和路由，Istio将以轮询方式将请求路由到所有可用版本。这样将导致在浏览器中访问Bookinfo应用程序时，输出有时包含星级评分，有时则不包含。

Kubernetes方式下控制流量分配需要调整每个Deployment的副本数目。例如，将10％的流量发送到金丝雀版本（v2），v1和v2的副本可以分别设置为9和1。由于在启用Istio后不再需要保持副本比例，所以可以安全地设置Kubernetes HPA来管理三个版本Deployment的副本：

```shell
[root@k8s-master bookinfo]# kubectl autoscale deployment reviews-v1 --cpu-percent=50 --min=1 --max=10
horizontalpodautoscaler.autoscaling/reviews-v1 autoscaled
[root@k8s-master bookinfo]# kubectl autoscale deployment reviews-v2 --cpu-percent=50 --min=1 --max=10
horizontalpodautoscaler.autoscaling/reviews-v2 autoscaled
[root@k8s-master bookinfo]# kubectl autoscale deployment reviews-v3 --cpu-percent=50 --min=1 --max=10
horizontalpodautoscaler.autoscaling/reviews-v3 autoscaled
```

如果要仅路由到一个版本，请为微服务设置默认版本的Virtual Service。在这种情况下，Virtual Service将所有流量路由到每个微服务的v1版本。

默认请求路由配置文件如下：

```shell
[root@k8s-master bookinfo]# vim virtual-service-all-v1.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: productpage
spec:
  hosts:
  - productpage
  http:
  - route:
    - destination:
        host: productpage
        subset: v1
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
  - reviews
  http:
  - route:
    - destination:
        host: reviews
        subset: v1
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: ratings
spec:
  hosts:
  - ratings
  http:
  - route:
    - destination:
        host: ratings
        subset: v1
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: details
spec:
  hosts:
  - details
  http:
  - route:
    - destination:
        host: details
        subset: v1
```

配置默认请求路由：

```shell
[root@k8s-master bookinfo]# kubectl apply -f virtual-service-all-v1.yaml
virtualservice.networking.istio.io/productpage created
virtualservice.networking.istio.io/reviews created
virtualservice.networking.istio.io/ratings created
virtualservice.networking.istio.io/details created
```

现在已将Istio配置为路由到Bookinfo微服务的v1版本，最重要的是reviews服务的v1版本。

在浏览器中打开Bookinfo站点，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/21.png)

此时无论刷新多少次，页面的评分部分都不会显示评级星标。这是因为Istio被配置为将评分服务的所有流量路由到v1版本的reviews，而此版本的服务不调用星级评分服务。

查看实时流量监控，也可以看到此时无流量流向v2和v3版本，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/22.png)

3.流量转移

使用下面的命令把50%的流量从reviews:v1转移到reviews:v3（金丝雀版本）：

```shell
[root@k8s-master bookinfo]# vim virtual-service-reviews-50-50.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
    - reviews
  http:
  - route:
    - destination:
        host: reviews
        subset: v1
      weight: 50
    - destination:
        host: reviews
        subset: v3
      weight: 50
[root@k8s-master bookinfo]# kubectl apply -f virtual-service-reviews-50-50.yaml
virtualservice.networking.istio.io/reviews configured
```

当规则设置生效后，Istio将确保只有50%的请求发送到金丝雀版本，无论每个版本的运行副本数量是多少。

刷新浏览器中的/productpage页面，因为reviews的v3版本可以访问带星级评价服务，而v1版本不能，所以大约有50%的几率会看到页面中带红色星级的评价内容，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/23.png)

在Kiali上查看实时流量监控，可以看到流量已经流向了v3版本，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/24.png)

点击productpage服务与reviews服务之间的连线，在右侧可以看到每秒发送到reviews服务的http请求为1.89，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/25.png)

点击reviews服务和v1版本之间的连线，可以看到v1版本每秒接收到的http请求为1.13，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/26.png)

点击reviews服务和v3版本之间的连线，可以看到v3版本每秒接收到的http请求为0.76，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/27.png)

说明流向reviews服务的流量各有50%流向了v1和v3，路由规则应用成功。

假如认为reviews:v3微服务已经稳定，可以通过应用Virtual Service规则将100%的流量路由reviews:v3：

```shell
[root@k8s-master bookinfo]# vim virtual-service-reviews-v3.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
    - reviews
  http:
  - route:
    - destination:
        host: reviews
        subset: v3
[root@k8s-master bookinfo]# kubectl apply -f virtual-service-reviews-v3.yaml
virtualservice.networking.istio.io/reviews configured
```

再次刷新/productpage时，将始终看到带有红色星级评分的书评。

![](https://github.com/Xiao254182/notes/blob/master/img/5/28.png)

查看实时流量，发现流量全部流向v3，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/29.png)

5.流量镜像

流量镜像，也称为影子流量，是一个以尽可能低的风险为生产带来变化的强大的功能。镜像会将实时流量的副本发送到镜像服务。镜像流量发生在主服务的关键请求路径之外。

初始化默认路由规则，将所有流量路由到服务的v1版本：

```shell
[root@k8s-master bookinfo]# kubectl apply -f virtual-service-all-v1.yaml
virtualservice.networking.istio.io/productpage unchanged
virtualservice.networking.istio.io/reviews configured
virtualservice.networking.istio.io/ratings unchanged
virtualservice.networking.istio.io/details unchanged
```

改变reviews服务的流量规则，将v1版本的流量镜像到v2版本：

```shell
[root@k8s-master bookinfo]# vim virtual-service-mirroring.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
    - reviews
  http:
  - route:
    - destination:
        host: reviews
        subset: v1
      weight: 100
    mirror:
        host: reviews
        subset: v2
[root@k8s-master bookinfo]# kubectl apply -f virtual-service-mirroring.yaml
virtualservice.networking.istio.io/reviews configured
```

6.分布式追踪

分布式追踪可以让用户对跨多个分布式服务网格的1个请求进行追踪分析。这样进而可以通过可视化的方式更加深入地了解请求的延迟，序列化和并行度。

Istio利用Envoy的分布式追踪功能提供了开箱即用的追踪集成。确切地说，Istio提供了安装各种追踪后端服务的选项，并且通过配置代理来自动发送追踪Span到追踪后端服务。

部署Jaeger

```shell
[root@k8s-master bookinfo]# cd ../ && mkdir jaeger && cd jaeger
[root@k8s-master jaeger]# mv /root/istio-1.12.0/samples/addons/jaeger.yaml .
[root@k8s-master jaeger]# kubectl apply -f jaeger.yaml
deployment.apps/jaeger created
service/tracing created
service/zipkin created
service/jaeger-collector created
[root@k8s-master jaeger]# kubectl get pod -n istio-system
NAME                                   READY   STATUS    RESTARTS   AGE
grafana-6ccd56f4b6-m4bmb               1/1     Running   0          76m
istio-egressgateway-7f4864f59c-vhb5z   1/1     Running   0          4h27m
istio-ingressgateway-55d9fb9f-7q96k    1/1     Running   0          4h27m
istiod-555d47cb65-7bbbc                1/1     Running   0          4h27m
jaeger-5d44bc5c5d-47gwm                1/1     Running   0          81s
kiali-79b86ff5bc-kn2ng                 1/1     Running   0          52m
prometheus-64fd8ccd65-r28kz            2/2     Running   0          94m
[root@k8s-master jaeger]# kubectl get service -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                     AGE
grafana                NodePort       10.96.64.221    <none>        3000:57237/TCP                                                              76m
istio-egressgateway    ClusterIP      10.96.143.20    <none>        80/TCP,443/TCP                                                              4h27m
istio-ingressgateway   LoadBalancer   10.96.215.98    <pending>     15021:44913/TCP,80:7541/TCP,443:18948/TCP,31400:57642/TCP,15443:51913/TCP   4h27m
istiod                 ClusterIP      10.96.227.52    <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP                                       4h27m
jaeger-collector       ClusterIP      10.96.6.78      <none>        14268/TCP,14250/TCP,9411/TCP                                                108s
kiali                  NodePort       10.96.191.214   <none>        20001:25942/TCP,9090:38543/TCP                                              53m
prometheus             NodePort       10.96.137.156   <none>        9090:29735/TCP                                                              94m
tracing                ClusterIP      10.96.236.162   <none>        80/TCP,16685/TCP                                                            108s
zipkin                 ClusterIP      10.96.120.4     <none>        9411/TCP                                                                    108s
```

可以看到zipkin这个service的类型为ClusterIP，外部环境访问不了

```shell
[root@k8s-master jaeger]# kubectl edit service tracing -n istio-system
service/tracing edited
```

![](https://github.com/Xiao254182/notes/blob/master/img/5/30.png)

![](https://github.com/Xiao254182/notes/blob/master/img/5/31.png)

```shell
[root@k8s-master jaeger]# kubectl get service -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                     AGE
grafana                NodePort       10.96.64.221    <none>        3000:57237/TCP                                                              90m
istio-egressgateway    ClusterIP      10.96.143.20    <none>        80/TCP,443/TCP                                                              4h41m
istio-ingressgateway   LoadBalancer   10.96.215.98    <pending>     15021:44913/TCP,80:7541/TCP,443:18948/TCP,31400:57642/TCP,15443:51913/TCP   4h41m
istiod                 ClusterIP      10.96.227.52    <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP                                       4h41m
jaeger-collector       ClusterIP      10.96.6.78      <none>        14268/TCP,14250/TCP,9411/TCP                                                15m
kiali                  NodePort       10.96.191.214   <none>        20001:25942/TCP,9090:38543/TCP                                              67m
prometheus             NodePort       10.96.137.156   <none>        9090:29735/TCP                                                              108m
tracing                NodePort       10.96.236.162   <none>        80:17939/TCP,16685:57090/TCP                                                15m
zipkin                 ClusterIP      10.96.120.4     <none>        9411/TCP                                                                    15m
```

登录Jaeger控制台（<ip地址>:port），如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/32.png)

从仪表盘左边面板的Service下拉列表中选择“productpage.default”，然后点击“Find Traces”，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/33.png)

点击位于最上面的最近一次追踪，查看对应最近一次访问/productpage的详细信息，如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/34.png)

追踪信息由一组Span组成，每个Span对应一个Bookinfo Service。这些Service在执行/productpage请求时被调用，或是Istio内部组件。

(PS:

使用 Zipkin 进行分布式追踪:

Zipkin是一个分布式追踪系统。我们可以轻松地监控服务网格中发生的分布式事务，发现任何性能或延迟问题。

为了让我们的服务参与分布式追踪，我们需要在进行任何下游服务调用时传播服务的 HTTP 头信息。尽管所有的请求都要经过 Istio sidecar，但 Istio 没有办法将出站请求与产生这些请求的入站请求联系起来。通过在应用程序中传播相关的头信息可以帮助 Zipkin 将这些跟踪信息拼接起来。

Istio 依赖于 B3 跟踪头（以 `x-b3` 开头的 header）和 Envoy 生成的请求 ID（`x-request-id`）。B3 头信息用于跨服务边界的跟踪上下文传播。

以下是我们需要在我们的应用程序中对每个发出的请求进行传播的特定头文件名称：

```shell
 x-request-id
 x-b3-traceid
 x-b3-spanid
 x-b3-parentspanid
 x-b3-sampled
 x-b3-flags
 b3
```

如果你使用 Lightstep，你还需要转发名为 `x-ot-span-context` 的头。

传播头信息最常见的方法是从传入的请求中复制它们，并将它们包含在所有从你的应用程序发出的请求中。

你用 Istio 服务网格得到的跟踪只在服务边界捕获。为了了解应用程序的行为并排除故障，你需要通过创建额外的跨度（span）来正确检测你的应用程序。

部署Zipkin

```shell
[root@k8s-master jaeger]# cd ../ && mkdir zipkin && cd zipkin/
[root@k8s-master zipkin]# mv /root/istio-1.12.0/samples/addons/extras/zipkin.yaml .
[root@k8s-master zipkin]# kubectl apply -f zipkin.yaml
deployment.apps/zipkin created
service/tracing configured
service/zipkin configured
[root@k8s-master zipkin]# kubectl get pod -n istio-system
NAME                                   READY   STATUS    RESTARTS   AGE
grafana-6ccd56f4b6-m4bmb               1/1     Running   0          98m
istio-egressgateway-7f4864f59c-vhb5z   1/1     Running   0          4h49m
istio-ingressgateway-55d9fb9f-7q96k    1/1     Running   0          4h49m
istiod-555d47cb65-7bbbc                1/1     Running   0          4h50m
jaeger-5d44bc5c5d-47gwm                1/1     Running   0          23m
kiali-79b86ff5bc-kn2ng                 1/1     Running   0          75m
prometheus-64fd8ccd65-r28kz            2/2     Running   0          116m
zipkin-dbcd8698b-fgvlg                 1/1     Running   0          2m12s
[root@k8s-master zipkin]# kubectl get service -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                     AGE
grafana                NodePort       10.96.64.221    <none>        3000:57237/TCP                                                              98m
istio-egressgateway    ClusterIP      10.96.143.20    <none>        80/TCP,443/TCP                                                              4h50m
istio-ingressgateway   LoadBalancer   10.96.215.98    <pending>     15021:44913/TCP,80:7541/TCP,443:18948/TCP,31400:57642/TCP,15443:51913/TCP   4h50m
istiod                 ClusterIP      10.96.227.52    <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP                                       4h50m
jaeger-collector       ClusterIP      10.96.6.78      <none>        14268/TCP,14250/TCP,9411/TCP                                                24m
kiali                  NodePort       10.96.191.214   <none>        20001:25942/TCP,9090:38543/TCP                                              75m
prometheus             NodePort       10.96.137.156   <none>        9090:29735/TCP                                                              117m
tracing                ClusterIP      10.96.236.162   <none>        80/TCP                                                                      24m
zipkin                 ClusterIP      10.96.120.4     <none>        9411/TCP                                                                    24m
```

可以看到zipkin这个service的类型为ClusterIP，外部环境访问不了

```shell
[root@k8s-master zipkin]# kubectl edit service zipkin -n istio-system
service/zipkin edited
```

![](https://github.com/Xiao254182/notes/blob/master/img/5/35.png)

```shell
[root@k8s-master zipkin]# kubectl get service -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                     AGE
grafana                NodePort       10.96.64.221    <none>        3000:57237/TCP                                                              100m
istio-egressgateway    ClusterIP      10.96.143.20    <none>        80/TCP,443/TCP                                                              4h51m
istio-ingressgateway   LoadBalancer   10.96.215.98    <pending>     15021:44913/TCP,80:7541/TCP,443:18948/TCP,31400:57642/TCP,15443:51913/TCP   4h51m
istiod                 ClusterIP      10.96.227.52    <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP                                       4h51m
jaeger-collector       ClusterIP      10.96.6.78      <none>        14268/TCP,14250/TCP,9411/TCP                                                25m
kiali                  NodePort       10.96.191.214   <none>        20001:25942/TCP,9090:38543/TCP                                              77m
prometheus             NodePort       10.96.137.156   <none>        9090:29735/TCP                                                              118m
tracing                ClusterIP      10.96.236.162   <none>        80/TCP                                                                      25m
zipkin                 NodePort       10.96.120.4     <none>        9411:17844/TCP                                                              25m
```

登录Zipkin控制台（<ip地址>:port），如图所示：

![](https://github.com/Xiao254182/notes/blob/master/img/5/36.png)

)
