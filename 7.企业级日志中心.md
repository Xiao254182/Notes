#### 7.企业级日志中心

⼀、组件介绍  

1、Elasticsearch  

```
是⼀个基于Lucene的搜索服务器。提供搜集、分析、存储数据 三⼤功能。它提供了⼀个分布式多⽤户能⼒的全⽂搜索引擎，基于 RESTful web接⼝。
Elasticsearch是⽤Java开发的，并作为Apache许可 条款下的开放源码发布，是当前流⾏的企业级搜索引擎。设计⽤于云计 算中，能够达到实时搜
索，稳定，可靠，快速，安装使⽤⽅便。
```

 2、Logstash  

```
主要是用来日志的搜集、分析、过滤日志的工具。用于管理日志 和事件的工具，你可以用它去收集日志、转换日志、解析日志并将他们 作为数据
提供给其它模块调用，例如搜索、存储等。
```

3、Kibana  

```
⼀个优秀的前端⽇志展示框架，它可以⾮常详细的将⽇志转化 为各种图表，为⽤户提供强⼤的数据可视化⽀持,它能够搜索、展示存 储在
 Elasticsearch 中索引数据。使⽤它可以很⽅便的⽤图表、表格、 地图展示和分析数据。
```

 4、Kafka

```
数据缓冲队列。作为消息队列解耦了处理过程，同时提⾼了可扩展 性。具有峰值处理能⼒，使⽤消息队列能够使关键组件顶住突发的访问压⼒，⽽
不会因为突发的超负荷的请求⽽完全崩溃。 
1.发布和订阅记录流，类似于消息队列或企业消息传递系统。 
2.以容错持久的⽅式存储记录流。 
3.处理记录发⽣的流。 
```

5、Filebeat  

```
⾪属于Beats,轻量级数据收集引擎。基于原先 Logstash-fowarder 的源码改造出来。换句话说：Filebeat就是新版的 Logstash fowarder，也会是
ELK Stack在 Agent 的第⼀选择
⽬前Beats包含四种⼯具:
1.Packetbeat(搜集⽹络流量数据)
2.Metricbeat(搜集系统、进程和⽂件系统级别的 CPU 和内存 使⽤情况等数据。通过从操作系统和服务收集指标，帮助您监 控服务器及其托管的服务)
3.Filebeat(搜集⽂件数据)
4.Winlogbeat(搜集 Windows 事件⽇志数据)
```

二：节点规划

|       IP       | 主机名 |                   安装软件                    |
| :------------: | :----: | :-------------------------------------------: |
| 192.168.100.11 | master | Elasticsearch/zookeeper/kafka/Logstash/kibana |
| 192.168.100.12 | node1  |         Elasticsearch/zookeeper/kafka         |
| 192.168.100.13 | node2  |         Elasticsearch/zookeeper/kafka         |
| 192.168.100.14 | node3  |                   Filebeat                    |

三：版本说明

```
jdk: 1.8
Elasticsearch: 6.5.4
Logstash: 6.5.4
Kibana: 6.5.4
Kafka: 2.11-1
Filebeat: 6.5.4
相应的版本最好下载对应的插件
```

四：搭建架构

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\日志中心架构图.png)

五：部署

1.Elasticsearch部署

节点ip:192.168.100.11,192.168.100.12,192.168.100.13

软件：jdk-8u211-linux-x64.tar.gz

​           下载地址：https://pan.baidu.com/s/19Eg67jW6V5XkDAfYQ0lxhA?pwd=6r5f

​           elasticsearch 6.5.4.tar.gz

​           下载地址：http://dl.elasticsearch.cn/elasticsearch/elasticsearch-6.5.4.tar.gz

​           node-v14.21.3-linux-x64.tar.xz

​           下载地址：https://npmmirror.com/mirrors/node/v14.21.3/node-v14.21.3-linux-x64.tar.xz

1.1 安装配置jdk8

```shell
[root@master ~]# ls
jdk-8u211-linux-x64_.tar.gz
[root@master ~]# tar -zxf jdk-8u211-linux-x64_.tar.gz -C /usr/local/
[root@master ~]# mv /usr/local/jdk1.8.0_211/ /usr/local/java
[root@master ~]# echo 'JAVA_HOME=/usr/local/java
PATH=$JAVA_HOME/bin:$PATH
export JAVA_HOME PATH' >>/etc/profile.d/java.sh
```

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\1.png)

```shell
[root@master ~]# source /etc/profile.d/java.sh
[root@master ~]# java -version
java version "1.8.0_211"
Java(TM) SE Runtime Environment (build 1.8.0_211-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)
```

1.2 安装配置ES

(1)创建运行ES的普通用户

```shell
[root@master ~]# useradd ela
[root@master ~]# echo "000000" | passwd --stdin ela
更改用户 ela 的密码 。
passwd：所有的身份验证令牌已经成功更新。
```

(2)安装配置ES

```shell
[root@master ~]# wget http://dl.elasticsearch.cn/elasticsearch/elasticsearch-6.5.4.tar.gz
--2024-07-23 11:55:18--  http://dl.elasticsearch.cn/elasticsearch/elasticsearch-6.5.4.tar.gz
正在解析主机 dl.elasticsearch.cn (dl.elasticsearch.cn)... 120.25.241.170
正在连接 dl.elasticsearch.cn (dl.elasticsearch.cn)|120.25.241.170|:80... 已连接。
已发出 HTTP 请求，正在等待回应... 200 OK
长度：113322649 (108M) [application/octet-stream]
正在保存至: “elasticsearch-6.5.4.tar.gz”

100%[=========================================================>] 113,322,649 1.33MB/s 用时 86s

2024-07-23 11:56:44 (1.26 MB/s) - 已保存 “elasticsearch-6.5.4.tar.gz” [113322649/113322649])

[root@master ~]# tar -zxf elasticsearch-6.5.4.tar.gz -C /usr/local/
[root@master ~]# echo 'cluster.name: qf01-elk                                                                               #集群名称，各节点配成相同的集群名称
node.name: elk01                      #节点名称，各节点配置不同(各节点不同)
node.master: true					  #指示某个节点是否符合成为主节点的条件
node.data: true						  #指示节点是否为数据节点。数据节点包含并管理索引的一部分
path.data: /data/elasticsearch/data   #数据存储目录
path.logs: /data/elasticsearch/logs   #日志存储目录
bootstrap.memory_lock: true           #内存锁定，是否禁用交换
bootstrap.system_call_filter: false   #系统调用过滤器
network.host: 0.0.0.0                 #绑定节点IP
http.port: 9200                       #rest api端⼝
discovery.zen.ping.unicast.hosts: ["192.168.100.11","192.168.100.12","192.168.100.13"]                                                                       #提供其他Elasticsearch 服务节点的单点广播发现功能
discovery.zen.minimum_master_nodes: 2 #集群中可工作的具有Master节点资格的最小数量,避免裂脑情况
discovery.zen.ping_timeout: 150s      #节点在发现过程中的等待时间
discovery.zen.fd.ping_retries: 10     #节点发现重试次数
client.transport.ping_timeout: 60s    #设置了客户端与集群节点之间的 ping 超时时间
http.cors.enabled: true               #是否允许跨源REST请求，用于允许head插件访问ES
http.cors.allow-origin: "*"           #允许的源地址'>> /usr/local/elasticsearch-6.5.4/config/elasticsearch.yml
```

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\2.png)

(PS:

避免“裂脑”情况

“裂脑”情况是集群中节点之间的通信由于网络故障或其中一个节点的内部故障而失败。在这种情况下，可能会有多个节点认为自己是主节点，从而导致数据不一致的状态。为了避免这种情况，我们可以对 Elasticsearch 配置文件中的discovery.zen.minimum_master_nodes指令进行更改，该指令确定需要通信（投票）多少节点来选举主节点。确定此数量的最佳做法是使用以下公式来确定此数量：N/2 + 1。N 是集群中符合条件的主节点的数量。然后将结果四舍五入到最接近的整数

禁用交换

换出未使用的内存是一种通用行为，但在 Elasticsearch 的上下文中可能会导致断开连接、性能不佳以及一般情况下 — 不稳定的集群。为了避免交换，您可以禁用所有交换（如果 Elasticsearch 是服务器上运行的唯一服务，则建议使用），或者您可以使用mlockall将 Elasticsearch 进程锁定到 RAM

)

(3)设置JVM堆大小

```shell
[root@master ~]# sed -i 's/-Xms1g/-Xms2g/' /usr/local/elasticsearch-6.5.4/config/jvm.options
[root@master ~]# sed -i 's/-Xmx1g/-Xmx2g/' /usr/local/elasticsearch-6.5.4/config/jvm.options
```

（PS： 1.确保堆内存最小值（Xms）与最大值（Xmx）的大小相同，防止程序在运行时改变堆内存大小。 

​             2.如果系统内存足够大，将堆内存最大和最小值设置为31G，因为 有⼀个32G性能瓶颈问题

​             3.堆内存大小不要超过系统内存的50%）

(4)创建ES数据及日志存储目录

```shell
[root@master ~]# mkdir -pv /data/elasticsearch/data
mkdir: 已创建目录 "/data"
mkdir: 已创建目录 "/data/elasticsearch"
mkdir: 已创建目录 "/data/elasticsearch/data"
[root@master ~]# mkdir -pv /data/elasticsearch/logs
mkdir: 已创建目录 "/data/elasticsearch/logs"
```

(5)修改安装⽬录及存储⽬录权限

```shell
[root@master ~]# chown -R ela:ela /data/elasticsearch
[root@master ~]# chown -R ela:ela /usr/local/elasticsearch-6.5.4
```

1.3 系统优化

(1)增加最大文件打开数

```shell
[root@master ~]# echo "* - nofile 65536" >> /etc/security/limits.conf
```

(2)增加最大进程数

```shell
[root@master ~]# echo "* soft nproc 31717" >> /etc/security/limits.conf
```

(3)增加最大内存映射数

```shell
[root@master ~]# echo "vm.max_map_count=262144" >> /etc/sysctl.conf
[root@master ~]# sysctl -p
vm.max_map_count = 262144
```

1.4 启动ES

```shell
[root@master ~]#systemctl stop firewalld && systemctl disable firewalld && sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config && setenforce 0
[root@master ~]# su - ela -c "cd /usr/local/elasticsearch-6.5.4 && nohup bin/elasticsearch &"
[root@master ~]# nohup: 把输出追加到"nohup.out"
```

等待一段时间后，在浏览器访问\<ip地址:9200>

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\3.png)

在node1节点上同样部署Elasticsearch

```shell
[root@master ~]# ssh 192.168.100.12
root@192.168.100.12's password:
Last login: Tue Jul 23 11:35:56 2024 from 192.168.100.11
[root@node1 ~]# scp 192.168.100.11:/root/* .
The authenticity of host '192.168.100.11 (192.168.100.11)' can't be established.
ECDSA key fingerprint is SHA256:WtLNUEB1BF+ZHoow3WB7exoDL//ThqC96/kGUOhp/so.
ECDSA key fingerprint is MD5:d3:ee:58:ff:cc:e2:6a:09:b4:bd:9f:28:47:3b:6d:53.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.100.11' (ECDSA) to the list of known hosts.
root@192.168.100.11's password:
elasticsearch-6.5.4.tar.gz                                       100%  108MB 100.2MB/s   00:01
jdk-8u211-linux-x64_.tar.gz                                      100%  186MB 109.7MB/s   00:01
[root@node1 ~]# tar -zxf jdk-8u211-linux-x64_.tar.gz -C /usr/local/
[root@node1 ~]# mv /usr/local/jdk1.8.0_211/ /usr/local/java
[root@node1 ~]# echo 'JAVA_HOME=/usr/local/java
PATH=$JAVA_HOME/bin:$PATH
export JAVA_HOME PATH' >>/etc/profile.d/java.sh
[root@node1 ~]# source /etc/profile.d/java.sh
[root@node1 ~]# java -version
java version "1.8.0_211"
Java(TM) SE Runtime Environment (build 1.8.0_211-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)
[root@node1 ~]# useradd ela
[root@node1 ~]# echo "000000" | passwd --stdin ela
更改用户 ela 的密码 。
passwd：所有的身份验证令牌已经成功更新。
[root@node1 ~]# tar -zxf elasticsearch-6.5.4.tar.gz -C /usr/local/
[root@node1 ~]# scp 192.168.100.11:/usr/local/elasticsearch-6.5.4/config/elasticsearch.yml /usr/local/elasticsearch-6.5.4/config/elasticsearch.yml
root@192.168.100.11's password:
elasticsearch.yml                                                100%  528   508.8KB/s   00:00
[root@node1 ~]# sed -i "s/elk01/elk02/g" /usr/local/elasticsearch-6.5.4/config/elasticsearch.yml   
[root@node1 ~]# sed -i 's/-Xms1g/-Xms2g/' /usr/local/elasticsearch-6.5.4/config/jvm.options
[root@node1 ~]# sed -i 's/-Xmx1g/-Xmx2g/' /usr/local/elasticsearch-6.5.4/config/jvm.options
[root@node1 ~]# mkdir -pv /data/elasticsearch/data
mkdir: 已创建目录 "/data"
mkdir: 已创建目录 "/data/elasticsearch"
mkdir: 已创建目录 "/data/elasticsearch/data"
[root@node1 ~]# mkdir -pv /data/elasticsearch/logs
mkdir: 已创建目录 "/data/elasticsearch/logs"
[root@node1 ~]# chown -R ela:ela /data/elasticsearch
[root@node1 ~]# chown -R ela:ela /usr/local/elasticsearch-6.5.4/
[root@node1 ~]# echo "* - nofile 65536" >> /etc/security/limits.conf
[root@node1 ~]# echo "* soft nproc 31717" >> /etc/security/limits.conf
[root@node1 ~]# echo "vm.max_map_count=262144" >> /etc/sysctl.conf
[root@node1 ~]# sysctl -p
vm.max_map_count = 262144
[root@node1 ~]# systemctl stop firewalld && systemctl disable firewalld && sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config && setenforce 0
Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.
Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.
[root@node1 ~]# su - ela -c "cd /usr/local/elasticsearch-6.5.4 && nohup bin/elasticsearch &"
nohup: 把输出追加到"nohup.out"
```

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\4.png)

在node2节点上同样部署Elasticsearch

```shell
[root@node1 ~]# ssh 192.168.100.13
root@192.168.100.13's password:
Last login: Tue Jul 23 11:36:23 2024 from 192.168.100.12
[root@node2 ~]# scp 192.168.100.11:/root/* .
The authenticity of host '192.168.100.11 (192.168.100.11)' can't be established.
ECDSA key fingerprint is SHA256:WtLNUEB1BF+ZHoow3WB7exoDL//ThqC96/kGUOhp/so.
ECDSA key fingerprint is MD5:d3:ee:58:ff:cc:e2:6a:09:b4:bd:9f:28:47:3b:6d:53.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.100.11' (ECDSA) to the list of known hosts.
root@192.168.100.11's password:
elasticsearch-6.5.4.tar.gz                                       100%  108MB  37.6MB/s   00:02
jdk-8u211-linux-x64_.tar.gz                                      100%  186MB  25.2MB/s   00:07
[root@node2 ~]# tar -zxf jdk-8u211-linux-x64_.tar.gz -C /usr/local/
[root@node2 ~]# mv /usr/local/jdk1.8.0_211/ /usr/local/java
[root@node2 ~]# echo 'JAVA_HOME=/usr/local/java
PATH=$JAVA_HOME/bin:$PATH
export JAVA_HOME PATH' >>/etc/profile.d/java.sh
[root@node2 ~]# source /etc/profile.d/java.sh
[root@node2 ~]# java -version
java version "1.8.0_211"
Java(TM) SE Runtime Environment (build 1.8.0_211-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)
[root@node2 ~]# useradd ela
[root@node2 ~]# echo "000000" | passwd --stdin ela
更改用户 ela 的密码 。
passwd：所有的身份验证令牌已经成功更新。
[root@node2 ~]# tar -zxf elasticsearch-6.5.4.tar.gz -C /usr/local/
[root@node2 ~]# scp 192.168.100.11:/usr/local/elasticsearch-6.5.4/config/elasticsearch.yml /usr/local/elasticsearch-6.5.4/config/elasticsearch.yml
root@192.168.100.11's password:
elasticsearch.yml                                                100%  528   470.3KB/s   00:00
[root@node2 ~]# sed -i "s/elk01/elk03/g" /usr/local/elasticsearch-6.5.4/config/elasticsearch.yml
[root@node2 ~]# sed -i 's/-Xms1g/-Xms2g/' /usr/local/elasticsearch-6.5.4/config/jvm.options
[root@node2 ~]# sed -i 's/-Xmx1g/-Xmx2g/' /usr/local/elasticsearch-6.5.4/config/jvm.options
[root@node2 ~]# mkdir -pv /data/elasticsearch/data
mkdir: 已创建目录 "/data"
mkdir: 已创建目录 "/data/elasticsearch"
mkdir: 已创建目录 "/data/elasticsearch/data"
[root@node2 ~]# mkdir -pv /data/elasticsearch/logs
mkdir: 已创建目录 "/data/elasticsearch/logs"
[root@node2 ~]# chown -R ela:ela /data/elasticsearch
[root@node2 ~]# chown -R ela:ela /usr/local/elasticsearch-6.5.4/
[root@node2 ~]# echo "* - nofile 65536" >> /etc/security/limits.conf
[root@node2 ~]# echo "* soft nproc 31717" >> /etc/security/limits.conf
[root@node2 ~]# echo "vm.max_map_count=262144" >> /etc/sysctl.conf
[root@node2 ~]# sysctl -p
vm.max_map_count = 262144
[root@node2 ~]# systemctl stop firewalld && systemctl disable firewalld && sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config && setenforce 0
Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.
Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.
[root@node2 ~]# su - ela -c "cd /usr/local/elasticsearch-6.5.4 && nohup bin/elasticsearch &"
[root@node2 ~]# nohup: 把输出追加到"nohup.out"
```

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\5.png)

1.5 安装配置head监控插件

(1)安装node

```shell
[root@node2 ~]# exit
登出
Connection to 192.168.100.13 closed.
[root@node1 ~]# exit
登出
Connection to 192.168.100.12 closed.
[root@master ~]# wget https://npmmirror.com/mirrors/node/v14.21.3/node-v14.21.3-linux-x64.tar.xz
--2024-07-23 13:51:40--  https://npmmirror.com/mirrors/node/v14.21.3/node-v14.21.3-linux-x64.tar.xz
正在解析主机 npmmirror.com (npmmirror.com)... 47.96.233.62
正在连接 npmmirror.com (npmmirror.com)|47.96.233.62|:443... 已连接。
已发出 HTTP 请求，正在等待回应... 302 Moved Temporarily
位置：https://cdn.npmmirror.com/binaries/node/v14.21.3/node-v14.21.3-linux-x64.tar.xz [跟随至新的 URL]
--2024-07-23 13:51:41--  https://cdn.npmmirror.com/binaries/node/v14.21.3/node-v14.21.3-linux-x64.tar.xz
正在解析主机 cdn.npmmirror.com (cdn.npmmirror.com)... 183.204.241.229, 183.204.241.235, 183.204.241.233, ...
正在连接 cdn.npmmirror.com (cdn.npmmirror.com)|183.204.241.229|:443... 已连接。
已发出 HTTP 请求，正在等待回应... 200 OK
长度：22187736 (21M) [application/x-xz]
正在保存至: “node-v14.21.3-linux-x64.tar.xz”

100%[=========================================================>] 22,187,736  6.41MB/s 用时 3.7s

2024-07-23 13:51:45 (5.75 MB/s) - 已保存 “node-v14.21.3-linux-x64.tar.xz” [22187736/22187736])
[root@master ~]# tar -xf node-v14.21.3-linux-x64.tar.xz -C /usr/local/
[root@master ~]# echo 'NODE_HOME=/usr/local/node-v14.21.3-linux-x64
PATH=$NODE_HOME/bin:$PATH
export NODE_HOME PATH' >>/etc/profile.d/node.sh
[root@master ~]# source /etc/profile
[root@master ~]# node --version
v14.21.3
```

(2)下载head插件

```shell
[root@master ~]# wget https://github.com/mobz/elasticsearch-head/archive/master.zip
--2024-07-23 13:43:32--  https://github.com/mobz/elasticsearch-head/archive/master.zip
正在解析主机 github.com (github.com)... 20.205.243.166
正在连接 github.com (github.com)|20.205.243.166|:443... 已连接。
已发出 HTTP 请求，正在等待回应... 302 Found
位置：https://codeload.github.com/mobz/elasticsearch-head/zip/refs/heads/master [跟随至新的 URL]
--2024-07-23 13:43:33--  https://codeload.github.com/mobz/elasticsearch-head/zip/refs/heads/master
正在解析主机 codeload.github.com (codeload.github.com)... 20.205.243.165
正在连接 codeload.github.com (codeload.github.com)|20.205.243.165|:443... 已连接。
已发出 HTTP 请求，正在等待回应... 200 OK
长度：1357536 (1.3M) [application/zip]
正在保存至: “master.zip”

100%[=========================================================>] 1,357,536   40.0KB/s 用时 23s

2024-07-23 13:43:56 (58.6 KB/s) - 已保存 “master.zip” [1357536/1357536])

[root@master ~]# unzip -d /usr/local master.zip
```

(3)安装grunt

```shell
[root@master ~]# cd /usr/local/elasticsearch-head-master/
[root@master elasticsearch-head-master]# npm install -g cnpm -- registry=https://registry.npm.taobao.org --strict-ssl=false
[root@master elasticsearch-head-master]# cnpm install -g grunt-cli
[root@master elasticsearch-head-master]# grunt --version
grunt-cli v1.5.0
```

(4)修改head源码

```shell
[root@master elasticsearch-head-master]# vim Gruntfile.js
#95行左右
```

添加hostname，注意在上一行末尾添加逗号,hostname 不需要添加逗号

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\6.png)

(PS:

如果head和ES不在同⼀个节点， 注意将以下文件的localhost修改成ES的IP地址,如果在同⼀台机器,可以不修改

```shell
[root@master ~]# vim /usr/local/elasticsearch-head-master/_site/app.js
#第4380行左右
```

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\7.png)

)

(5)下载head必要的文件

```shell
[root@master elasticsearch-head-master]# cd
[root@master ~]# wget https://github.com/Medium/phantomjs/releases/download/v2.1.1/phantomjs-2.1.1-linux-x86_64.tar.bz2
[root@master ~]# mkdir /tmp/phantomjs/
[root@master ~]# cp phantomjs-2.1.1-linux-x86_64.tar.bz2 /tmp/phantomjs/
```

(6)运行head

```shell
[root@master ~]# cd /usr/local/elasticsearch-head-master/
[root@master elasticsearch-head-master]# yum install -y bzip2
[root@master elasticsearch-head-master]# cnpm install
✔ Installed 10 packages on /usr/local/elasticsearch-head-master
✔ All packages installed (used 23ms(network 21ms), speed 0B/s, json 0(0B), tarball 0B, manifests cache hit 0, etag hit 0 / miss 0)

[root@master elasticsearch-head-master]# nohup grunt server &
[1] 68917
[root@master elasticsearch-head-master]# nohup: 忽略输入并把输出追加到"nohup.out"
```

(7)测试

访问\<ip地址:9100>

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\8.png)

2.1 Kibana部署

节点ip:192.168.100.11

软件：kibana-6.5.4-linux-x86_64.tar.gz

​           下载地址：https://artifacts.elastic.co/downloads/kibana/kibana-6.5.4-linux-x86_64.tar.gz

 (1)安装Kibana

```shell
[root@master ~]# wget  https://artifacts.elastic.co/downloads/kibana/kibana-6.5.4-linux-x86_64.tar.gz
[root@master ~]# tar -zxf kibana-6.5.4-linux-x86_64.tar.gz -C /usr/local/
```

(2)配置

```shell
[root@master ~]# echo 'server.port: 56          #kibana服务端口，默认5601
server.host: "192.168.100.11"					#kibana主机IP地址，默认localhost
elasticsearch.url: "http://192.168.100.11:9200" #用来做查询的ES节点的URL，默认http://localhost:9200
kibana.index: ".kibana"							#kibana在Elasticsearch中使用索引来存储保存的searches, visualizations和dashboards，默认.kibana'>> /usr/local/kibana-6.5.4-linux-x86_64/config/kibana.yml
```

(PS:其他配置项可参考： https://www.elastic.co/guide/en/kibana/6.5/settings.html)

(3)启动

```shell
[root@master ~]# cd /usr/local/kibana-6.5.4-linux-x86_64/
[root@master kibana-6.5.4-linux-x86_64]# nohup ./bin/kibana &
[2] 76642
[root@master kibana-6.5.4-linux-x86_64]# nohup: 忽略输入并把输出追加到"nohup.out"
```

2.2 安装配置Nginx反向代理

(1)配置yum源

```shell
[root@master ~]# rpm -ivh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm
获取http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm
警告：/var/tmp/rpm-tmp.kLsBiF: 头V4 RSA/SHA1 Signature, 密钥 ID 7bd9bf62: NOKEY
准备中...                          ################################# [100%]
正在升级/安装...
   1:nginx-release-centos-7-0.el7.ngx ################################# [100%]
```

(2)安装

```shell
[root@master ~]# yum install -y nginx httpd-tools
```

(3)配置反向代理

```nginx
[root@master ~]# vim /etc/nginx/nginx.conf
user  nginx;
 worker_processes  4;
 error_log  /var/log/nginx/error.log;
 pid        /var/run/nginx.pid;
 worker_rlimit_nofile 65535;
 events {
    worker_connections  65535;
    use epoll;
 }
 http {
    include       mime.types;
    default_type  application/octet-stream;
    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"';
    access_log  /var/log/nginx/access.log  main;
    server_names_hash_bucket_size 128;
    autoindex on;
    sendfile        on;
    tcp_nopush     on;
    tcp_nodelay on;
    keepalive_timeout  120;
    fastcgi_connect_timeout 300;
    fastcgi_send_timeout 300;
    fastcgi_read_timeout 300;
    fastcgi_buffer_size 64k;
    fastcgi_buffers 4 64k;
    fastcgi_busy_buffers_size 128k;
    fastcgi_temp_file_write_size 128k;
 #gzip模块设置
    gzip on; #开启gzip压缩输出
    gzip_min_length 1k;    
    gzip_buffers 4 16k;    
    gzip_http_version 1.0; #压缩版本（默认1.1,前端如果是squid2.5请使⽤1.0）
    gzip_comp_level 2; #压缩等级
    gzip_types text/plain application/x-javascript 
text/css application/xml;  #压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。
    gzip_vary on;
 #开启限制IP连接数的时候需要使用
#limit_zone crawler $binary_remote_addr 10m;
 #tips:
 #upstream bakend{#定义负载均衡设备的Ip及设备状态}{
 #    ip_hash;
 #    server 127.0.0.1:9090 down;
 #    server 127.0.0.1:8080 weight=2;
 #    server 127.0.0.1:6060;
 #    server 127.0.0.1:7070 backup;
 #}
 #在需要使用负载均衡的server中增加 proxy_pass http://bakend/;
    server {
        listen       80;
        server_name  192.168.100.11;
		
		#charset koi8-r
		
 # access_log  /var/log/nginx/host.access.log 
 
		access_log off;
		
         location / {  
             auth_basic "Kibana";   #可以是string或off，任意string表示开启认证，off表示关闭认证。
             auth_basic_user_file /etc/nginx/passwd.db;   #指定存储用户名和密码的认证文件
             proxy_pass http://192.168.100.11:5601;
             proxy_set_header Host $host:5601;  
             proxy_set_header X-Real-IP $remote_addr;  
             proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  
             proxy_set_header Via "nginx";  
                     }
					 
         location /status { 
             stub_status on; #开启网站监控状态
             access_log /var/log/nginx/kibana_status.log; #监控日志
             auth_basic "NginxStatus"; 
						} 
			 
         location /head/{
             auth_basic "head";
             auth_basic_user_file /etc/nginx/passwd.db;
             proxy_pass http://192.168.100.11:9100;
             proxy_set_header Host $host:9100;
             proxy_set_header X-Real-IP $remote_addr;
             proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
             proxy_set_header Via "nginx";
                        }  
# redirect server error pages to the static page /50x.html
        error_page   
500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
 }
[root@master ~]# nginx -t
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
```

(4)配置授权用户和密码

```shell
[root@master ~]# htpasswd -cm /etc/nginx/passwd.db username
New password:
Re-type new password:
Adding password for user username
```

(5)启动nginx

```shell
[root@master ~]# systemctl start nginx
```

浏览器访问\<ip地址>

刚开始没有任何数据，会提示你创建新的索引

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\9.png)

3 部署Kafka

节点ip:192.168.100.11,192.168.100.12,192.168.100.13

软件：jdk-8u211-linux-x64.tar.gz

​           下载地址：https://pan.baidu.com/s/19Eg67jW6V5XkDAfYQ0lxhA?pwd=6r5f

​           kafka_2.11-2.0.0.tgz

​           下载地址：https://archive.apache.org/dist/kafka/2.0.0/kafka_2.11-2.0.0.tgz

3.1 安装部署jdk8

Kafka、Zookeeper（简称：ZK）运行依赖jdk8

```shell
[root@master ~]# java -version
java version "1.8.0_211"
Java(TM) SE Runtime Environment (build 1.8.0_211-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)
[root@master ~]#
```

3.2 安装配置ZK

(1)安装

```shell
[root@master ~]# wget https://archive.apache.org/dist/kafka/2.0.0/kafka_2.11-2.0.0.tgz
--2024-07-23 15:39:19--  https://archive.apache.org/dist/kafka/2.0.0/kafka_2.11-2.0.0.tgz
正在解析主机 archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2
正在连接 archive.apache.org (archive.apache.org)|65.108.204.189|:443... 已连接。
已发出 HTTP 请求，正在等待回应... 200 OK
长度：55751827 (53M) [application/x-gzip]
正在保存至: “kafka_2.11-2.0.0.tgz”

100%[=========================================================>] 55,751,827  9.67MB/s 用时 7.3s

2024-07-23 15:39:28 (7.25 MB/s) - 已保存 “kafka_2.11-2.0.0.tgz” [55751827/55751827])
[root@master ~]# tar -zxf kafka_2.11-2.0.0.tgz -C /usr/local/
```

(2)配置

```shell
[root@master ~]# sed -i 's/^[^#]/#&/' /usr/local/kafka_2.11-2.0.0/config/zookeeper.properties
[root@master ~]# echo 'dataDir=/opt/data/zookeeper/data      #ZK数据存放目录
dataLogDir=/opt/data/zookeeper/logs   #ZK日志存放目录
clientPort=2181                       #客户端连接ZK服务的端⼝
tickTime=2000                         #ZK服务器之间或客户端与服务器之间维持心跳的时间间隔
initLimit=20                          #允许follower(相对于Leaderer而言“客户端”)连接并同步到Leader的初始化连接时间，以tickTime为单位。当初始化连接时间超过该值，则表示连接失败
syncLimit=10 						  #Leader与Follower之间发送消息时，请求和应答时间长度，如果follower在设置时间内不能与leader通信，那么此follower将会被丢弃
server.1=192.168.100.11:2888:3888	  #kafka集群IP:Port
server.2=192.168.100.12:2888:3888	  #2888是follower与leader交换信息的端⼝，3888是当leader挂了时用来执行选举时服务器相互通信的端口
server.3=192.168.100.13:2888:3888'>>/usr/local/kafka_2.11-2.0.0/config/zookeeper.properties
```

创建data和log目录

```shell
[root@master ~]# mkdir -p /opt/data/zookeeper/{data,logs}
```

创建myid文件

```shell 
[root@master ~]# echo 1 > /opt/data/zookeeper/data/myid    #每台kafka机器都要做成唯⼀的ID
```

3.3 配置Kafka

(1)配置

```shell
[root@master ~]# sed -i 's/^[^#]/#&/' /usr/local/kafka_2.11-2.0.0/config/server.properties
[root@master ~]# echo '
broker.id=1                                #每个server需要单独配置broker id，如果不配置系统会自动配置，需要和上一步ID一致
listeners=PLAINTEXT://192.168.100.11:9092  #监听地址，格式PLAINTEXT://IP:端⼝
num.network.threads=3                      #接收和发送网络信息的线程数
num.io.threads=8                           #服务器用于处理请求的线程数
socket.send.buffer.bytes=102400            #套接字服务器使用的发送缓冲区(SO_SNDBUF)
socket.receive.buffer.bytes=102400         #套接字服务器使用的接收缓冲区(SO_RCVBUF)
socket.request.max.bytes=104857600         #套接字服务器将接受的请求的最大大小(防止OOM)
log.dirs=/opt/data/kafka/logs              #日志文件目录
num.partitions=6                           #partition数量
num.recovery.threads.per.data.dir=1        #在启动时恢复日志，关闭时刷盘日志每个数据目录的线程的数量，默认为1
offsets.topic.replication.factor=2         #偏移量话题的复制因子(设置更高保证可用)，为了保证有效的复制，偏移话题的复制因子是可配置的，在偏移话题的第一次请求的时候可用的broker的数量至少为复制因子的大小，否则要么话题创建失败，要么复制因子取可用broker的数量和配置复制因子的最小值
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1
log.retention.hours=168                    #日志文件删除之前保留的时间(单位小时),默认168
log.segment.bytes=536870912                #单个日志文件的大小,默认1073741824
log.retention.check.interval.ms=300000     #检查日志段以查看是否可以根据保留策略删除他们的时间间隔
zookeeper.connect=192.168.100.11:2181,192.168.100.12:2181,192.168.100.13:2181
                                           #ZK主机地址，如果zookeeper是集群则以逗号隔开
zookeeper.connection.timeout.ms=6000       #连接到Zookeeper的超时时间
group.initial.rebalance.delay.ms=0' >>/usr/local/kafka_2.11-2.0.0/config/server.properties 
```

创建log目录

```shell
[root@master ~]# mkdir -p /opt/data/kafka/logs
```

3.4 配置其他节点

配置node1节点

```shell
[root@master ~]# ssh 192.168.100.12
root@192.168.100.12's password:
Last login: Tue Jul 23 15:29:04 2024 from 192.168.100.11
[root@node1 ~]# scp 192.168.100.11:/root/kafka_2.11-2.0.0.tgz .
root@192.168.100.11's password:
kafka_2.11-2.0.0.tgz                                             100%   53MB 127.6MB/s   00:00
[root@node1 ~]# tar -zxf kafka_2.11-2.0.0.tgz -C /usr/local/
[root@node1 ~]# sed -i 's/^[^#]/#&/' /usr/local/kafka_2.11-2.0.0/config/zookeeper.properties
[root@node1 ~]# scp 192.168.100.11:/usr/local/kafka_2.11-2.0.0/config/zookeeper.properties /usr/local/kafka_2.11-2.0.0/config/zookeeper.properties
root@192.168.100.11's password:
zookeeper.properties                                             100% 1986     1.7MB/s   00:00
[root@node1 ~]# mkdir -p /opt/data/zookeeper/{data,logs}
[root@node1 ~]# echo 2 > /opt/data/zookeeper/data/myid
[root@node1 ~]# sed -i 's/^[^#]/#&/' /usr/local/kafka_2.11-2.0.0/config/server.properties
[root@node1 ~]# scp 192.168.100.11:/usr/local/kafka_2.11-2.0.0/config/server.properties /usr/local/kafka_2.11-2.0.0/config/server.properties
root@192.168.100.11's password:
server.properties                                                100% 8923     9.0MB/s   00:00
[root@node1 ~]# sed -i "s/192.168.100.11:9092/192.168.100.12:9092/g" /usr/local/kafka_2.11-2.0.0/config/server.properties
[root@node1 ~]# sed -i "s/broker.id=1/broker.id=2/g" /usr/local/kafka_2.11-2.0.0/config/server.properties
[root@node1 ~]# mkdir -p /opt/data/kafka/logs
```

配置node2节点

```shell
[root@node1 ~]# ssh 192.168.100.13
root@192.168.100.13's password:
Last login: Tue Jul 23 15:29:13 2024 from 192.168.100.12
[root@node2 ~]# scp 192.168.100.11:/root/kafka_2.11-2.0.0.tgz .
root@192.168.100.11's password:
kafka_2.11-2.0.0.tgz                                             100%   53MB 116.7MB/s   00:00
[root@node2 ~]# tar -zxf kafka_2.11-2.0.0.tgz -C /usr/local/
[root@node2 ~]# sed -i 's/^[^#]/#&/' /usr/local/kafka_2.11-2.0.0/config/zookeeper.properties
[root@node2 ~]# scp 192.168.100.11:/usr/local/kafka_2.11-2.0.0/config/zookeeper.properties /usr/local/kafka_2.11-2.0.0/config/zookeeper.properties
root@192.168.100.11's password:
zookeeper.properties                                             100% 1986     2.2MB/s   00:00
[root@node2 ~]# mkdir -p /opt/data/zookeeper/{data,logs}
[root@node2 ~]# echo 3 > /opt/data/zookeeper/data/myid
[root@node2 ~]# sed -i 's/^[^#]/#&/' /usr/local/kafka_2.11-2.0.0/config/server.properties
[root@node2 ~]# scp 192.168.100.11:/usr/local/kafka_2.11-2.0.0/config/server.properties /usr/local/kafka_2.11-2.0.0/config/server.properties
root@192.168.100.11's password:
server.properties                                                100% 8923     5.2MB/s   00:00
[root@node2 ~]# sed -i "s/192.168.100.11:9092/192.168.100.13:9092/g" /usr/local/kafka_2.11-2.0.0/config/server.properties
[root@node2 ~]# sed -i "s/broker.id=1/broker.id=3/g" /usr/local/kafka_2.11-2.0.0/config/server.properties
[root@node2 ~]# mkdir -p /opt/data/kafka/logs
```

3.5 启动，验证ZK集群

(1)启动

在三个节点依次执行

```shell
[root@master ~]# cd /usr/local/kafka_2.11-2.0.0/
[root@master kafka_2.11-2.0.0]# nohup bin/zookeeper-server-start.sh config/zookeeper.properties &
[3] 30585
[root@master kafka_2.11-2.0.0]# nohup: 忽略输入并把输出追加到"nohup.out"

[root@master kafka_2.11-2.0.0]# ssh 192.168.100.12
root@192.168.100.12's password:
Last login: Tue Jul 23 16:12:25 2024 from 192.168.100.11
[root@node1 ~]# cd /usr/local/kafka_2.11-2.0.0/
[root@node1 kafka_2.11-2.0.0]# nohup bin/zookeeper-server-start.sh config/zookeeper.properties &
[1] 1750
[root@node1 kafka_2.11-2.0.0]# nohup: 忽略输入并把输出追加到"nohup.out"

[root@node1 kafka_2.11-2.0.0]# ssh 192.168.100.13
root@192.168.100.13's password:
Last login: Tue Jul 23 16:22:42 2024 from 192.168.100.12
[root@node2 ~]# cd /usr/local/kafka_2.11-2.0.0/
[root@node2 kafka_2.11-2.0.0]# nohup bin/zookeeper-server-start.sh config/zookeeper.properties &
[1] 1682
[root@node2 kafka_2.11-2.0.0]# nohup: 忽略输入并把输出追加到"nohup.out"

[root@node2 kafka_2.11-2.0.0]# exit
登出
Connection to 192.168.100.13 closed.
[root@node1 kafka_2.11-2.0.0]# exit
登出
Connection to 192.168.100.12 closed.
```

(2)验证

查看ZK配置

```shell
[root@master kafka_2.11-2.0.0]# echo conf | nc 127.0.0.1 2181
clientPort=2181
dataDir=/opt/data/zookeeper/data/version-2
dataLogDir=/opt/data/zookeeper/logs/version-2
tickTime=2000
maxClientCnxns=60
minSessionTimeout=4000
maxSessionTimeout=40000
serverId=1
initLimit=20
syncLimit=10
electionAlg=3
electionPort=3888
quorumPort=2888
peerType=0
```

查看ZK状态

```shell
[root@master kafka_2.11-2.0.0]# echo stat | nc 127.0.0.1 2181
Zookeeper version: 3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT
Clients:
 /127.0.0.1:42710[0](queued=0,recved=1,sent=0)

Latency min/avg/max: 0/0/0
Received: 2
Sent: 1
Connections: 1
Outstanding: 0
Zxid: 0x0
Mode: follower
Node count: 4
```

查看端⼝

```shell
[root@master kafka_2.11-2.0.0]# lsof -i:2181
COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
java    30585 root   96u  IPv6  59626      0t0  TCP *:eforward (LISTEN)
```

3.6 启动验证KafKa

(1)启动

在三个节点依次执行

```shell
[root@master kafka_2.11-2.0.0]# nohup bin/kafka-server-start.sh config/server.properties &
[4] 37453
[root@master kafka_2.11-2.0.0]# nohup: 忽略输入并把输出追加到"nohup.out"

[root@master kafka_2.11-2.0.0]# ssh 192.168.100.12
root@192.168.100.12's password:
Last login: Tue Jul 23 16:37:35 2024 from 192.168.100.11
[root@node1 ~]# cd /usr/local/kafka_2.11-2.0.0/
[root@node1 kafka_2.11-2.0.0]# nohup bin/kafka-server-start.sh config/server.properties &
[1] 2084
[root@node1 kafka_2.11-2.0.0]# nohup: 忽略输入并把输出追加到"nohup.out"

[root@node1 kafka_2.11-2.0.0]# ssh 192.168.100.13
root@192.168.100.13's password:
Last login: Tue Jul 23 16:38:54 2024 from 192.168.100.12
[root@node2 ~]# cd /usr/local/kafka_2.11-2.0.0/
[root@node2 kafka_2.11-2.0.0]# nohup bin/kafka-server-start.sh config/server.properties &
[1] 2008
[root@node2 kafka_2.11-2.0.0]# nohup: 忽略输入并把输出追加到"nohup.out"

[root@node2 kafka_2.11-2.0.0]# exit
登出
Connection to 192.168.100.13 closed.
[root@node1 kafka_2.11-2.0.0]# exit
登出
Connection to 192.168.100.12 closed.
[root@master kafka_2.11-2.0.0]#
```

(2)验证

在192.168.100.11上创建topic

```shell
[root@master kafka_2.11-2.0.0]# bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic testtopic
Created topic "testtopic".
```

查询192.168.100.11上的topic

```shell
[root@master kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper 192.168.100.11:2181 --list
testtopic
```

查询192.168.100.12上的topic

```shell
[root@master kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper 192.168.100.12:2181 --list
testtopic
```

查询192.168.100.13上的topic

```shell
[root@master kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper 192.168.100.13:2181 --list
testtopic
```

模拟消息生产和消费

发送信息到192.168.100.11

```shell
[root@master kafka_2.11-2.0.0]# bin/kafka-console-producer.sh --broker-list 192.168.100.11:9092 --topic testtopic
>Hello World!
```

从192.168.100.12上接收消息

```shell
[root@node1 ~]# cd /usr/local/kafka_2.11-2.0.0/
[root@node1 kafka_2.11-2.0.0]# bin/kafka-console-consumer.sh --bootstrap-server 192.168.100.12:9092 --topic testtopic --from-beginning
Hello World!
```

3.7 部署监控Kafka Manager

节点ip:192.168.100.11

软件：kafka-manager-1.3.3.7.zip

​           下载地址：https://pan.baidu.com/share/init?surl=qYifoa4&pwd=el4o

安装

```shell
[root@master ~]# unzip kafka-manager-1.3.3.7.zip 
[root@master ~]# mv kafka-manager-1.3.3.7 /usr/local/
[root@master ~]# cd /usr/local/kafka-manager-1.3.3.7
[root@master kafka-manager-1.3.3.7]# vim conf/application.conf
```

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\10.png)

启动Kafka-manager

```shell
[root@master kafka-manager-1.3.3.7]# nohup bin/kafka-manager -Dconfig.file=conf/application.conf &
[5] 72185
[root@master kafka-manager-1.3.3.7]# nohup: 忽略输入并把输出追加到"nohup.out"
```

浏览器访问\<ip地址:9000>

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\11.png)

点击顶部Cluster选择Add Cluster

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\12.png)

做基础配置
![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\13.png)

保存

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\14.png)

点击刚才创建的集群

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\15.png)

我们点击Topic的2,此时我们可以查看到之前创建的testtopic

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\16.png)

如果遇到 Kafka 消费不及时的话，可以通过到具体 cluster页面上，增加 partition。Kafka 通过 partition 分区来提⾼并发消费速度

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\17.png)

4.部署Logstash

节点ip:192.168.100.13

软件：logstash-6.5.4.tar.gz

​           下载地址：https://artifacts.elastic.co/downloads/logstash/logstash-6.5.4.tar.gz

(1)安装

```shell
[root@node3 kafka-manager-1.3.3.7]# cd
[root@node3 ~]# wget https://artifacts.elastic.co/downloads/logstash/logstash-6.5.4.tar.gz
[root@node3 ~]# tar -zxf logstash-6.5.4.tar.gz -C /usr/local/
```

(2)配置

```shell
[root@node3 ~]# mkdir -p /usr/local/logstash-6.5.4/etc/conf.d
[root@node3 ~]# vim /usr/local/logstash-6.5.4/etc/conf.d/input.conf
input {
kafka {
    type => "nginx_kafka"
    codec => "json"
    topics => "nginx"
    decorate_events => true
    bootstrap_servers => "192.168.100.11:9092,192.168.100.12:9092,192.168.100.13:9092"
  }
}
```

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\18.png)

```shell
[root@node3 ~]# vim /usr/local/logstash-6.5.4/etc/conf.d/output.conf
output {
 if [type] == "nginx_kafka" {
      elasticsearch {
      hosts => ["192.168.100.11","192.168.100.12","192.168.100.13"]
      index => 'logstash-nginx-%{+YYYY-MM-dd}'
      }
    }
  }

```

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\19.png)

(3)启动

```shell
[root@node3 ~]# cd /usr/local/logstash-6.5.4
[root@node3 logstash-6.5.4]# bin/logstash -f etc/conf.d/ --config.reload.automatic
```

5.部署Filebeat

(为什么用Filebeat，而不用原来的 Logstash 呢？原因很简单，资源消耗比较大)

节点ip:192.168.100.13

软件：filebeat-6.5.4-linux-x86_64.tar.gz

​           下载地址：https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.5.4-linux-x86_64.tar.gz

(1)安装

```shell
[root@node3 ~]# wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.5.4-linux-x86_64.tar.gz
[root@node3 ~]# tar -zxf filebeat-6.5.4-linux-x86_64.tar.gz -C /usr/local/
[root@node3 ~]# cd /usr/local/
[root@node3 local]# mv filebeat-6.5.4-linux-x86_64/ filebeat
[root@node3 local]# cd filebeat/
```

(2)配置

```shell
[root@node3 filebeat]# mv filebeat.yml filebeat-yml.bak
[root@node3 filebeat]# vim filebeat.yml
filebeat.prospectors:
- input_type: log
  paths:
    -  /opt/logs/server/nginx.log
  json.keys_under_root: true        #keys_under_root可以让字段位于根节点，默认为false
  json.add_error_key: true          #将解析错误的消息记录储存在error.message字段中
  json.message_key: log             #message_key是⽤来合并多⾏json⽇志使⽤
output.kafka:
  hosts: [ "192.168.100.11:9092","192.168.100.12:9092","192.168.100.13:9092" ]
  topic: 'nginx'
```

![](C:\Users\Hu\Desktop\blog\7.企业级日志中心\7\20.png)

(3)启动

```shell
[root@node3 filebeat]# nohup ./filebeat -e -c filebeat.yml &
```

