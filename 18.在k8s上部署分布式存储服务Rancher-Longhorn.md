#### 18.在k8s上部署分布式存储服务Rancher-Longhorn

在分布式存储服务的上下文中，Rancher 和 Longhorn 是两个相关但不同的组件。

##### Rancher

Rancher 是一个开源的容器管理平台，用于简化 Kubernetes 集群的部署、管理和维护。它提供了一种直观的用户界面，使得用户可以轻松地管理多个 Kubernetes 集群，并提供了许多功能，如监控、日志管理、权限管理等。Rancher 支持多种基础设施，不论是公有云、私有云还是本地数据中心。

##### Longhorn

Longhorn 是一个开源的分布式块存储解决方案，专为 Kubernetes 环境设计。它允许用户在 Kubernetes 集群中创建和管理持久性存储卷，支持快照、克隆和备份等功能。Longhorn 提供高可用性和数据冗余，确保即使在节点故障的情况下数据也能得到保护。

##### 关系与联系

结合使用：Rancher 和 Longhorn 可以结合使用，以提供完整的容器编排和存储管理解决方案。通过在 Rancher 中部署 Longhorn，用户可以方便地为他们的 Kubernetes 应用程序提供持久化存储。

集成管理：在 Rancher 的界面中，可以直接管理 Longhorn 存储卷，这样用户就能够在一个统一的界面中处理容器和存储的问题，提升了管理的便利性。

综上，Rancher 是一个容器管理平台，而 Longhorn 是一个用于 Kubernetes 的分布式存储解决方案。两者结合使用，可以大大简化 Kubernetes 环境中的存储管理，利于在Kubernetes环境中轻松、快速和可靠地部署高可用性持久化块存储

下面，我们来单独聊聊Longhorn

##### 什么是Longhorn

Longhorn是一个轻量级、可靠且易于使用的Kubernetes分布式块存储系统。(官方文档:https://longhorn.io/docs/1.7.0/)使用 Longhorn，您可以：

```
1.使用 Longhorn 卷作为 Kubernetes 集群中分布式有状态应用程序的持久存储
2.将块存储分区为 Longhorn 卷，以便您可以在有或没有云提供商的情况下使用 Kubernetes 卷
3.跨多个节点和数据中心复制块存储以提高可用性
4.将备份数据存储在外部存储中，例如 NFS 或 AWS S3
5.创建跨集群灾难恢复卷，以便主 Kubernetes 集群中的数据可以从第二个 Kubernetes 集群中的备份快速恢复
6.计划卷的定期快照，并计划定期备份到 NFS 或 S3 兼容的辅助存储
7.从备份恢复卷
8.在不中断持久卷的情况下升级 Longhorn
9.Longhorn 带有独立的 UI，可以使用 Helm、kubectl 或 Rancher 应用程序目录进行安装。
```

##### Longhorn的底层存储协议:

iSCSI（Internet Small Computer Systems Interface）是一种网络协议，用于在 TCP/IP 网络上传输 SCSI 命令，允许两台计算机进行远程存储和检索操作。iSCSI 是一种流行的存储区域网络（SAN）技术，广泛用于连接存储设备，如磁盘阵列和磁带库，与服务器和数据中心。

下面，我们就在K8S上部署Longhorn（使用虚拟机的方式部署的K8S集群）

1.在集群所有节点的主机上添加两块硬盘

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\1.png)

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\2.png)

2.格式化新添加的磁盘

```shell
mkfs.ext4 /dev/sdb
mkfs.ext4 /dev/sdc
```

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\3.png)

3.创建挂载点目录

```shell
mkdir /mnt/longhorn-sdb && mkdir /mnt/longhorn-sdc
```

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\4.png)

4.挂载磁盘并设置开机自动挂载

```shell
echo "/dev/sdb /mnt/longhorn-sdb ext4 defaults 0 1" >> /etc/fstab && echo "/dev/sdc /mnt/longhorn-sdc ext4 defaults 0 1" >> /etc/fstab
```

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\5.png)

```shell
mount -a && df -Th|grep -E 'longhorn-sdb|longhorn-sdc'
```

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\6.png)

5.配置k8s集群中的节点信息(所有集群中节点的配置信息都要修改)

```yaml
# kubectl edit node <节点名称>

metadata:
labels:
    node.longhorn.io/create-default-disk: "config"
annotations:
    node.longhorn.io/default-disks-config: '[
    {
        "path":"/mnt/longhorn-sdb",
        "allowScheduling":true
    },
    {    
        "path":"/mnt/longhorn-sdc",
        "allowScheduling":true
    }
    ]'
```

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\7.png)

6.利用helm安装longhorn服务

```shell
[root@k8s-master-1]# wget https://github.com/longhorn/longhorn/archive/refs/tags/v1.5.3.zip
[root@k8s-master-1 ~]# unzip v1.5.3.zip && rm -rf v1.5.3.zip && cd longhorn-1.5.3
[root@k8s-master-1 longhorn-1.5.3]# helm install longhorn ./chart/ --namespace longhorn-system --create-namespace --set defaultSettings.createDefaultDiskLabeledNodes=true
NAME: longhorn
LAST DEPLOYED: Tue Sep  3 00:00:39 2024
NAMESPACE: longhorn-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Longhorn is now installed on the cluster!

Please wait a few minutes for other Longhorn components such as CSI deployments, Engine Images, and Instance Managers to be initialized.

Visit our documentation at https://longhorn.io/docs/
[root@k8s-master-1 longhorn-1.5.3]# kubectl -n longhorn-system get pod -o wide
```

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\11.png)

（PS:

此时可能会出现如下现象：

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\8.png)

我们使用`kubectl describe -n longhorn-system pod longhorn-manager-rnf9l`命令查看该pod的event事件，发现该容器启动后就直接关闭了

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\9.png)

我们查看该pod的日志

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\10.png)

根据提供的日志信息，Longhorn 管理器启动失败，原因是缺少 `iscsiadm` 或 `open-iscsi` 工具，只需要在系统中安装该包即可

```shell
yum install iscsi-initiator-utils
```

此时我们重启这两个pod，服务正常启动

）

安装完longhorn服务后会自动创建一个默认的sc

```shell
kubectl get sc
```

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\12.png)

此时我们可以通过修改longhorn服务的service的配置文件的方式来实现ui界面的访问（再实际生产环境中建议配置一个ingress）

```shell
[root@k8s-master-1 ~]# kubectl get svc -n longhorn-system
NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
longhorn-admission-webhook    ClusterIP   172.17.207.8     <none>        9502/TCP   23m
longhorn-backend              ClusterIP   172.17.8.247     <none>        9500/TCP   23m
longhorn-conversion-webhook   ClusterIP   172.17.72.165    <none>        9501/TCP   23m
longhorn-engine-manager       ClusterIP   None             <none>        <none>     23m
longhorn-frontend             ClusterIP   172.17.142.205   <none>        80/TCP     23m
longhorn-recovery-backend     ClusterIP   172.17.228.64    <none>        9503/TCP   23m
longhorn-replica-manager      ClusterIP   None             <none>        <none>     23m
[root@k8s-master-1 ~]# kubectl edit -n longhorn-system svc longhorn-frontend
```

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\13.png)

此时我们就可以通过该主机的IP地址加映射的端口来访问longhorn的ui界面了

```shell
[root@k8s-master-1 ~]# kubectl get svc -n longhorn-system
NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
longhorn-admission-webhook    ClusterIP   172.17.207.8     <none>        9502/TCP       25m
longhorn-backend              ClusterIP   172.17.8.247     <none>        9500/TCP       25m
longhorn-conversion-webhook   ClusterIP   172.17.72.165    <none>        9501/TCP       25m
longhorn-engine-manager       ClusterIP   None             <none>        <none>         25m
longhorn-frontend             NodePort    172.17.142.205   <none>        80:31687/TCP   25m
longhorn-recovery-backend     ClusterIP   172.17.228.64    <none>        9503/TCP       25m
longhorn-replica-manager      ClusterIP   None             <none>        <none>         25m
```

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\14.png)

(PS:之所以只有40G存储和两个节点是因为我的`k8s-master-1`、`k8s-master-2` 和 `k8s-master-3` 节点的角色都是 `control-plane,master`

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\15.png)

这意味着它们是 Kubernetes 的控制平面节点。在 Kubernetes 中，控制平面节点通常会被标记为不可调度，以确保所有的管理任务和 API 服务器等可以正常运行，这种配置是为了保持控制平面的稳定性和性能

如果希望在 master 节点上也可以调度 Pod，可以通过 `kubectl taint` 命令来移除 master 节点的不可调度标志

```shell
kubectl taint nodes k8s-master-1 node-role.kubernetes.io/master:NoSchedule-
kubectl taint nodes k8s-master-2 node-role.kubernetes.io/master:NoSchedule-
kubectl taint nodes k8s-master-3 node-role.kubernetes.io/master:NoSchedule-
```

此时longhorn会在master节点部署pod

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\16.png)

)

测试：

```yaml
# cat test.yaml

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-claim
spec:
  storageClassName: longhorn
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Mi   # 实测最小存储分配为10Mi
---
kind: Pod
apiVersion: v1
metadata:
  name: test
spec:
  containers:
  - name: test
    image: registry.cn-shanghai.aliyuncs.com/acs/busybox:v1.29.2
    imagePullPolicy: IfNotPresent
    command:
      - "/bin/sh"
    args:
      - "-c"
      - "echo 'hello k8s' > /mnt/SUCCESS && sleep 36000 || exit 1"
    volumeMounts:
      - name: longhorn-pvc
        mountPath: "/mnt"
  restartPolicy: "Never"
  volumes:
    - name: longhorn-pvc
      persistentVolumeClaim:
        claimName: test-claim
```

部署我们的服务，此时我们查看pvc，发现成功挂载到了longhorn

```shell
[root@k8s-master-1 ~]# kubectl apply -f test.yaml
persistentvolumeclaim/test-claim created
pod/test created
[root@k8s-master-1 ~]# kubectl get pvc
NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
test-claim   Bound    pvc-4280c520-bb7f-4928-ad81-8490ce49c879   10Mi       RWX            longhorn       89s
```

我们也可以通过web界面查看，可以看到已经显示成功创建了一个Volume

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\17.png)

因为是分布式存储，所以他的挂载点在多个node节点上

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\18.png)

(PS:

如果创建的服务pod一直卡在容器创建状态

```shell
[root@k8s-master-1 ~]# kubectl get pod
NAME   READY   STATUS              RESTARTS   AGE
test   0/1     ContainerCreating   0          7m36s
```

我们使用`kubectl describe`命令来查看他的事件发现是在Kubernetes 尝试挂载 Longhorn 的持久卷时遇到了问题

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\19.png)

我们只需要在节点上安装 NFS 客户端并删除这个pod重新创建即可

```shell
[root@k8s-master-1 ~]# yum install nfs-utils -y
[root@k8s-master-1 ~]# kubectl delete pod test --force
pod "test" deleted
[root@k8s-master-1 ~]# kubectl apply -f test.yaml
persistentvolumeclaim/test-claim unchanged
pod/test created
[root@k8s-master-1 ~]# kubectl get pod
NAME   READY   STATUS    RESTARTS   AGE
test   1/1     Running   0          21s
```

）

此时我们进入到pod中并查看该pod的磁盘挂载情况，发现该磁盘已经成功被挂载到了/mnt目录上

```shell
[root@k8s-master-1 ~]# kubectl exec -it test -- sh
/ # df -Th
```

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\20.png)

我们进入/mnt目录，可以看到有个SUCCESS文件，查看该文件，里边正是test.yaml中配置的内容，说明

挂载存储成功

```shell
/ # cd /mnt/
/mnt # ls
SUCCESS     lost+found
/mnt # cat SUCCESS
hello k8s
```

(PS:longhorn是写入严格限制大小的，因此无法创建超过限制大小的数据，我们使用dd命令创建一个11M大小的文件，可以看到会直接提示存储空间不足

```shell
/mnt # dd if=/dev/zero of=./test.log bs=1M count=11
dd: ./test.log: No space left on device
/mnt # du -h ./*
1.0K    ./SUCCESS
12.0K   ./lost+found
8.0M    ./test.log
```

)

我们另外部署Rancher(建议部署Rancher的主机有至少6G的内存)

1.运行server

在主机上执行以下Docker命令

```shell
docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher:stable
```

2.打开浏览器，输入https://<安装容器的IP地址>，即可以访问Rancher Server的UI

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\21.png)

3.我们根据提示输入密码后登陆

```shell
[root@k8s-master-1 ~]# docker ps | grep rancher
c25e24894d4b   rancher/rancher:stable                                          "entrypoint.sh"          21 minutes ago   Up 21 minutes   0.0.0.0:80->80/tcp, :::80->80/tcp, 0.0.0.0:443->443/tcp, :::443->443/tcp   practical_gauss
[root@k8s-master-1 ~]# docker logs c25e24894d4b 2>&1 | grep "Bootstrap Password:"
2024/09/03 06:16:33 [INFO] Bootstrap Password: qjbshv2pdwh9shdpx6p6nrfwwbrmddhjzgrclhk2nwm88vt7b9qmf9
```

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\22.png)

4.选择“导入已有集群”导入我们的K8S集群

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\23.png)

5.输入名称后点击创建

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\24.png)

6.根据提示将我们本地的K8S集群导入到 Rancher 中

```shell
[root@k8s-master-1 ~]# curl --insecure -sfL https://192.168.100.31/v3/import/b7stnl9kcdngn2r5j2zxxm8cmxtdvvlw847k6f9sjgf8r662b49d9h_c-m-qbp6mml9.yaml | kubectl apply -f -
clusterrole.rbac.authorization.k8s.io/proxy-clusterrole-kubeapiserver created
clusterrolebinding.rbac.authorization.k8s.io/proxy-role-binding-kubernetes-master created
namespace/cattle-system created
serviceaccount/cattle created
clusterrolebinding.rbac.authorization.k8s.io/cattle-admin-binding created
secret/cattle-credentials-4bb4eab created
clusterrole.rbac.authorization.k8s.io/cattle-admin created
deployment.apps/cattle-cluster-agent created
service/cattle-cluster-agent created
```

7.等待pod全部创建成功

```shell
[root@k8s-master-1 ~]# kubectl get pod -n cattle-system
NAME                                    READY   STATUS      RESTARTS   AGE
cattle-cluster-agent-57dcc5d78c-nh8dt   1/1     Running     0          2m50s
cattle-cluster-agent-57dcc5d78c-twtkc   1/1     Running     0          4m18s
helm-operation-t55v9                    0/2     Completed   0          2m39s
rancher-webhook-d6d4d686-5x49n          1/1     Running     0          2m5s
```

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\25.png)

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\29.png)

此时我们选择左侧的Longhorn即可前往Longhorn的UI界面

（PS：

如下所示

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\26.png)

rancher警告组件Scheduler和Controller Manager不健康，这是因为 kube-controller-manager和kube-scheduler没有开启监控端口，可以在`/etc/kubernetes/manifests/kube-controller-manager.yaml`和`/etc/kubernetes/manifests/kube-scheduler.yaml`文件中注解此端口(- --port=0)，然后重启kubelete 即可显示正常

(注：图片中的注释是代指删除的意思，实际在下述文件中不允许使用注释，会导致集群崩溃！！！)

kube-controller-manager.yaml：

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\27.png)

kube-scheduler.yaml：

![](C:\Users\Hu\Desktop\Notes\18.在k8s上部署分布式存储服务Rancher-Longhorn\18\28.png)

）
