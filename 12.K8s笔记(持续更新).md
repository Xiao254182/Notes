# K8s笔记

## 1.docker和containerd的命令对比

| 镜像相关功能     | Docker                    | Containerd                                    |
| ---------------- | ------------------------- | --------------------------------------------- |
| 显示本地镜像列表 | docker images             | ctr -n \<namespace> images ls                 |
| 下载镜像         | docker pull               | ctr -n \<namespace>  images pull              |
| 上传镜像         | docker push               | ctr -n \<namespace>  images push              |
| 删除本地镜像     | docker rmi                | ctr -n \<namespace> delete                    |
| 查看镜像详情     | docker inspect IMAGE-ID   | crictl inspecti IMAGE-ID                      |
| 导出离线镜像     | docker save xxx > xxx.tar | ctr -n \<namespace> images export xxx.tar xxx |
| 导入离线镜像     | docker load -i xxx.tar    | ctr -n \<namespace> images import xxx.tar     |

| 容器相关功能 | Docker         | Containerd     |
| ------------ | -------------- | -------------- |
| 显示容器列表 | docker ps      | crictl ps      |
| 创建容器     | docker create  | crictl create  |
| 启动容器     | docker start   | crictl start   |
| 停止容器     | docker stop    | crictl stop    |
| 删除容器     | docker rm      | crictl rm      |
| 查看容器详情 | docker inspect | crictl inspect |
| attach       | docker attach  | crictl attach  |
| exec         | docker exec    | crictl exec    |
| 查看日志     | docker logs    | crictl logs    |
| 查看状态     | docker stats   | crictl stats   |

Containerd新增了管理pod的功能，但一般不常用

| POD 相关功能  | Containerd      |
| ------------- | --------------- |
| 显示 POD 列表 | crictl pods     |
| 查看 POD 详情 | crictl inspectp |
| 运行 POD      | crictl runp     |
| 停止 POD      | crictl stopp    |

## 2.k8s删除namespaces状态一直为terminating问题处理

```shell
# kubectl get ns
NAME              STATUS        AGE
default           Active        5d4h
ingress-nginx     Active        30h
kube-node-lease   Active        5d4h
kube-public       Active        5d4h
kube-system       Active        5d4h
kubevirt          Terminating   2d2h   # <------ here

1、新开一个窗口运行命令  kubectl proxy
> 此命令启动了一个代理服务来接收来自你本机的HTTP连接并转发至API服务器，同时处理身份认证

2、新开一个终端窗口，将下面shell脚本整理到文本内`1.sh`并执行，$1参数即为删除不了的ns名称
#------------------------------------------------------------------------------------
#!/bin/bash

set -eo pipefail

die() { echo "$*" 1>&2 ; exit 1; }

need() {
        which "$1" &>/dev/null || die "Binary '$1' is missing but required"
}

# checking pre-reqs

need "jq"
need "curl"
need "kubectl"

PROJECT="$1"
shift

test -n "$PROJECT" || die "Missing arguments: kill-ns <namespace>"

kubectl proxy &>/dev/null &
PROXY_PID=$!
killproxy () {
        kill $PROXY_PID
}
trap killproxy EXIT

sleep 1 # give the proxy a second

kubectl get namespace "$PROJECT" -o json | jq 'del(.spec.finalizers[] | select("kubernetes"))' | curl -s -k -H "Content-Type: application/json" -X PUT -o /dev/null --data-binary @- http://localhost:8001/api/v1/namespaces/$PROJECT/finalize && echo "Killed namespace: $PROJECT"
#------------------------------------------------------------------------------------

3. 执行脚本删除
# bash 1.sh kubevirt
Killed namespace: kubevirt
1.sh: line 23: kill: (9098) - No such process

5、查看结果
# kubectl get ns    
NAME              STATUS   AGE
default           Active   5d4h
ingress-nginx     Active   30h
kube-node-lease   Active   5d4h
kube-public       Active   5d4h
kube-system       Active   5d4h
```

## 3.扩容pod的数量从而达到负载均衡并实现版本回滚

```shell
[root@k8s-master-node1 ~]# kubectl get deployments.apps
No resources found in default namespace.

#创建一个deployment，引用nginx的服务镜像，这里的副本数量默认是1
[root@k8s-master-node1 ~]# kubectl create deployment nginx --image=swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.21
deployment.apps/nginx created

# 查看创建结果
[root@k8s-master-node1 ~]# kubectl get pod
NAME                     READY   STATUS    RESTARTS   AGE
nginx-6dc5786599-b4m92   1/1     Running   0          47s

# 扩容pod的数量(--replicas=2表示扩容到2个pod)
[root@k8s-master-node1 ~]# kubectl scale deployment nginx --replicas=2
deployment.apps/nginx scaled

# 查看创建结果
[root@k8s-master-node1 ~]# kubectl get pod
NAME                     READY   STATUS    RESTARTS   AGE
nginx-6dc5786599-2v2kc   1/1     Running   0          3s
nginx-6dc5786599-b4m92   1/1     Running   0          57s

# 我们先来创建一个service
[root@k8s-master-node1 ~]# kubectl create service clusterip nginx --tcp=80:80
service/nginx created
[root@k8s-master-node1 ~]# kubectl get svc -o wide
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE     SELECTOR
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   11d     <none>
nginx        ClusterIP   10.96.130.127   <none>        80/TCP    7m54s   app=nginx

# 我们进入容器修改nginx默认首页的内容，方便后期测试
[root@k8s-master-node1 ~]# kubectl exec -it nginx-6dc5786599-2v2kc -- bash
root@nginx-6dc5786599-2v2kc:/# echo "111" > /usr/share/nginx/html/index.html
root@nginx-6dc5786599-2v2kc:/# exit
exit
[root@k8s-master-node1 ~]# kubectl exec -it nginx-6dc5786599-b4m92 -- bash
root@nginx-6dc5786599-b4m92:/# echo "222" > /usr/share/nginx/html/index.html
root@nginx-6dc5786599-b4m92:/# exit
exit

# 此时我们循环访问service，发现service已经实现了内部的负载均衡
[root@k8s-master-node1 ~]# while sleep 1;do curl 10.96.130.127 ; done
111
222
111
222
^C

#我们去访问一个nginx服务不存在的路径，根据输出可以看到版本号是nginx/1.21
[root@k8s-master-node1 ~]# curl 10.96.130.127/1
<html>
<head><title>404 Not Found</title></head>
<body>
<center><h1>404 Not Found</h1></center>
<hr><center>nginx/1.21.6</center>
</body>
</html>

#先查看一下本地有哪些镜像，可以看到有nginx1.21和1.27两个版本，我们这里模拟服务发版操作，假设我们需要用nginx1.27版本来替换1.21版本，就要在deployment中替换镜像
[root@k8s-master-node1 ~]# docker images | grep nginx
swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx   1.27      dde0cca083bc   2 months ago   188MB
swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx   1.21    0e901e68141f   2 years ago    142MB
goharbor/nginx-photon                                      v2.3.4    2218fcda1ff0   2 years ago    45MB

# 我们可以用--help去查看如何替换镜像
```

![](https://github.com/Xiao254182/notes/blob/master/img/12/1.png)

```shell
# 此时我们开始替换镜像。注意命令最后面的 `--record` 参数，这个在生产中作为资源创建更新用来回滚的重要标记
[root@k8s-master-node1 ~]# kubectl set image deployment/nginx  nginx=swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.27 --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/nginx image updated

# 观察下pod的信息，可以看到旧nginx的2个pod逐渐被新的pod一个一个的替换掉
[root@k8s-master-node1 ~]# kubectl  get pod -w
NAME                     READY   STATUS              RESTARTS   AGE
nginx-6dc5786599-2v2kc   1/1     Running             0          37m
nginx-97c95f4c7-ljwgk    0/1     ContainerCreating   0          22s
nginx-97c95f4c7-pfxn2    1/1     Running             0          24s
nginx-97c95f4c7-ljwgk    1/1     Running             0          22s
nginx-6dc5786599-2v2kc   1/1     Terminating         0          37m
nginx-6dc5786599-2v2kc   0/1     Terminating         0          37m
nginx-6dc5786599-2v2kc   0/1     Terminating         0          37m
nginx-6dc5786599-2v2kc   0/1     Terminating         0          37m

# 此时我们再次访问service服务地址，发现nginx版本变为了1.27，说明我们更新成功，服务发版完成
[root@k8s-master-node1 ~]# curl 10.96.130.127/1
<html>
<head><title>404 Not Found</title></head>
<body>
<center><h1>404 Not Found</h1></center>
<hr><center>nginx/1.27.0</center>
</body>
</html>

# 接下来我们进行快速回滚操作，我们还是先回到nginx1.21版本，然后发版1.27版本
[root@k8s-master-node1 ~]# kubectl set image deployment/nginx  nginx=swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.21 --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/nginx image updated
[root@k8s-master-node1 ~]# kubectl set image deployment/nginx  nginx=swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.27 --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/nginx image updated
[root@k8s-master-node1 ~]# curl 10.96.130.127/1
<html>
<head><title>404 Not Found</title></head>
<body>
<center><h1>404 Not Found</h1></center>
<hr><center>nginx/1.27.0</center>
</body>
</html>

#假设我们发现nginx1.27.0版本有问题，需要快速回滚到上一个版本(即1.21.6版本)，还记得我们上面提到的 --record  参数嘛，这里它就会发挥很重要的作用了，我们使用如下命令可以看到当前历史版本情况，只有接了`--record`参数的命令操作才会有详细的记录，因此建议在生产中一定要加上`--record`参数
[root@k8s-master-node1 ~]# kubectl rollout history deployment nginx
deployment.apps/nginx
REVISION  CHANGE-CAUSE
1         <none>
3         kubectl set image deployment/nginx nginx=swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.21 --record=true
4         kubectl set image deployment/nginx nginx=swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.27 --record=true

# 根据历史发布版本前面的阿拉伯数字序号来选择回滚版本，这里我们回到上个版本号，也就是选择3，执行命令如下
[root@k8s-master-node1 ~]# kubectl rollout undo deployment nginx --to-revision=3
deployment.apps/nginx rolled back

# 等一会pod更新完成后，再次访问service，此时发现已经回滚完成
[root@k8s-master-node1 ~]# curl 10.96.130.127/1
<html>
<head><title>404 Not Found</title></head>
<body>
<center><h1>404 Not Found</h1></center>
<hr><center>nginx/1.21.6</center>
</body>
</html>
```

## 4.K8s的健康检查

对线上业务来说，保证服务的正常稳定是重中之重，对故障服务的及时处理避免影响业务以及快速恢复一直是开发运维的难点。Kubernetes提供了健康检查服务，对于检测到故障服务会被及时自动下线，以及通过重启服务的方式使服务自动恢复。

（PS：容器重启策略

| 重启策略  | 说明                                                   |
| --------- | ------------------------------------------------------ |
| Always    | 当容器失效时，由kubelet自动重启该容器                  |
| OnFailure | 当容器终止运行且退出码不为0时，由kubelet自动重启该容器 |
| Never     | 当容器终止运行且退出码不为0时，由kubelet自动重启该容器 |

）

Kubelet通过调用Pod中容器的Handler来执行检查的动作，Handler有三种类型。

```
ExecAction：在容器中执行特定的命令，命令退出返回0表示成功；
TCPSocketAction：根据容器IP地址及特定的端口进行TCP检查，端口开放表示成功；
HTTPGetAction：根据容器IP、端口及访问路径发起一次HTTP请求，如果返回码在200到400之间表示成功。
```

每种检查动作都可能有三种返回状态：

```
Success：表示通过了健康检查；
Failure：表示没有通过健康检查；
Unknown：表示检查动作失败。
```

在创建Pod时，可以通过liveness和readiness两种方式来探测Pod内容器的运行情况。

Kubelet使用liveness probe（存活探针）来确定何时重启容器。例如，当应用程序处于运行状态但无法做进一步操作，liveness探针将捕获到deadlock，重启处于该状态下的容器，使应用程序在存在bug的情况下依然能够继续运行下去

Kubelet使用readiness probe（就绪探针）来确定容器是否已经就绪可以接受流量。只有当Pod中的容器都处于就绪状态时kubelet才会认定该Pod处于就绪状态。该信号的作用是控制哪些Pod应该作为service的后端。如果Pod处于非就绪状态，那么它们将会被从service的load balancer中移除。

下面对 Liveness 检测和 Readiness 检测做个比较：

```
1.Liveness 检测和 Readiness 检测是两种 Health Check 机制，如果不特意配置，Kubernetes 将对两种检测采取相同的默认行为，即通过判断容器启动进程的返回值是否为零来判断检测是否成功。
2.两种检测的配置方法完全一样，支持的配置参数也一样。不同之处在于检测失败后的行为：Liveness 检测是重启容器；Readiness 检测则是将容器设置为不可用，不接收 Service 转发的请求。
3.Liveness 检测和 Readiness 检测是独立执行的，二者之间没有依赖，所以可以单独使用，也可以同时使用。用 Liveness 检测判断容器是否需要重启以实现自愈；用 Readiness 检测判断容器是否已经准备好对外提供服务。
```

### 1.Liveness

#### （1）定义liveness命令

许多长时间运行的应用程序最终会转换到broken状态，除非重新启动，否则无法恢复。Kubernetes提供了liveness probe来检测和补救这种情况。

我们首先基于busybox镜像创建一个容器pod

```yaml
#exec-liveness.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    image: 
    imagePullPolicy: IfNotPresent
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5   #表示在容器启动后等待多长时间才开始进行第一个活跃性探测
      periodSeconds: 5    #两次活跃性探测之间的间隔时间
```

```
initialDelaySeconds:在容器启动后等待多长时间才开始进行第一个活跃性探测，允许应用程序在启动过程中有一些时间来完成初始化过程，从而避免在还未准备好的情况下被误判为不健康
periodSeconds:如果上一次探测成功，下次探测将在多长时间后执行。
```

该配置文件给Pod配置了一个容器。periodSeconds规定kubelet要每隔5秒执行一次liveness probe。initialDelaySeconds 规定kubelet在第一次执行probe之前要的等待5秒钟。探针检测命令是在容器中执行cat /tmp/healthy命令。如果命令执行成功，将返回0，kubelet就会认为该容器是活着的并且很健康。如果返回非0值，kubelet就会杀掉这个容器并重启它。

容器启动时，执行该命令：

```shell
/bin/sh -c "touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600"
```

在容器生命的最初30秒内有一个/tmp/healthy 文件，在这30秒内cat /tmp/healthy命令会返回一个成功的返回码。30秒后，cat /tmp/healthy将返回失败的返回码。

创建Pod：

```shell
kubectl apply -f exec-liveness.yaml 
pod/liveness-exec created
```

30秒后查看Pod的event

![](https://github.com/Xiao254182/notes/blob/master/img/12/2.png)

发现容器的健康检测失败导致容器重启

![](https://github.com/Xiao254182/notes/blob/master/img/12/3.png)

我们可以在对pod的描述中找到一下信息

![](https://github.com/Xiao254182/notes/blob/master/img/12/4.png)

图上所示第一个红框使是我们在创建容器时自定义的命令。第二个红框为健康检测机制，具体参数信息如下：

| 参数                    | 作用                                                         |
| ----------------------- | ------------------------------------------------------------ |
| exec [cat /tmp/healthy] | 这是执行的命令，当 Kubernetes 执行活跃性探测时，它会在容器内部运行这个命令。 这里的命令是 `cat /tmp/healthy`，意思是读取 `/tmp/healthy` 文件的内容。如果该文件存在且可读取，则认为容器健康。 |
| delay=5s:               | 这表示在启动容器后，Kubernetes 将等待 5 秒钟，然后开始第一次活跃性探测。这是为了给应用程序一些时间来启动，可在yaml文件中通过initialDelaySeconds参数定义 |
| timeout=1s:             | 这表示当执行 `cat /tmp/healthy` 命令时，如果超过 1 秒没有响应，则视为超时。 |
| period=5s:              | 这表示每隔 5 秒进行一次活跃性探测，可在yaml文件中通过periodSeconds参数定义 |
| #success=1:             | 这表示如果探测成功（即命令返回状态码 0），那么容器被视为健康。成功的次数为 1，意味着只需一次成功就可以。 |
| #failure=3:             | 这表示如果探测失败（命令返回非零状态码）达到 3 次，则 Kubernetes 会认为容器不健康，并可能重启该容器。 |

#### （2）定义一个liveness HTTP请求

可以使用HTTP GET请求作为liveness probe，如：

```yaml
# http-liveness.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness
    args:
    - /server
    image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/sig-storage/livenessprobe:v2.12.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
          - name: X-Custom-Header
            value: Awesome
      initialDelaySeconds: 3
      periodSeconds: 3
```

该配置文件只定义了一个容器，livenessProbe 指定kubelet需要每隔3秒执行一次liveness probe。initialDelaySeconds 指定kubelet在该执行第一次探测之前需要等待3秒钟。该探针将向容器中的server的8080端口发送一个HTTP GET请求。如果server的/healthz路径的handler返回一个成功的返回码，kubelet就会认定该容器是活着的并且很健康。如果返回失败的返回码，kubelet将杀掉该容器并重启它。

任何大于200小于400的返回码都会认定是成功的返回码。其他返回码都会被认为是失败的返回码。

最开始的10秒该容器是活着的， /healthz handler返回200的状态码。这之后将返回500的返回码。

容器启动3秒后，kubelet开始执行健康检查。第一次健康监测会成功，但是10秒后，健康检查将失败，kubelet将杀掉和重启容器。

创建一个Pod来测试一下HTTP liveness检测：

```
kubectl create -f http-liveness.yaml 
pod/liveness-http created
```

10秒后，查看Pod的event，确认liveness probe失败并重启了容器：

![](https://github.com/Xiao254182/notes/blob/master/img/12/5.png)

![](https://github.com/Xiao254182/notes/blob/master/img/12/6.png)

#### (3）定义TCP liveness探针

第三种liveness probe使用TCP Socket。使用此配置，kubelet将尝试在指定端口上打开容器的套接字。如果可以建立连接，容器被认为是健康的，如果不能就认为是失败的。如：

```yaml
# tcp-liveness.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: liveness-tcp
  labels:
    test: liveness
spec:
  containers:
  - name: liveness-tcp
    image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/sig-storage/livenessprobe:v2.12.0
    imagePullPolicy: IfNotPresent
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
```

创建一个Pod来测试一下TCP liveness检测：

```
kubectl create -f tcp-liveness.yaml 
pod/liveness-http created
```

10秒后，查看Pod的event，确认liveness probe失败并重启了容器：

![](https://github.com/Xiao254182/notes/blob/master/img/12/7.png)

![](https://github.com/Xiao254182/notes/blob/master/img/12/8.png)

TCP检查的配置与HTTP检查非常相似。 此示例同时使用了readiness和liveness probe。 容器启动后5秒钟，kubelet将发送第一个readiness probe。 这将尝试连接到端口8080上的liveness probe容器。如果探测成功，则该pod将被标记为就绪。Kubelet将每隔10秒钟执行一次该检查。

除了readiness probe之外，该配置还包括liveness probe。 容器启动15秒后，kubelet将运行第一个liveness probe。 就像readiness probe一样，这将尝试连接到liveness probe容器上的8080端口。如果liveness probe失败，容器将重新启动。

### 2.Readiness

#### （1）定义readiness探针

有时，应用程序暂时无法对外部流量提供服务。例如，应用程序可能需要在启动期间加载大量数据或配置文件。 在这种情况下，你不想杀死应用程序，但你也不想发送请求。 Kubernetes提供了readiness probe来检测和减轻这些情况。 Pod中的容器可以报告自己还没有准备，不能处理Kubernetes服务发送过来的流量。

我们可以通过Readiness检测来告诉K8s什么时候可以将pod加入到服务Service的负载均衡池中，对外提供服务，这个在生产场景服务发布新版本时非常重要，当我们上线的新版本发生程序错误时，Readiness会通过检测发布，从而不导入流量到pod内，将服务的故障控制在内部，在生产场景中，建议这个是必加的，Liveness不加都可以，因为有时候我们需要保留服务出错的现场来查询日志，定位问题，告之开发来修复程序

Readiness probe的配置跟liveness probe很像。唯一的不同是使用 readinessProbe而不是livenessProbe。

```yaml
readinessProbe:
 exec:
  command:
  - cat
  - /tmp/healthy
 initialDelaySeconds: 5
 periodSeconds: 5
```

Readiness probe的HTTP和TCP的探测器配置跟liveness probe一样。

Readiness和livenss probe可以并行用于同一容器。 使用两者可以确保流量无法到达未准备好的容器，并且容器在失败时重新启动。

#### （2）配置Probe

Probe 中有很多精确和详细的配置，通过它们你能准确的控制liveness和readiness检查：

```
initialDelaySeconds：容器启动后第一次执行探测是需要等待多少秒。
periodSeconds：执行探测的频率。默认是10秒，最小1秒。
timeoutSeconds：探测超时时间。默认1秒，最小1秒。
successThreshold：探测失败后，最少连续探测成功多少次才被认定为成功。默认是1。对于liveness必须是1。最小值是1。
failureThreshold：探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1。
```

HTTP probe 中可以给 httpGet设置其他配置项：

```
host：连接的主机名，默认连接到pod的IP。你可能想在http header中设置"Host"而不是使用IP。
scheme：连接使用的schema，默认HTTP。
path: 访问的HTTP server的path。
httpHeaders：自定义请求的header。HTTP运行重复的header。
port：访问的容器的端口名字或者端口号。端口号必须介于1和65535之间。
```

对于HTTP探测器，kubelet向指定的路径和端口发送HTTP请求以执行检查。 Kubelet将probe发送到容器的IP地址，除非地址被httpGet中的可选host字段覆盖。 在大多数情况下，你不想设置主机字段。 有一种情况下你可以设置它。 假设容器在127.0.0.1上侦听，并且Pod的hostNetwork字段为true。 然后，在httpGet下的host应该设置为127.0.0.1。 如果你的pod依赖于虚拟主机，这可能是更常见的情况，你不应该是用host，而是应该在httpHeaders中设置Host头。

## 5.资源配额

### 1.资源配额（ResourceQuota）

ResourceQuota对象用来定义某个命名空间下所有资源的使用限额，其实包括：

```
计算资源的配额
存储资源的配额
对象数量的配额
```

如果集群的总容量小于命名空间的配额总额，可能会产生资源竞争。这时会按照先到先得来处理。

资源竞争和配额的更新都不会影响已经创建好的资源。

#### （1）启用资源配额

很多kubernetes的发行版中资源配额支持默认是开启的，当ResourceQuota作为apiserver的--enable-admission-plugins=的其中一个值时，资源配额被开启。

当某一名称空间包含ResourceQuota对象时资源配额在这个名称空间下生效。

#### （2）计算资源配额

可以限制一个名称空间下可以被申请的计算机资源的总和。kubernetes支持以下资源类型:

| 资源名称        | 描述                                           |
| --------------- | ---------------------------------------------- |
| cpu             | 非终止态的所有pod, cpu请求总量不能超出此值。   |
| limits.cpu      | 非终止态的所有pod，  cpu限制总量不能超出此值。 |
| limits.memory   | 非终止态的所有pod, 内存限制总量不能超出此值。  |
| memory          | 非终止态的所有pod, 内存请求总量不能超出此值。  |
| requests.cpu    | 非终止态的所有pod, cpu请求总量不能超出此值。   |
| requests.memory | 非终止态的所有pod, 内存请求总量不能超出此值。  |

#### （3）存储资源配额

可以限制某一名称空间下的存储空间总量的申请。此外也可以根据关联的storage-class来限制存储空间资源的使用

| 资源名称                                            | 描述                                                         |
| --------------------------------------------------- | ------------------------------------------------------------ |
| requests.storage                                    | 所有PVC, 存储请求总量不能超出此值                            |
| persistentvolumeclaims                              | 命名空间中可以存在的PVC（persistent volume claims）总数      |
| .storageclass.storage.k8s.io/requests.storage       | 和该存储类关联的所有PVC, 存储请求总和不能超出此值            |
| .storageclass.storage.k8s.io/persistentvolumeclaims | 和该存储类关联的所有PVC，命名空间中可以存在的PVC（persistent volume claims）总数 |

例如，一个operator想要想要使gold和bronze单独申请存储空间，那么这个operator可以像如下一样申请配额：

```
gold.storageclass.storage.k8s.io/requests.storage: 500Gi
bronze.storageclass.storage.k8s.io/requests.storage: 100Gi
```

#### （4）对象数量的配额

| 资源名称               | 描述                                                         |
| ---------------------- | ------------------------------------------------------------ |
| congfigmaps            | 命名空间中可以存在的配置映射的总数。                         |
| persistentvolumeclaims | 命名空间中可以存在的PVC总数。                                |
| pods                   | 命名空间中可以存在的非终止态的pod总数。如果一个pod的status.phase 是 Failed,  Succeeded, 则该pod处于终止态。 |
| replicationcontrollers | 命名空间中可以存在的rc总数。                                 |
| resourcequotas         | 命名空间中可以存在的资源配额总数。                           |
| services               | 命名空间中可以存在的服务总数量。                             |
| services.loadbalancers | 命名空间中可以存在的服务的负载均衡的总数量。                 |
| services.nodeports     | 命名空间中可以存在的服务的主机接口的总数量。                 |
| secrets                | 命名空间中可以存在的secrets的总数量。                        |

#### （5）request和limit

当分配计算资源时，每个容器可以为cpu或者内存指定一个请求值和一个限度值。可以配置限额值来限制它们中的任何一个值。

如果指定了requests.cpu 或者 requests.memory的限额值，那么就要求传入的每一个容器显式的指定这些资源的请求。如果指定了limits.cpu或者limits.memory，那么就要求传入的每一个容器显式的指定这些资源的限度。

#### （6）查看和设置配额

创建namespace：

```
kubectl create ns limit
```

创建resourcequota：

```yaml
# compute-resources.yaml 
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
  namespace: limit
spec:
  hard:
    pods: "4"
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
    
# kubectl apply -f compute-resources.yaml     
```

查询resourcequota：

![](https://github.com/Xiao254182/notes/blob/master/img/12/9.png)

查询resourcequota的详细信息：

![](https://github.com/Xiao254182/notes/blob/master/img/12/10.png)

### 2.Pod限额

ResourceQuota对象是限制某个namespace下所有Pod(容器)的资源限额。

LimitRange对象是限制某个namespace单个Pod(容器)的资源限额。

LimitRange对象用来定义某个命名空间下某种资源对象的使用限额，其中资源对象包括：Pod、Container、PersistentVolumeClaim。

#### （1）为namespace配置内存默认值

如果在一个拥有默认内存或CPU限额的命名空间中创建一个容器，并且这个容器未指定它自己的内存或CPU的limit， 它会被分配这个默认的内存或CPU的limit。既没有设置pod的limit和request才会分配默认的内存或CPU的request。

为namespace设置默认内存值，创建namespace：

```
kubectl create namespace default-mem
```

创建LimitRange：

```yaml
# memory-defaults.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
  namespace: default-mem
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container

# kubectl apply -f memory-defaults.yaml 
```

创建Pod,未指定内存的limit和request：

```yaml
# memory-defaults-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: default-mem-demo
  namespace: default-mem
spec:
  containers:
  - name: default-mem-demo-ctr
    image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.27.0
    imagePullPolicy: IfNotPresent
  
# kubectl apply -f memory-defaults-pod.yaml 
```

查看Pod：

```yaml
# kubectl get pod default-mem-demo --output=yaml -n default-mem-example
..........
containers:
- image: nginx
  imagePullPolicy: IfNotPresent
  name: default-mem-demo-ctr
  resources:
    limits:
      memory: 512Mi
    requests:
      memory: 256Mi
.............
```

![](https://github.com/Xiao254182/notes/blob/master/img/12/11.png)

#### （2）为namespace配置内存的最大最小值

创建namespace：

```
kubectl create namespace constraints-mem
```

创建LimitRange：

```yaml
# memory-constraints.yaml 
apiVersion: v1
kind: LimitRange
metadata:
 name: mem-min-max-demo-lr
 namespace: constraints-mem
spec:
 limits:
 - max:
   memory: 1Gi
  min:
   memory: 500Mi
  type: Container

# kubectl apply -f memory-constraints.yaml
```

查看LimitRange：

```yaml
# kubectl get limitrange mem-min-max-demo-lr -n constraints-mem-example --output=yaml
........
spec:
 limits:
 - default:
     memory: 1Gi
   defaultRequest:
     memory: 1Gi
   max:
     memory: 1Gi
   min:
     memory: 500Mi
   type: Container
........
```

![](https://github.com/Xiao254182/notes/blob/master/img/12/13.png)

创建符合要求的Pod：

```yaml
# memory-constraints-pod-1.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: constraints-mem-demo-1
  namespace: constraints-mem
spec:
  containers:
  - name: constraints-mem-demo-ctr-1
    image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.27.0
    imagePullPolicy: IfNotPresent
    resources:
      limits:
        memory: "800Mi"
      requests:
        memory: "600Mi"
    
# kubectl apply -f memory-constraints-pod-1.yaml
pod/constraints-mem-demo-1 created
```

查看Pod：

```yaml
# kubectl get pod constraints-mem-demo-1 --output=yaml -n constraints-mem
.......
  resources:
   limits:
    memory: 800Mi
   requests:
    memory: 600Mi
........
```

![](https://github.com/Xiao254182/notes/blob/master/img/12/14.png)

创建超过最大内存limit的pod

```yaml
# memory-constraints-pod-2.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: constraints-mem-demo-2
  namespace: constraints-mem
spec:
  containers:
  - name: constraints-mem-demo-2-ctr
    image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.27.0
    imagePullPolicy: IfNotPresent
    resources:
      limits:
        memory: "1.5Gi" # 超过最大值 1Gi
      requests:
        memory: "800Mi"
    
# kubectl create -f memory-constraints-pod-2.yaml 
[root@k8s-master-node1 ~]# kubectl apply -f memory-constraints-pod-2.yaml
Error from server (Forbidden): error when creating "memory-constraints-pod-2.yaml": pods "constraints-mem-demo-2" is forbidden: maximum memory usage per Container is 1Gi, but limit is 1536Mi
```

Pod创建失败，因为容器指定的limit过大。如果容器指定的内存request过小，也会创建失败。

容器没有指定自己的CPU请求和限制，那么它将从LimitRange获取默认的CPU请求和限制值。

#### （3）为namespace配置CPU默认值

创建namespace：

```
kubectl create namespace default-cpu
```

创建LimitRange：

```yaml
# cpu-defaults.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
  namespace: default-cpu-example
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container
  
# kubectl apply -f cpu-defaults.yaml 
```

创建Pod，未指定CPU的limit和request：

```yaml
# cpu-defaults-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo
  namespace: default-cpu
spec:
  containers:
  - name: default-cpu-demo-ctr
    image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.27.0
    imagePullPolicy: IfNotPresent
  
# kubectl apply -f cpu-defaults-pod.yaml 
```

查看Pod：

```yaml
# kubectl get pod default-cpu-demo --output=yaml -n default-cpu
..................
containers:
- image: nginx
  imagePullPolicy: IfNotPresent
  name: default-cpu-demo-ctr
  resources:
    limits:
      cpu: "1"
    requests:
      cpu: 500m
......................
```

![](https://github.com/Xiao254182/notes/blob/master/img/12/12.png)

如果没有指定pod的request和limit，则创建的pod会使用LimitRange对象定义的默认值（request和limit）

如果指定pod的limit但未指定request，则创建的pod的request值会取limit的值，而不会取LimitRange对象定义的request默认值。

如果指定pod的request但未指定limit，则创建的pod的limit值会取LimitRange对象定义的limit默认值。

如果命名空间具有资源配额（ResourceQuota），它为内存限额（CPU限额）设置默认值是有意义的。以下是资源配额对命名空间施加的两个限制：

```
在命名空间运行的每一个容器必须有它自己的内存限额（CPU限额）。
在命名空间中所有的容器使用的内存总量（CPU总量）不能超出指定的限额。
```

如果一个容器没有指定它自己的内存限额（CPU限额），它将被赋予默认的限额值，然后它才可以在被配额限制的命名空间中运行。

#### （4）为namespace配置CPU的最大最小值

创建namespace：

```
kubectl create namespace constraints-cpu
```

创建LimitRange

```yaml
# cpu-constraints.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-min-max-demo-lr
  namespace: constraints-cpu
spec:
  limits:
  - max:
      cpu: "800m"
    min:
      cpu: "200m"
    type: Container
  
# kubectl apply -f cpu-constraints.yaml 
```

查看LimitRange：

```
kubectl describe limitrange cpu-min-max-demo-lr -n constraints-cpu
```

![](https://github.com/Xiao254182/notes/blob/master/img/12/15.png)

创建符合要求的Pod：

```yaml
# cat cpu-constraints-pod-1.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: constraints-cpu-demo-1
  namespace: constraints-cpu
spec:
  containers:
  - name: constraints-cpu-demo-ctr-1
    image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.27.0
    imagePullPolicy: IfNotPresent
    resources:
      limits:
        cpu: "800m"
      requests:
        cpu: "500m"

# kubectl apply -f cpu-constraints-pod-1.yaml 

pod/constraints-cpu-demo created
```

查看Pod：

```yaml
# kubectl describe pods constraints-cpu-demo-1 -n constraints-cpu
..........
  Limits:
    cpu: 800m
  Requests:
    cpu:    500m
...........
```

![](https://github.com/Xiao254182/notes/blob/master/img/12/16.png)

创建小于最小CPU request的Pod：

```yaml
# cpu-constraints-pod-2.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: constraints-cpu-demo-2
  namespace: constraints-cpu
spec:
  containers:
  - name: constraints-cpu-demo-2-ctr
    image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.27.0
    imagePullPolicy: IfNotPresent
    resources:
      limits:
        cpu: "800m"
      requests:
        cpu: "100m"

# kubectl apply -f cpu-constraints-pod-2.yaml 

Error from server (Forbidden): error when creating "cpu-constraints-pod-2.yaml": pods "constraints-cpu-demo-2" is forbidden: minimum cpu usage per Container is 200m, but request is 100m
```

Pod创建失败，因为容器指定的CPU request过小。同样的，如果容器指定的CPU limit过大也会导致创建失败。

如果容器没有指定自己的CPU请求和限制，那么它将从LimitRange获取默认的CPU请求和限制值。

LimitRange在namespace中施加的最小和最大内存（CPU）限制只有在创建和更新Pod时才会被应用。改变LimitRange不会对之前创建的Pod造成影响。

Kubernetes会执行下列步骤：

```
如果容器没有指定自己的内存（CPU）请求（request）和限制（limit），系统将会为其分配默认值。
验证容器的内存（CPU）请求大于等于最小值。
验证容器的内存（CPU）限制小于等于最大值。
```

## 6.Kubernetes ServiceAccount管理

ServiceAccount为Pod中的进程和外部用户提供身份信息。所有的kubernetes集群中账户分为两类，Kubernetes管理的serviceaccount(服务账户)和useraccount（用户账户）。

Apiserver是集群的入口，对于kunbernetes的apiserver是肯定不能随便访问。所以我们必须需要一些认证信息。例如当用户访问集群（例如使用kubectl命令）时，apiserver会将用户认证为一个特定的UserAccount（目前通常是admin，除非您的系统管理员自定义了集群配置）。Pod容器中的进程也可以与apiserver联系。当它们在联系apiserver的时候，它们会被认证为一个特定的ServiceAccount。

因为Kubernetes是高度模块化，所有认证方式和授权方式都可以通过插件的方式让客户自定义的，可以支持很多种。客户端请求的时候首先需要进行认证，认证通过后再进行授权检查，因有些操作需要级联到其他资源或者环境，但是级联环境是否有授权权限，这时候需要准入控制。

### 1.使用默认的ServiceAccount访问API server

当创建pod的时候，如果没有指定一个ServiceAccount，系统就会自动在与该Pod相同的namespace下为其指派一个default serviceaccount。如果获取刚创建的Pod的原始json或yaml信息（例如使用kubectl get pods/pod name -o yaml命令），将看到spec.serviceAccountName字段已经被设置为default。

可以在Pod中使用自动挂载的serviceaccount凭证来访问API，ServiceAccount是否能够取得访问API的许可取决于您使用的授权插件和策略。

在1.6以上版本中，可以选择取消为ServiceAccount自动挂载API凭证，只需在ServiceAccount中设置automountServiceAccountToken:false：

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-robot
automountServiceAccountToken: false
...
```

也可以选择只取消单个Pod 的 API 凭证自动挂载：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  serviceAccountName: build-robot
  automountServiceAccountToken: false
  ...
```

如果在Pod和ServiceAccount中同时设置了automountServiceAccountToken，Pod设置中的优先级更高。

### 2.使用多个ServiceAccount

每个namespace中都有一个默认的叫做default的serviceaccount资源。

可以使用以下命令列出namespace下的所有ServiceAccount资源。

```shell
# kubectl get ServiceAccounts
NAME      SECRETS   AGE
default   1         11d
```

编写yaml文件创建 ServiceAccount 对象：

```yaml
# serviceaccount.yaml 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test
 
# kubectl apply -f serviceaccount.yaml 
serviceaccount/test created
```

查看SA：

```shell
# kubectl get sa
NAME      SECRETS   AGE
default   1         11d
test      1         19s
```

查看sa完整输出信息：

```yaml
# kubectl get serviceaccounts/test -o yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"name":"test","namespace":"default"}}
  creationTimestamp: "2024-08-02T04:17:30Z"
  name: test
  namespace: default
  resourceVersion: "41188"
  uid: 0b6c7c29-acca-4c88-9919-22f30a6618a6
secrets:
- name: test-token-qr7m2
```

设置非默认的serviceaccount，只需要在pod的spec.serviceAccountName字段中将name设置为想要用的serviceaccount名字即可。

在pod创建之初serviceaccount就必须已经存在，否则创建将被拒绝。

已创建的pod的serviceaccount不能被更新。

删除SA：

```shell
# kubectl delete serviceaccount/test
serviceaccount "test" deleted
```

### 3.手动创建ServiceAccount的API token

假设已经有了一个名为test的ServiceAccount，手动创建一个新的secret：

```yaml
# test-secret.yaml 
apiVersion: v1
kind: Secret
metadata:
  name: test-secret
  annotations: 
    kubernetes.io/service-account.name: test
  type: kubernetes.io/service-account-token

# kubectl apply -f test-secret.yaml 
secret/test-secret created
```

所有已不存在的 ServiceAccount 的 token 将被 token controller 清理掉：

```
kubectl describe secrets/test-secret
```

![](https://github.com/Xiao254182/notes/blob/master/img/12/18.png)

## 7.Kubernetes RBAC管理

RBAC(Role-Based Access Control，基于角色的访问控制)在k8s v1.5中引入，在v1.6版本时升级为Beta版本，并成为kubeadm安装方式下的默认选项，相对于其它访问控制方式，RBAC具有如下优势：

```
对集群中的资源和非资源权限均有完整的覆盖；
整个RBAC完全由几个API对象完成，同其他API对象一样，可以用kubectl或API进行操作；
可以在运行时进行调整，无需重启API Server。
```

要使用RBAC授权模式，需要在API Server的启动参数中加上--authorization-mode=RBAC

### 1.Role与ClusterRole

#### （1）Role

在RBAC API中，一个角色包含了一套表示一组权限的规则。权限以纯粹的累加形式累积（没有”否定”的规则）。角色可以由命名空间（namespace）内的Role对象定义，而整个Kubernetes集群范围内有效的角色则通过ClusterRole对象实现。

一个Role对象只能用于授予对某一单一命名空间中资源的访问权限。以下示例描述了“default”命名空间中的一个Role对象的定义，用于授予对Pod的读访问权限：

```yaml
# role.yaml  
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
  - apiGroups: [""] # 空字符串"" 表明使用 core API group
    resources: ["pods"]
    verbs: ["get", "watch", "list"]
```

创建Role：

```shell
# kubectl apply -f role.yaml 
role.rbac.authorization.k8s.io/pod-reader created
```

（PS:可能会出现如下错误：

```shell
# kubectl apply -f role.yaml
error: unable to recognize "role.yaml": no matches for kind "Role" in version "rbac.authorization.k8s.io/v1beta1"
```

这个错误提示表明 Kubernetes 无法识别 `role.yaml` 文件中的 `Role` 类型，这是因为你使用的是过时的 API 版本 `rbac.authorization.k8s.io/v1beta1`。从 Kubernetes 1.22 版本开始，`v1beta1` 已被移除，建议使用 `rbac.authorization.k8s.io/v1`，所以将上述yaml文件中的`rbac.authorization.k8s.io/v1beta1`改为`rbac.authorization.k8s.io/v1`即可）

查看Role：

```shell
# kubectl get role
NAME         CREATED AT
pod-reader   2024-08-02T06:59:43Z

# kubectl describe role pod-reader
Name:         pod-reader
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  pods       []                 []              [get watch list]
```

#### （2）ClusterRole

ClusterRole对象可以授予与Role对象相同的权限，但由于它们属于集群范围对象，也可以使用它们授予对以下几种资源的访问权限：

```
集群范围资源（例如节点，即node）
非资源类型endpoint（例如”/healthz”）
跨所有命名空间的命名空间范围资源（例如Pod，需要运行命令kubectl get pods --all-namespaces来查询集群中所有的Pod）
```

下面示例中的 ClusterRole 定义可用于授予用户对某一特定命名空间，或者所有命名空间中的 secret（取决于其 绑定 方式）的读访问权限：

```yaml
# cat clusterrole.yaml 
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: secret-reader
  # 鉴于 ClusterRole 是集群范围对象，所以这里不需要定义 "namespace" 字段
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
```

创建并查看ClusterRole：

```shell
# kubectl create -f clusterrole.yaml
clusterrole.rbac.authorization.k8s.io/secret-reader created

# kubectl describe clusterrole secret-reader
Name:         secret-reader
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  secrets    []                 []              [get watch list]
```

### 2.RoleBinding与ClusterRoleBinding

角色绑定将一个角色中定义的各种权限授予一个或者一组用户。角色绑定包含了一组相关主体（即subject，包括用户User、用户组Group或者服务账户ServiceAccount）以及对被授予角色的引用。在命名空间中可以通过RoleBinding对象授予权限，而集群范围的权限授予则通过ClusterRoleBinding对象完成。

RoleBinding可以引用在同一命名空间内定义的Role对象。下面示例中定义的RoleBinding对象在“default”命名空间中将“pod-reader”角色授予用户“jane”。这一授权将允许用户“jane”从“default”命名空间中读取Pod。

以下角色绑定定义将允许用户“jane”从“default”命名空间中读取Pod。

```yaml
# rolebinding.yaml 
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

创建RoleBinding：

```shell
# kubectl apply -f rolebinding.yaml
rolebinding.rbac.authorization.k8s.io/read-pods created

#  kubectl get rolebinding
NAME        ROLE              AGE
read-pods   Role/pod-reader   5s
```

RoleBinding对象也可以引用一个ClusterRole对象用于在RoleBinding所在的命名空间内授予用户对所引用的ClusterRole中定义的命名空间资源的访问权限。这一点允许管理员在整个集群范围内首先定义一组通用的角色，然后再在不同的命名空间中复用这些角色。

例如下面示例中的 RoleBinding 引用的是一个 ClusterRole 对象，但是用户“dave”（即角色绑定主体）还是只能读取“development” 命名空间中的 secret（即 RoleBinding 所在的命名空间）。

```yaml
# 以下角色绑定允许用户 "dave" 读取 "development" 命名空间中的 secret。
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-secrets
  namespace: development # 这里表明仅授权读取 "development" 命名空间中的资源。
subjects:
- kind: User
  name: dave
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
```

最后，可以使用ClusterRoleBinding在集群级别和所有命名空间中授予权限。下面示例中所定义的ClusterRoleBinding允许在用户组“manager”中的任何用户都可以读取集群中任何命名空间中的secret。

以下ClusterRoleBinding对象允许在用户组“manager”中的任何用户都可以读取集群中任何命名空间中的secret。

```yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-secrets-global
subjects:
- kind: Group
  name: manager
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
```

### 3.对资源的引用

#### （1）资源引用

大多数资源由代表其名字的字符串表示，例如“pods”，就像它们出现在相关API endpoint的URL中一样。然而，有一些Kubernetes API还包含了“子资源”，比如pod的logs。在Kubernetes中，pod logs endpoint的URL格式为：

```apl
GET /api/v1/namespaces/{namespace}/pods/{name}/log
```

在这种情况下，“pods”是命名空间资源，而“log”是pods的子资源。为了在RBAC角色中表示出这一点，需要使用斜线来划分资源与子资源。如果需要角色绑定主体读取pods以及pod log，需要定义以下角色：

```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-and-pod-logs-reader
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "list"]
```

通过resourceNames列表，角色可以针对不同种类的请求根据资源名引用资源实例。当指定了resourceNames列表时，不同动作种类的请求的权限，如使用“get”、“delete”、“update”以及“patch”等动词的请求，将被限定到资源列表中所包含的资源实例上。例如，如果需要限定一个角色绑定主体只能“get”或者“update”一个configmap时，可以定义以下角色：

```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  namespace: default
  name: configmap-updater
rules:
- apiGroups: [""]
  resources: ["configmap"]
  resourceNames: ["my-configmap"]
  verbs: ["update", "get"]
```

值得注意的是，如果设置了resourceNames，则请求所使用的动词不能是list、watch、create或者deletecollection。由于资源名称不会出现在create、list、watch和deletecollection等API请求的URL中，所以这些请求动词不会被设置了resourceNames的规则所允许，因为规则中的resourceNames部分不会匹配这些请求。

#### （2）定义角色示例

允许读取 core API Group 中定义的资源“pods”：

```yaml
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
```

允许读写在“extensions” 和“apps” API Group 中定义的“deployments”：

```yaml
rules:
- apiGroups: ["extensions", "apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
```

允许读取“pods” 以及读写“jobs”：

```yaml
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch", "extensions"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
```

允许读取一个名为“my-config”的ConfigMap实例（需要将其通过RoleBinding绑定从而限制针对某一个命名空间中定义的一个ConfigMap实例的访问）：

```yaml
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["my-config"]
  verbs: ["get"]
```

允许读取 core API Group 中的“nodes” 资源（由于Node是集群级别资源，所以此ClusterRole定义需要与一个ClusterRoleBinding绑定才能有效）：

```yaml
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]
```

允许对非资源endpoint“/healthz”及其所有子路径的“GET”和“POST”请求（此ClusterRole定义需要与一个ClusterRoleBinding绑定才能有效）：

```yaml
rules:
- nonResourceURLs: ["/healthz", "/healthz/*"] # 在非资源 URL 中，'*' 代表后缀通配符
  verbs: ["get", "post"]
```

对角色绑定主体（Subject）的引用RoleBinding或者ClusterRoleBinding将角色绑定到角色绑定主体（Subject）。角色绑定主体可以是用户组（Group）、用户（User）或者服务账户（ServiceAccounts）。

用户由字符串表示。可以是纯粹的用户名，例如“alice”、电子邮件风格的名字，如`xxx@example.com`或者是用字符串表示的数字id。由Kubernetes管理员配置认证模块以产生所需格式的用户名。对于用户名，RBAC授权系统不要求任何特定的格式。然而，前缀system:是为Kubernetes系统使用而保留的，所以管理员应该确保用户名不会意外地包含这个前缀。

Kubernetes中的用户组信息由授权模块提供。用户组与用户一样由字符串表示。Kubernetes对用户组字符串没有格式要求，但前缀system:同样是被系统保留的。

服务账户拥有包含system:serviceaccount:前缀的用户名，并属于拥有system:serviceaccounts:前缀的用户组。

（3）角色绑定示例

定义一个名为`alice@example.com`的用户：

```yaml
subjects:
- kind: User
  name: "alice@example.com"
  apiGroup: rbac.authorization.k8s.io
```

定义一个名为“frontend-admins” 的用户组：

```yaml
subjects:
- kind: Group
  name: "frontend-admins"
  apiGroup: rbac.authorization.k8s.io
```

kube-system 命名空间中的默认服务账户：

```yaml
subjects:
- kind: ServiceAccount
  name: default
  namespace: kube-system
```

名为“front” 命名空间中的所有服务账户：

```yaml
subjects:
- kind: Group
  name: system:serviceaccounts:front
  apiGroup: rbac.authorization.k8s.io
```

在集群中的所有服务账户：

```yaml
subjects:
- kind: Group
  name: system:serviceaccounts
  apiGroup: rbac.authorization.k8s.io
```

所有认证过的用户（version 1.5+）：

```yaml
subjects:
- kind: Group
  name: system:authenticated
  apiGroup: rbac.authorization.k8s.io
```

所有未认证的用户（version 1.5+）：

```yaml
subjects:
- kind: Group
  name: system:unauthenticated
  apiGroup: rbac.authorization.k8s.io
```

所有用户（version 1.5+）：

```yaml
subjects:
- kind: Group
  name: system:authenticated
  apiGroup: rbac.authorization.k8s.io
- kind: Group
  name: system:unauthenticated
  apiGroup: rbac.authorization.k8s.io
```

## 8.Kubernetes Network Policy

网络策略说明一组 Pod 之间是如何被允许互相通信，以及如何与其它网络 Endpoint 进行通信。 NetworkPolicy 资源使用标签来选择 Pod，并定义了一些规则，这些规则指明允许什么流量进入到选中的 Pod 上。关于 Network Policy 的详细用法请参考 Kubernetes 官网。

Network Policy 的作用对象是 Pod，也可以应用到 Namespace 和集群的 Ingress、Egress 流量。Network Policy 是作用在 L3/4 层的，即限制的是对 IP 地址和端口的访问，如果需要对应用层做访问限制需要使用如 Istio 这类 Service Mesh。

### 1.隔离的与未隔离的Pod

默认Pod是未隔离的，它们可以从任何的源接收请求。具有一个可以选择Pod的网络策略后，Pod就会变成隔离的。一旦Namespace中配置的网络策略能够选择一个特定的Pod，这个Pod将拒绝任何该网络策略不允许的连接。（Namespace中其它未被网络策略选中的Pod将继续接收所有流量）

#### （1）NetworkPolicy资源

下面是一个NetworkPolicy 的例子：

```yaml
# networkpolicy.yaml 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978
```

像所有其它 Kubernetes 配置一样， NetworkPolicy 需要 apiVersion、kind 和 metadata 这三个必选字段。其它字段解释如下：

| 字段        | 含义                                                         |
| ----------- | ------------------------------------------------------------ |
| spec        | NetworkPolicy spec具有在给定Namespace中定义特定网络的全部信息 |
| podSelector | 每个NetworkPolicy包含一个podSelector，它可以选择一组应用了网络策略的Pod。由于NetworkPolicy当前只支持定义ingress规则，这个podSelector实际上为该策略定义了一组“目标Pod”。示例中的策略选择了标签为“role=db”的Pod。一个空的podSelector选择了该Namespace中的所有Pod |
| ingress     | 每个NetworkPolicy包含了一个白名单ingress规则列表。每个规则只允许能够匹配上from和ports配置段的流量。示例策略包含了单个规则，它从这两个源中匹配在单个端口上的流量，第一个是通过namespaceSelector指定的，第二个是通过podSelector指定的 |
| egress      | 每个NetworkPolicy包含了一个白名单ingress规则列表。每个规则只允许能够匹配上to和ports配置段的流量。示例策略包含了单个规则，它匹配目的地10.0.0.0/24单个端口的流量 |

#### （2）默认策略

通过创建一个可以选择所有Pod但不允许任何流量的NetworkPolicy，可以为一个Namespace创建一个“默认的”隔离策略，如下所示：

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector:
```

这确保了即使是没有被任何 NetworkPolicy 选中的 Pod，将仍然是被隔离的。

在 Namespace 中，如果想允许所有的流量进入到所有的 Pod（即使已经添加了某些策略，使一些 Pod 被处理为 “隔离的”），可以通过创建一个策略来显式地指定允许所有流量：

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all
spec:
  podSelector:
  ingress:
  - {}
```

## 9.Kubernetes存储管理

### Kubernetes本地存储

Docker 中也有一个 volume 的概念，尽管它稍微宽松一些，管理也很少。在 Docker 中，卷就像是磁盘或是另一个容器中的一个目录。它的生命周期不受管理，直到最近才有了 local-disk-backed 卷。Docker 现在提供了卷驱动程序，但是功能还非常有限（例如Docker1.7只允许每个容器使用一个卷驱动，并且无法给卷传递参数）。

另一方面，Kubernetes 中的卷有明确的寿命——与封装它的 Pod 相同。所以，卷的生命比 Pod 中的所有容器都长，当这个容器重启时数据仍然得以保存。当然，当 Pod 不再存在时，卷也将不复存在。也许更重要的是，Kubernetes 支持多种类型的卷，Pod 可以同时使用任意数量的卷。

卷的核心是目录，可能还包含了一些数据，可以通过 pod 中的容器来访问。该目录是如何形成的、支持该目录的介质以及其内容取决于所使用的特定卷类型。

要使用卷，需要为 pod 指定为卷（spec.volumes 字段）以及将它挂载到容器的位置（spec.containers.volumeMounts 字段）。

容器中的进程看到的是由其 Docker 镜像和卷组成的文件系统视图。 Docker 镜像位于文件系统层次结构的根目录，任何卷都被挂载在镜像的指定路径中。卷无法挂载到其他卷上或与其他卷有硬连接。Pod 中的每个容器都必须独立指定每个卷的挂载位置。

Kubernetes提供了非常丰富的Volume类型，下面是一些常用的Volume类型：

```
emptyDir
hostPath
gcePersistentDisk
```

#### 1.emptyDir

一个emptyDir volume是在Pod分配到Node时创建的。顾名思义，它的初始内容为空，在同一个Pod中的所有容器均可以读写这个emptyDir volume。当 Pod 从 Node 上被删除（Pod 被删除，或者 Pod 发生迁移），emptyDir 也会被删除，并且数据永久丢失。

emptyDir类型的volume适合于以下场景：

```
临时空间。例如某些程序运行时所需的临时目录，无需永久保存。
一个容器需要从另一容器中获取数据的目录（多容器共享目录）
```

下面是一个简单的例子：

```yaml
# emptydir.yaml
apiVersion: v1
kind: Pod    #类型是Pod
metadata:
  labels:
    name: redis
    role: master    #定义为主redis
  name: redis-master
spec:
  containers:
    - name: master
      image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/redis:7.2.5
      imagePullPolicy: IfNotPresent
      env:    #定义环境变量
        - name: MASTER
          value: "true"
      ports:    #容器内端口
        - containerPort: 6379
      volumeMounts:    #容器内挂载点
        - mountPath: /data
          name: redis-data    #必须有名称
  volumes:
     - name: redis-data    #跟上面的名称对应
       emptyDir: {}    #宿主机挂载点
```

创建Pod：

```shell
# kubectl apply -f emptydir.yaml
pod/redis-master created

# kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
redis-master   1/1     Running   0          4s
```

此时Emptydir已经创建成功，在宿主机上的访问路径为/var/lib/kubelet/pods/\<poduid>/volumes/kubernetes.io~empty-dir/redis-data,如果在此目录中创建删除文件，都将对容器中的/data目录有影响，如果删除Pod，文件将全部删除，即使是在宿主机上创建的文件也是如此，在宿主机上删除容器则k8s会再自动创建一个容器，此时文件仍然存在。

#### 2.hostPath

hostPath类型的volume允许用户挂在Node上的文件系统到Pod中，如果 Pod 需要使用 Node 上的文件，可以使用 hostPath。

下面是一个简单的例子：

```yaml
# hostpath.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: nginx-volume
  namespace: default
spec:
  containers:
  - name: nginx
    image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.27.0
    imagePullPolicy: IfNotPresent
    ports:
    - containerPort: 80
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html/
  volumes:
  - name: html
    hostPath:
      path: /data/nginx/v1/
      type: DirectoryOrCreate
```

创建Pod：

```shell
# kubectl apply -f hostpath.yaml
pod/nginx-volume created

# kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
nginx-volume   1/1     Running   0          5s
```

#### 3.ConfigMap和Secre

镜像使用的过程中，经常需要利用配置文件、启动脚本等方式来影响容器的运行方式，如果仅有少量配置，可以使用环境变量的方式来进行配置。然而对于一些较为复杂的配置，例如Apache之类，就很难用这种方式进行控制了。另外一些敏感信息暴露在YAML中也是不合适的。

ConfigMap和Secret除了使用文件方式进行应用之外，还有其他的应用方式；这里仅就文件方式做一点说明。

例如下面的ConfigMap，将一个存储在ConfigMap中的配置目录加载到卷中。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/gcr.io/google-containers/busybox:latest
      imagePullPolicy: IfNotPresent
      command: [ "/bin/sh", "-c", "ls /etc/config/" ]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
		#提供包含所需文件的ConfigMap的名称
		#添加到容器中
        name: special-config
  restartPolicy: Never
```

这里的 ConfigMap 会映射为一个目录，ConfigMap 的 Key 就是文件名，每个 Value 就是文件内容，比如下面命令用一个目录创建一个 ConfigMap：

```shell
kubectl create configmap \
  game-config \
  --from-file=docs/user-guide/configmap/kubectl
```

创建一个 Secret：

```shell
kubectl create secret generic \
  db-user-pass --from-file=./username.txt \
  --from-file=./password.txt
```

使用 Volume 加载 Secret：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
  namespace: myns
spec:
  containers:
    - name: mypod
      image: redis
      imagePullPolicy: IfNotPresent
      volumeMounts:
        - name: foo
          mountPath: /etc/foo
          readOnly: true
  volumes:
    - name: foo
      secret:
        secretName: mysecret
```

可以看到Secret 和 ConfigMap 的创建和使用是很相似的。在 RBAC 中，Secret 和 ConfigMap 可以进行分别赋权，以此限定操作人员的可见、可控权限。

### Kubernetes持久化存储

PersistentVolume（持久卷）和PersistentVolumeClaim（持久卷申请）是k8s提供的两种API资源，用于抽象存储细节。也提供了基础设施和应用之间的分界，管理员关注于如何通过PV提供存储功能而无需关注用户如何使用，同样的用户只需要挂载PVC到容器中而不需要关注存储卷采用何种技术实现。换种说法就是管理员创建一系列的 PV 提供存储，然后为应用提供 PVC，应用程序仅需要加载一个 PVC，就可以进行访问

PVC和PV的关系与pod和node关系类似，前者消耗后者的资源。PVC可以向PV申请指定大小的存储资源并设置访问模式,这就可以通过Provision -> Claim 的方式，来对存储资源进行控制。

#### 1.PV

PV 的全称是：PersistentVolume（持久化卷），是对底层的共享存储的一种抽象，PV 由管理员进行创建和配置，它和具体的底层的共享存储技术的实现方式有关，比如 Ceph、GlusterFS、NFS 等，都是通过插件机制完成与共享存储的对接。

PVC 的全称是：PersistentVolumeClaim（持久化卷声明），PVC 是用户存储的一种声明，PVC 和 Pod 比较类似，Pod 消耗的是节点，PVC 消耗的是 PV 资源，Pod 可以请求 CPU 和内存，而 PVC 可以请求特定的存储空间和访问模式。对于真正使用存储的用户不需要关心底层的存储实现细节，只需要直接使用 PVC 即可。

但是通过 PVC 请求到一定的存储空间也很有可能不足以满足应用对于存储设备的各种需求，而且不同的应用程序对于存储性能的要求可能也不尽相同，比如读写速度、并发性能等，为了解决这一问题，Kubernetes引入了一个新的资源对象：StorageClass，通过 StorageClass 的定义，管理员可以将存储资源定义为某种类型的资源，比如快速存储、慢速存储等，用户根据 StorageClass 的描述就可以非常直观的知道各种存储资源的具体特性了，这样就可以根据应用的特性去申请合适的存储资源了。

##### （1）安装NFS服务端

这里为了演示方便，决定使用相对简单的 NFS 这种存储资源，在master节点上来安装 NFS 服务：

```shell
[root@k8s-master-node1 ~]# yum install -y nfs-utils rpcbind
```

创建共享目录并设置权限：

```shell
[root@k8s-master-node1 ~]# mkdir /data/k8s/
[root@k8s-master-node1 ~]# chmod 755 /data/k8s/
```

配置 nfs，nfs 的默认配置文件在 /etc/exports 文件下，在该文件中添加下面的配置信息：

```shell
[root@k8s-master-node1 ~]# echo "/data/k8s  *(rw,sync,no_root_squash)" > /etc/exports
```

配置说明：

```
/data/k8s：是共享的数据目录；
*：表示任何人都有权限连接，当然也可以是一个网段，一个 IP，也可以是域名；
rw：读写的权限；
sync：表示文件同时写入硬盘和内存；
no_root_squash：当登录 NFS 主机使用共享目录的使用者是 root 时，其权限将被转换成为匿名使用者，通常它的 UID 与 GID，都会变成 nobody 身份。
```

启动服务 nfs 需要向 rpc 注册，rpc 一旦重启了，注册的文件都会丢失，向其注册的服务都需要重启。

注意启动顺序，先启动 rpcbind：

```shell
[root@k8s-master-node1 ~]# systemctl start rpcbind.service && systemctl enable rpcbind
```

然后启动 nfs 服务：

```shell
[root@k8s-master-node1 ~]# systemctl start nfs.service && systemctl enable nfs
Created symlink from /etc/systemd/system/multi-user.target.wants/nfs-server.service to /usr/lib/systemd/system/nfs-server.service.
```

测试：

```shell
[root@k8s-master-node1 ~]# rpcinfo -p|grep nfs
    100003    3   tcp   2049  nfs
    100003    4   tcp   2049  nfs
    100227    3   tcp   2049  nfs_acl
    100003    3   udp   2049  nfs
    100003    4   udp   2049  nfs
    100227    3   udp   2049  nfs_acl
```

##### （2）安装NFS客户端

在node节点安装客户端来验证NFS：

```shell
[root@k8s-worker-node1 ~]# yum -y install nfs-utils rpcbind
```

安装完成后，先启动 rpc

```shell
[root@k8s-worker-node1 ~]# systemctl start rpcbind.service && systemctl enable rpcbind.service 
```

然后启动 nfs

```shell
[root@k8s-worker-node1 ~]# systemctl start nfs.service && systemctl enable nfs.service
Created symlink from /etc/systemd/system/multi-user.target.wants/nfs-server.service to /usr/lib/systemd/system/nfs-server.service.
```

首先在客户端检查下 nfs 是否有共享目录：

```shell
[root@k8s-worker-node1 ~]# showmount -e k8s-master-node1
Export list for k8s-master-node1:
/data/k8s *
```

在客户端上新建目录：

```shell
[root@k8s-worker-node1 ~]# mkdir -p /root/course/kubeadm/data
```

将 nfs 共享目录挂载到上面的目录：

```shell
[root@k8s-worker-node1 ~]# mount -t nfs k8s-master-node1:/data/k8s /root/course/kubeadm/data
```

挂载成功后，在客户端上面的目录中新建一个文件，然后观察下 nfs 服务端的共享目录下面是否也会出现该文件：

```shell
[root@k8s-worker-node1 ~]# touch /root/course/kubeadm/data/test.txt
```

然后在 nfs 服务端查看：

```shell
[root@k8s-master-node1 ~]# ls -ls /data/k8s/
总用量 0
0 -rw-r--r-- 1 root root 0 8月   2 17:31 test.txt
```

##### （3）新建PV

有了NFS 共享存储，下面就可以来使用 PV 和 PVC 了。PV 作为存储资源，主要包括存储能力、访问模式、存储类型、回收策略等关键信息，下面来新建一个 PV 对象，使用 nfs 类型的后端存储，1G 的存储空间，访问模式为 ReadWriteOnce，回收策略为 Recyle，yaml文件如下：

```yaml
# pv1-demo.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv1
spec:
  capacity: 
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /data/k8s
    server: 192.168.100.10    #NFS服务器的ip地址
```

Kubernetes支持的PV类型有很多，比如常见的Ceph、GlusterFs、NFS，甚至HostPath也可以，不过HostPath一般用于单机测试。

创建PV：

```shell
[root@k8s-master-node1 ~]# kubectl apply -f pv1-demo.yaml
persistentvolume/pv1 created
[root@k8s-master-node1 ~]# kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv1    1Gi        RWO            Recycle          Available                                   5s
```

可以看到pv1已经创建成功了，状态是Available，表示pv1就绪，可以被PVC申请。

一般来说，一个PV对象都要指定一个存储能力，通过PV的capacity属性来设置的，目前只支持存储空间的设置，就是这里的storage=1Gi，不过未来可能会加入IOPS、吞吐量等指标的配置。

AccessModes是用来对PV进行访问模式的设置，用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：

```
ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载
ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载
ReadWriteMany（RWX）：读写权限，可以被多个节点挂载
```

这里指定的PV的回收策略为Recycle，目前PV支持的策略有三种：

```
Retain（保留）-保留数据，需要管理员手工清理数据
Recycle（回收）-清除PV中的数据，效果相当于执行rm -rf /thevoluem/*
Delete（删除）-与PV相连的后端存储完成volume的删除操作，当然这常见于云服务商的存储服务，比如ASW EBS。
```

不过需要注意的是，目前只有NFS和HostPath两种类型支持回收策略。当然一般来说还是设置为Retain这种策略保险一点。

一个PV的生命周期中，可能会处于4中不同的阶段：

```
Available（可用）：表示可用状态，还未被任何PVC绑定
Bound（已绑定）：表示PVC已经被PVC绑定
Released（已释放）：PVC被删除，但是资源还未被集群重新声明
Failed（失败）：表示该PV的自动回收失败
```

#### 2.PVC

##### （1）新建PVC

新建一个数据卷声明，请求1Gi的存储容量，访问模式为ReadWriteOnce，yaml文件如下：

```yaml
# pvc-nfs.yaml 
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pv1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

可以看到PVC的声明方法几乎和新建 PV 是一样的，在新建 PVC 之前，可以看下之前创建的 PV 的状态：

```shell
[root@k8s-master-node1 ~]# kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv1    1Gi        RWO            Recycle          Available                                   5s
```

可以看到当前pv1 是在Available 的一个状态，所以这个时候的 PVC 可以和这个 PV 进行绑定。

创建PVC：

```shell
[root@k8s-master-node1 ~]# kubectl apply -f pvc-nfs.yaml
persistentvolumeclaim/pv1 created
[root@k8s-master-node1 ~]# kubectl get pvc
NAME   STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pv1    Bound    pv1      1Gi        RWO                           4s
```

再次查看PV状态：

```shell
[root@k8s-master-node1 ~]# kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AG
pv1    1Gi        RWO            Recycle          Bound    default/pv1                           4m
```

可以看到PV也是Bound状态了，对应的声明是default/pvc-nfs，就是default命名空间下面的pvc-nfs，证明刚刚新建的pvc-nfs和pv1绑定成功了。

前面并没有在pvc-nfs中指定关于pv的什么标志，它们之间是怎么关联起来的呢？其实是系统自动去匹配的，他会根据声明要求去查找处于Available状态的PV，如果没有找到的话，那么PVC就会一直处于Pending状态，找到了的话当然就会把当前的PVC和目标PV进行绑定，这个时候状态就会变成Bound状态了。比如新建一个PVC，yaml如下：

```yaml
# pvc2-nfs.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc2-nfs
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  selector:
    matchLabels:
      app: nfs
```

这里声明一个PV资源的请求，邀请访问模式是ReadWriteOnce，存储容量是2Gi，最后还要求匹配具有标签app=nfs的PV。

创建PVC：

```shell
[root@k8s-master-node1 ~]# kubectl apply -f pvc2-nfs.yaml
persistentvolumeclaim/pvc2-nfs created
[root@k8s-master-node1 ~]# kubectl get pvc
NAME       STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pv1        Bound     pv1      1Gi        RWO                           45s
pvc2-nfs   Pending                                                     5s
```

pvc2-nfs是 Pending 状态，因为并没有合适的 PV 给你使用，新建一个 PV，让上面的 PVC 有合适的 PV 使用：

```yaml
# pv2-nfs.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv2-nfs
  labels:
    app: nfs
spec:
  capacity:
    storage: 2Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    server: 192.168.100.10
    path: /data/k8s
    
# kubectl apply -f pv2-nfs.yaml 
persistentvolume/pv2-nfs created
```

创建完pv2-nfs后，可以看到该PV是Bound状态了，对应的PVC是default/pvc2-nfs，证明上面的pvc2-nfs找到合适的PV并进行了绑定：

```shell
[root@k8s-master-node1 ~]# kubectl get pv
NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REA
pv1       1Gi        RWO            Recycle          Bound    default/pv1
pv2-nfs   2Gi        RWO            Recycle          Bound    default/pvc2-nfs
[root@k8s-master-node1 ~]# kubectl get pvc
NAME       STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pv1        Bound    pv1       1Gi        RWO                           2m3s
pvc2-nfs   Bound    pv2-nfs   2Gi        RWO                           83s
```

##### （2）使用PVC

编写nfs-pvc-deploy.yaml：

```yaml
# nfs-pvc-deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-pvc
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nfs-pvc
  template:
    metadata:
      labels:
        app: nfs-pvc
    spec:
      containers:
      - name: nginx
        image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.27.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
      volumes:
      - name: www
        persistentVolumeClaim:
          claimName: pvc2-nfs
---
apiVersion: v1
kind: Service
metadata:
  name: nfs-pvc
  labels:
    app: nfs-pvc
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: web
  selector:
    app: nfs-pvc
```

这里使用nginx镜像，将容器的/usr/share/nginx/html目录通过volume挂载到名为pvc2-nfs的PVC上面，然后创建一个NodePort类型的Service来暴露服务：

```shell
[root@k8s-master-node1 ~]# vim nfs-pvc-deploy.yaml
[root@k8s-master-node1 ~]# kubectl apply -f nfs-pvc-deploy.yaml
deployment.apps/nfs-pvc created
service/nfs-pvc created
[root@k8s-master-node1 ~]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
nfs-pvc-6f8887fc5-4l55p   1/1     Running   0          15s
nfs-pvc-6f8887fc5-9bzrj   1/1     Running   0          13s
nfs-pvc-6f8887fc5-gpbwz   1/1     Running   0          11s
[root@k8s-master-node1 ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        12d
nfs-pvc      NodePort    10.96.123.204   <none>        80:63240/TCP   109s
```

现在可以通过任意节点来访问Nginx服务了，但是这个时候访问会出现403

```shell
[root@k8s-master-node1 ~]# curl 10.96.123.204
<html>
<head><title>403 Forbidden</title></head>
<body>
<center><h1>403 Forbidden</h1></center>
<hr><center>nginx/1.27.0</center>
</body>
</html>
```

查看nfs共享数据目录下面有没有数据：

```shell
[root@k8s-master-node1 ~]# ls /data/k8s
```

可以看到没有任何数据，这是因为容器目录/user/share/nginx/html挂载到了pvc2-nfs上面，这个PVC对应着nfs的共享数据目录，该目录下面还没有任何数据，所以访问就出现了403，在/data/k8s这个目录下面新建一个index.html的文件：

```shell
[root@k8s-master-node1 ~]# echo "<h1>Hello Kubernetes~</h1>" >> /data/k8s/index.html
[root@k8s-master-node1 ~]# ls /data/k8s
index.html
```

访问Nginx服务

```shell
[root@k8s-master-node1 ~]# curl 10.96.123.204
<h1>Hello Kubernetes~</h1>
```

#### 3.StorageClass

PV都是静态的，要使用一个PVC的话就必须手动去创建一个PV，这种方式在很大程度上并不能满足生产的需求，比如有一个应用需要对存储的并发度要求比较高，而另外一个应用对读写速度又要求比较高，特别是对于StatefulSet类型的应用简单的来使用静态的PV就很不合适了，这种情况下就需要用到动态PV，也就是StorageClass。

##### （1）创建Storageclass

要使用StorageClass，就得安装对应的自动配置程序，比如前面的存储后端使用的是nfs，那么就需要使用到一个nfs-client的自动配置程序，也叫作Provisioner，这个程序使用、已经配置好的nfs服务器来自动创建持久卷。

自动创建的PV以${namespace}-${pvcName}-${pvName}这样的命名格式创建在NFS服务器上的共享数据目录中，而当这个PV被回收后会以archieved-${namespace}-${pvcName}-${pvName}这样的命名格式存在NFS服务器上。

配置 Deployment，yaml文件如下：

```yaml
# nfs-client.yaml 
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nfs-client-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: quay.io/external_storage/nfs-client-provisioner:latest
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: fuseim.pri/ifs
            - name: NFS_SERVER
              value: 192.168.100.10
            - name: NFS_PATH
              value: /data/k8s
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.100.10
            path: /data/k8s
```

创建一个sa，然后绑定上对应的权限：

```yaml
# nfs-client-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["list", "watch", "create", "update", "patch"]
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["create", "delete", "get", "list", "watch", "patch", "update"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
```

这里新建了一个名为nfs-client-provisioner的ServiceAccount，然后绑定了一个名为nfs-client-provisioner-runner的ClusterRole，而该ClusterRole声明了一些权限，其中就包括对persistentvolumes的增、删、改、查等权限，所以可以利用该ServiceAccount来自动创建PV。

nfs-client的Deployment声明完成后，创建一个StorageClass对象：

```yaml
# nfs-client-class.yaml 
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: course-nfs-storage
provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME'
```

创建资源：

```shell
[root@k8s-master-node1 ~]# kubectl create -f nfs-client.yaml
deployment.apps/nfs-client-provisioner created
[root@k8s-master-node1 ~]# kubectl create -f nfs-client-sa.yaml
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
[root@k8s-master-node1 ~]# kubectl create -f nfs-client-class.yaml
storageclass.storage.k8s.io/course-nfs-storage created
```

创建完成后查看下资源状态：

```shell
[root@k8s-master-node1 ~]# kubectl get pods
NAME                                      READY   STATUS    RESTARTS   AGE
nfs-client-provisioner-5bddd6fd98-5b6tt   1/1     Running   0          14s
[root@k8s-master-node1 ~]# kubectl get storageclass
NAME                 PROVISIONER      RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSIO               N   AGE
course-nfs-storage   fuseim.pri/ifs   Delete          Immediate           false                                 13s
```

##### （2）新建

前面StorageClass资源对象创建成功了，接下来通过一个示例测试下动态PV，首先创建一个PVC对象：

```yaml
# test-pvc.yaml 
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Mi
```

这里声明了一个PVC对象，采用ReadWriteMany的访问模式，请求1Mi的空间，但是可以看到上面的PVC文件没有标识出任何和StorageClass相关联的信息，如果现在直接创建这个PVC对象是不能够自动绑定上合适的PV的，利用上面创建的StorageClass对象来自动创建一个合适的PV。

在PVC对象中添加一个声明StorageClass对象的标识，利用一个annotations属性来标识，如下：

```yaml
# class-pvc.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
  annotations:
    volume.beta.kubernetes.io/storage-class: "course-nfs-storage"
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Mi
```

创建PVC：

```shell
[root@k8s-master-node1 ~]# kubectl create -f class-pvc.yaml
persistentvolumeclaim/test-pvc created

[root@k8s-master-node1 ~]# kubectl get pvc
NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS         AGE
test-pvc   Bound    pvc-d5156fe9-9060-4876-b291-e59f2de1e1c4   1Mi        RWX            course-nfs-storage   13m
```

(PS:此处test-pvc可能会一直处于Pending状态

```shell
[root@k8s-master-node1 ~]# kubectl get pod
NAME                                      READY   STATUS    RESTARTS   AGE
nfs-client-provisioner-5bddd6fd98-vcxlz   1/1     Running   0          19m
```

此时我们使用`kubectl logs命令查看该pod的日志时会发现如下图所示的报错：

![](https://github.com/Xiao254182/notes/blob/master/img/12/19.png)

这是因为selfLink在1.16版本以后已经弃用，在1.20版本停用。

而由于nfs-provisioner的实现是基于selfLink功能（同时也会影响其他用到selfLink这个功能的第三方软件），需要等nfs-provisioner的制作方重新提供新的解决方案。

目前可用的临时方案是：

修改/etc/kubernetes/manifests/kube-apiserver.yaml文件，找到如下内容后，在最后添加一项参数

```yaml
spec:
  containers:
  - command:
    - kube-apiserver    
    - --advertise-address=192.168.210.20    
    - -- ......　#省略多行内容     
    - --feature-gates=RemoveSelfLink=false　　#添加此行
```

![](https://github.com/Xiao254182/notes/blob/master/img/12/20.png)

然后再删除apiserver的所有pod进行重启

![](https://github.com/Xiao254182/notes/blob/master/img/12/21.png)

```shell
# kubectl delete pod kube-apiserver-k8s-master-node1 -n kube-system
```

等待apiserver被kubelet重启拉起后，再次查询PVC，可以看到PVC状态都为Bound，可以正常被PV绑定了

)

一个名为test-pvc的PVC对象创建成功了，状态已经是Bound。

然后查看PV对象：

```shell
[root@k8s-master-node1 ~]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS         REASON   AGE
pvc-d5156fe9-9060-4876-b291-e59f2de1e1c4   1Mi        RWX            Delete           Bound    default/test-pvc   course-nfs-storage            43s
```

系统自动生成了一个关联的PV对象，访问模式是RWX，回收策略是Delete，这是通过上面的StorageClass对象自动创建的。

##### （3）测试

接下来用一个简单的示例来测试StorageClass 方式声明的 PVC 对象。

编写yaml文件如下：

```yaml
# test-pod.yaml
kind: Pod
apiVersion: v1
metadata:
  name: test-pod
spec:
  containers:
  - name: test-pod
    image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/gcr.io/google-containers/busybox:latest
    imagePullPolicy: IfNotPresent
    command:
    - "/bin/sh"
    args:
    - "-c"
    - "touch /mnt/SUCCESS && exit 0 || exit 1"
    volumeMounts:
    - name: nfs-pvc
      mountPath: "/mnt"
  restartPolicy: "Never"
  volumes:
  - name: nfs-pvc
    persistentVolumeClaim:
      claimName: test-pvc
```

上面这个Pod非常简单，就是用一个busybox容器，在/mnt目录下面新建一个SUCCESS的文件，然后把/mnt目录挂载到上面新建的test-pvc这个资源对象上面，要验证很简单，只需要去查看下nfs服务器上面的共享数据目录下面是否有SUCCESS这个文件即可：

```shell
[root@k8s-master-node1 ~]# kubectl apply -f test-pod.yaml
pod/test-pod created
[root@k8s-master-node1 ~]# kubectl get pods
NAME                                      READY   STATUS      RESTARTS   AGE
test-pod                                  0/1     Completed   0          13s
```

然后在 nfs 服务器的共享数据目录下面查看下数据：

```shell
[root@k8s-master-node1 ~]# ls /data/k8s/
default-test-pvc-pvc-d5156fe9-9060-4876-b291-e59f2de1e1c4
```

可以看到下面有名字很长的文件夹，这个文件夹的命名方式和规则：${namespace}-${pvcName}-${pvName}是一样的，再看下这个文件夹下面是否有其他文件：

```shell
[root@k8s-master-node1 ~]# ls /data/k8s/default-test-pvc-pvc-d5156fe9-9060-4876-b291-e59f2de1e1c4/
SUCCESS
```

可以看到下面有一个 SUCCESS 的文件。

在实际工作中，使用StorageClass更多的是StatefulSet类型的服务，StatefulSet类型的服务也可以通过一个volumeClaimTemplates属性来直接使用StorageClass，如下：

```yaml
# test-statefulset-nfs.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nfs-web
spec:
  serviceName: "nginx"
  replicas: 3
  selector:
    matchLabels:
      app: nfs-web
  template:
    metadata:
      labels:
        app: nfs-web
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/nginx:1.27.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
      annotations:
        volume.beta.kubernetes.io/storage-class: course-nfs-storage
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
```

实际上 volumeClaimTemplates 下面就是一个 PVC 对象的模板，就类似于StatefulSet 下面的template，实际上就是一个Pod的模板，这种方式在StatefulSet类型的服务下面使用得非常多。

直接创建上面的对象：

```shell
[root@k8s-master-node1 ~]# kubectl create -f test-statefulset-nfs.yaml
statefulset.apps/nfs-web created
[root@k8s-master-node1 ~]# kubectl get pods
NAME                                      READY   STATUS      RESTARTS   AGE
nfs-client-provisioner-5bddd6fd98-vcxlz   1/1     Running     0          37m
nfs-web-0                                 1/1     Running     0          32s
nfs-web-1                                 1/1     Running     0          18s
nfs-web-2                                 1/1     Running     0          15s
```

建完成后可以看到上面的3个 Pod 已经运行成功，然后查看下 PVC 对象：

```shell
[root@k8s-master-node1 ~]# kubectl get pvc
NAME            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS         AGE
www-nfs-web-0   Bound    pvc-3517f828-cac0-4885-9371-382250c3043b   1Gi        RWO            course-nfs-storage   40s
www-nfs-web-1   Bound    pvc-4eaeb603-3daf-4692-9a58-35ae36b73c73   1Gi        RWO            course-nfs-storage   26s
www-nfs-web-2   Bound    pvc-9d1f3f86-8339-41e3-9fe4-bb4fa683f534   1Gi        RWO            course-nfs-storage   23s

```

可以看到生成了3个PVC对象，名称由模板名称name加上Pod的名称组合而成，这3个PVC对象也都是绑定状态了，很显然查看PV也可以看到对应的3个PV对象：

```shell
[root@k8s-master-node1 ~]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS         REASON   AGE
pvc-3517f828-cac0-4885-9371-382250c3043b   1Gi        RWO            Delete           Bound    default/www-nfs-web-0   course-nfs-storage            48s
pvc-4eaeb603-3daf-4692-9a58-35ae36b73c73   1Gi        RWO            Delete           Bound    default/www-nfs-web-1   course-nfs-storage            33s
pvc-9d1f3f86-8339-41e3-9fe4-bb4fa683f534   1Gi        RWO            Delete           Bound    default/www-nfs-web-2   course-nfs-storage            31s
```

查看 nfs 服务器上面的共享数据目录：

```shell
[root@k8s-master-node1 ~]# ls /data/k8s/
default-www-nfs-web-0-pvc-3517f828-cac0-4885-9371-382250c3043b
default-www-nfs-web-1-pvc-4eaeb603-3daf-4692-9a58-35ae36b73c73
default-www-nfs-web-2-pvc-9d1f3f86-8339-41e3-9fe4-bb4fa683f534
```

nfs中也对应有3个数据目录，以上就是Kubernetes存储管理

## 10.Health Check在业务生产中滚动更新(rolling update)的应用场景

我们准备一个deployment资源的yaml文件

```yaml
# myapp-v1.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mytest
spec:
  replicas: 10     # 这里准备10个数量的pod
  selector:
    matchLabels:
      app: mytest
  template:
    metadata:
      labels:
        app: mytest
    spec:
      containers:
      - name: mytest
        image: registry.cn-hangzhou.aliyuncs.com/acs/busybox:v1.29.2
        args:
        - /bin/sh
        - -c
        - sleep 10; touch /tmp/healthy; sleep 30000
        readinessProbe:
          exec:
            command:
            - cat
            - /tmp/healthy
          initialDelaySeconds: 10
          periodSeconds: 5
```

运行这个配置，可以看到所有pod已正常运行

```shell
[root@k8s-master-node1 ~]# kubectl apply -f myapp-v1.yaml --record 
deployment.apps/mytest configured
[root@k8s-master-node1 ~]# kubectl get pod
NAME                      READY   STATUS    RESTARTS   AGE
mytest-86f4dd7f67-8lpkn   1/1     Running   0          28s
mytest-86f4dd7f67-b4m92   1/1     Running   0          28s
mytest-86f4dd7f67-dhzmf   1/1     Running   0          28s
mytest-86f4dd7f67-m4bmb   1/1     Running   0          28s
mytest-86f4dd7f67-ncx49   1/1     Running   0          28s
mytest-86f4dd7f67-ng2j7   1/1     Running   0          28s
mytest-86f4dd7f67-pdqk7   1/1     Running   0          28s
mytest-86f4dd7f67-pfxn2   1/1     Running   0          28s
mytest-86f4dd7f67-vcxlz   1/1     Running   0          28s
mytest-86f4dd7f67-x66fm   1/1     Running   0          28s
```

接着我们来准备更新这个服务，并且人为模拟版本故障来进行观察，新准备一个配置myapp-v2.yaml

```yaml
# myapp-v2.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mytest
spec:
  strategy:
    rollingUpdate:
      maxSurge: 35%   # 滚动更新的副本总数最大值(以10的基数为例)：10 + 10 * 35% = 13.5 --> 14
      maxUnavailable: 35%  # 可用副本数最大值(默认值两个都是25%)： 10 - 10 * 35% = 6.5  --> 7
  replicas: 10
  selector:
    matchLabels:
      app: mytest
  template:
    metadata:
      labels:
        app: mytest
    spec:
      containers:
      - name: mytest
        image: registry.cn-hangzhou.aliyuncs.com/acs/busybox:v1.29.2
        args:
        - /bin/sh
        - -c
        - sleep 30000   # 可见这里并没有生成/tmp/healthy这个文件，所以下面的检测必然失败
        readinessProbe:
          exec:
            command:
            - cat
            - /tmp/healthy
          initialDelaySeconds: 10
          periodSeconds: 5
```

运行这个配置，很明显这里因为我们更新的这个v2版本里面不会生成/tmp/healthy文件，那么自动是无法通过Readiness 检测的

```shell
[root@k8s-master-node1 ~]# kubectl apply -f myapp-v2.yaml --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/mytest configured
[root@k8s-master-node1 ~]# kubectl get deployment mytest
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
mytest   7/10    7            7           4m17s
# READY 现在正在运行的只有7个pod
# UP-TO-DATE 表示当前已经完成更新的副本数：即 7 个新副本
# AVAILABLE 表示当前处于 READY 状态的副本数

[root@k8s-master-node1 ~]# kubectl get pod
NAME                      READY   STATUS    RESTARTS   AGE
mytest-86f4dd7f67-8lpkn   1/1     Running   0          5m3s
mytest-86f4dd7f67-dhzmf   1/1     Running   0          5m3s
mytest-86f4dd7f67-m4bmb   1/1     Running   0          5m3s
mytest-86f4dd7f67-ncx49   1/1     Running   0          5m3s
mytest-86f4dd7f67-ng2j7   1/1     Running   0          5m3s
mytest-86f4dd7f67-pfxn2   1/1     Running   0          5m3s
mytest-86f4dd7f67-vcxlz   1/1     Running   0          5m3s
mytest-9b57c5f74-622lb    0/1     Running   0          53s
mytest-9b57c5f74-648vb    0/1     Running   0          53s
mytest-9b57c5f74-b9b72    0/1     Running   0          52s
mytest-9b57c5f74-cjc6t    0/1     Running   0          52s
mytest-9b57c5f74-dbmwc    0/1     Running   0          52s
mytest-9b57c5f74-vgvfv    0/1     Running   0          53s
mytest-9b57c5f74-wsvxb    0/1     Running   0          53s
```

上面可以看到，由于 Readiness 检测一直没通过，所以新版本的pod都是Not ready状态的，这样就保证了错误的业务代码不会被外界请求到

我们也可以通过describe命令查看需要的关键信息

![](https://github.com/Xiao254182/notes/blob/master/img/12/22.png)

综合上面的分析，我们很真实的模拟一次K8s上次错误的代码上线流程，所幸的是这里有Health Check的Readiness检测帮我们屏蔽了有错误的副本，不至于被外面的流量请求到，同时保留了大部分旧版本的pod，因此整个服务的业务并没有因这此更新失败而受到影响。

接下来我们详细分析下滚动更新的原理，为什么上面服务新版本创建的pod数量是7个，同时只销毁了3个旧版本的pod呢？

原因就在于这段配置：

```yaml
# 不显式配置这段的话，默认值均是25%
strategy:
  rollingUpdate:
    maxSurge: 35%
    maxUnavailable: 35%
```

滚动更新通过参数maxSurge和maxUnavailable来控制pod副本数量的更新替换

### 1.maxSurge：

这个参数控制滚动更新过程中pod副本总数超过设定总副本数量的上限。maxSurge可以是具体的整数（比如 3），也可以是百分比，向上取整（默认是25%）

```
在上面测试的例子里面，pod的总副本数量是10，那么在更新过程中，总副本数量的上限大最值计划公式为：
10 + 10 * 35% = 13.5 --> 14
我们查看下更新deployment的描述信息：
Replicas: 10 desired | 7 updated | 14 total | 7 available | 7 unavailable
旧版本available 的数量7个 + 新版本unavailable`的数量7个 = 总数量 14 total
maxSurge 值越大，初始创建的新副本数量就越多
```

### 2.maxUnavailable：

这个参数控制滚动更新过程中不可用的pod副本总数量的值，同样，maxUnavailable 可以是具体的整数（比如 3），也可以是百分百，向下取整（默认是25%）

```
在上面测试的例子里面，pod的总副本数量是10，那么要保证正常可用的pod副本数量为：
10 - 10 * 35% = 6.5 --> 7
我们查看下更新deployment的描述信息：
Replicas: 10 desired | 7 updated | 14 total | 7 available | 7 unavailable
7 available 正常可用的pod数量值就为7
maxUnavailable 值越大，初始销毁的旧副本数量就越多
```

正常更新理想情况下，我们这次版本发布案例滚动更新的过程是：

```
1.首先创建4个新版本的pod，使副本总数量达到14个
2.然后再销毁3个旧版本的pod，使可用的副本数量降为7个
3.当这3个旧版本的pod被 成功销毁后，可再创建3个新版本的pod，使总的副本数量保持为14个
4.当新版本的pod通过Readiness 检测后，会使可用的pod副本数量增加超过7个
5.然后可以继续销毁更多的旧版本的pod，使整体可用的pod数量回到7个
6.随着旧版本的pod销毁，使pod副本总数量低于14个，这样就可以继续创建更多的新版本的pod
7.这个新增销毁流程会持续地进行，最终所有旧版本的pod会被新版本的pod逐渐替换，整个滚动更新完成
```

而我们这里的实际情况是在第4步就卡住了，新版本的pod数量无法能过Readiness 检测，所以按正常的生产处理流程，在获取足够的新版本错误信息提交给开发分析后，我们可以通过kubectl rollout undo 来回滚到上一个正常的服务版本：

先查看下要回滚版本号前面的数字，这里为1

```shell
[root@k8s-master-node1 ~]# kubectl rollout history deployment mytest
deployment.apps/mytest
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=myapp-v1.yaml --record=true
2         kubectl apply --filename=myapp-v2.yaml --record=true
```

此时我们执行回滚操作

```shell
[root@k8s-master-node1 ~]# kubectl rollout undo deployment mytest --to-revision=1
deployment.apps/mytest rolled back
```

再次查询deployment和pod，服务成功回滚到上个版本，所有pod全部正常启动

```shell
[root@k8s-master-node1 ~]# kubectl get deployment mytest
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
mytest   10/10   10           10          28m
[root@k8s-master-node1 ~]# kubectl get pod
NAME                      READY   STATUS    RESTARTS   AGE
mytest-86f4dd7f67-8lpkn   1/1     Running   0          28m
mytest-86f4dd7f67-dhzmf   1/1     Running   0          28m
mytest-86f4dd7f67-m4bmb   1/1     Running   0          28m
mytest-86f4dd7f67-ncx49   1/1     Running   0          28m
mytest-86f4dd7f67-ng2j7   1/1     Running   0          28m
mytest-86f4dd7f67-nvq6c   1/1     Running   0          113s
mytest-86f4dd7f67-pfxn2   1/1     Running   0          28m
mytest-86f4dd7f67-t79q9   1/1     Running   0          113s
mytest-86f4dd7f67-vcxlz   1/1     Running   0          28m
mytest-86f4dd7f67-xzwnc   1/1     Running   0          113s
```

到这里为止，我们模拟了一次有问题的版本发布及回滚，并且可以看到，在这整个过程中，虽然出现了问题，但我们的业务依然是没有受到任何影响的

## 11.Service、Endpoint、Endpoint Slices

k8s中的运行机制能确保所需要运行的服务一直保持所期望的状态，同时，我们不能确保其pod运行的node节点什么时候会当掉，而且pod的IP每次重启都会发生改变，所以我们不应该期望K8s的pod是健壮的，而是要按最坏的打算来假设服务pod中的容器会因为代码有bug、外部或内部因素导致node节点不稳定等等原因发生故障而挂掉，这时候如果我们用的Deployment，那么它的controller会通过动态创建新pod到可用的node上，同时删除旧的pod来保证应用整体的健壮性；并且流量入口这块用一个能固定IP的service来充当抽象的内部负载均衡器，提供pod的访问，所以这里等于就是K8s成为了一个7 x 24小时在线处理服务pod故障的运维机器人

以nginx服务为例，我们首先创建一个nginx的Deployment控制器

```yaml
# nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: new-nginx
  labels:
    app: new-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: new-nginx
  template:
    metadata:
      labels:
        app: new-nginx
    spec:
      containers:
#--------------------------------------------------
      - name: new-nginx
        image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/library/nginx:1.27
        env:
          - name: TZ
            value: Asia/Shanghai
        ports:
        - containerPort: 80
        volumeMounts:
          - name: html-files
            mountPath: "/usr/share/nginx/html"
#--------------------------------------------------
      - name: busybox
        image: registry.cn-shanghai.aliyuncs.com/acs/busybox:v1.29.2
        args:
        - /bin/sh
        - -c
        - >
           while :; do
             if [ -f /html/index.html ];then
               echo "[$(date +%F\ %T)] ${MY_POD_NAMESPACE}-${MY_POD_NAME}-${MY_POD_IP}" > /html/index.html
               sleep 1
             else
               touch /html/index.html
             fi
           done
        env:
          - name: TZ
            value: Asia/Shanghai
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
        volumeMounts:
          - name: html-files
            mountPath: "/html"
          - mountPath: /etc/localtime
            name: tz-config
#--------------------------------------------------
      volumes:
        - name: html-files
          emptyDir:
            medium: Memory
            sizeLimit: 10Mi
        - name: tz-config
          hostPath:
            path: /usr/share/zoneinfo/Asia/Shanghai
            
# kubectl apply -f nginx.yaml
deployment.apps/new-nginx created

# kubectl get pods
NAME                       READY   STATUS    RESTARTS   AGE
new-nginx-db6899df-d5nl8   2/2     Running   0          56s
new-nginx-db6899df-m5tv8   2/2     Running   0          56s
```

创建一个service服务来提供固定IP轮巡访问上面创建的nginx服务的2个pod（nodeport）

```shell
# 给这个nginx的deployment生成一个service（简称svc）
# kubectl expose deployment new-nginx --port=80 --target-port=80
service/new-nginx exposed

# kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   172.17.0.1      <none>        443/TCP   2d
new-nginx    ClusterIP   172.17.73.244   <none>        80/TCP    6s

# 看下自动关联生成的endpoint
# kubectl  get endpoints
NAME         ENDPOINTS                                                     AGE
kubernetes   192.168.100.31:6443,192.168.100.32:6443,192.168.100.33:6443   2d
new-nginx    192.168.12.65:80,192.168.5.131:80                             5m38s

# 看下自动关联生成的endpointslices
# kubectl  get endpointslices.discovery.k8s.io
NAME             ADDRESSTYPE  PORTS  ENDPOINTS                                      AGE
kubernetes       IPv4         6443   192.168.100.31,192.168.100.32,192.168.100.33  2d
new-nginx-6zgxd  IPv4         80     192.168.12.65,192.168.5.131                   6m32s
```

（PS：在 Kubernetes 网络系统中，Endpoint 和 Endpoint Slices 都是非常重要的资源。Endpoint 和 Endpoint Slices 都负责追踪网络端点，但 Endpoint Slices 是 Kubernetes 1.16 之后引入的，用于替代 Endpoint 的新资源

那么，什么是Endpoint Slices呢？

Endpoint Slices是 Kubernetes 中的 API 对象，用于表示集群中的网络端点。与其前身 Endpoint 相比，Endpoint Slices 提供了一个更加可扩展和可扩展的方式来追踪网络端点

Endpoint Slices 的主要优势包括：

| 可扩展性         | Endpoint Slices 比 Endpoints 更具可扩展性。在大型集群中，使用 Endpoint 可能会导致网络性能问题，因为每次服务的后端更改时，都会更新所有的 Endpoints。但是，Endpoint Slices 分散了这些信息，使得网络流量可以在多个 Endpoint Slices 中分布，从而提高了可扩展性。 |
| ---------------- | ------------------------------------------------------------ |
| **内置拓扑信息** | **Endpoint Slices 支持内置的拓扑信息，如 nodeName、zone、region 等，这些信息可以用于实现更复杂的网络路由，以优化网络流量** |
| **多地址类型**   | **Endpoint Slices 支持多种地址类型，包括 IPv4、IPv6 和 FQDN（完全限定域名）** |

Endpoint Slices 的主要构成部分包括：

| **地址** | 这是 Endpoint Slices 中的主要字段，用于存储网络端点的地址    |
| -------- | ------------------------------------------------------------ |
| **端口** | **Endpoint Slices 可以包含多个端口，每个端点可以关联一个或多个端口** |
| **条件** | **Endpoint Slices 包含关于端点的一些条件信息，例如端点是否就绪** |
| **拓扑** | **这些字段用于表示端点的拓扑信息，例如 nodeName、zone、region 等** |

在 Kubernetes 中，Endpoint Slices 控制器默认会为每个服务创建和管理 Endpoint Slices。当创建新的 Kubernetes 服务时，相应的 Endpoint Slices 也会被创建，当服务的后端发生更改时，Endpoint Slices 也会相应地更新

```yaml
apiVersion: discovery.k8s.io/v1beta1
kind: EndpointSlice
metadata:
  name: example-abc
  labels:
    kubernetes.io/service-name: example
addressType: IPv4
ports:
  - name: http
    protocol: TCP
    port: 80
endpoints:
  - addresses:
      - "10.1.2.3"
    conditions:
      ready: true
    nodeName: node-1
    zone: us-central1-a
```

以上是一个 EndpointSlices 的简单例子。在此 YAML 文件中，我们定义了一个名为 “example-abc” 的 EndpointSlice，它追踪名为 “example” 的服务的网络端点。这个 EndpointSlice 包含一个端点，地址为 “10.1.2.3”，端口为 80，位于节点 “node-1” 和区域 “us-central1-a”。

总的来说，Endpoint Slices 是 Kubernetes 网络系统的重要组成部分，它提供了一种高效且可扩展的方式来追踪网络端点，使得 Kubernetes 网络可以更好地扩展和优化。）

```shell
# 接下来测试下svc的负载均衡效果吧
[root@k8s-master-1 ~]# while sleep 1 ; do curl 172.17.73.244 ; done
[2024-08-03 03:17:48] default-new-nginx-db6899df-d5nl8-192.168.12.65
[2024-08-03 03:17:49] default-new-nginx-db6899df-m5tv8-192.168.5.131
[2024-08-03 03:17:50] default-new-nginx-db6899df-d5nl8-192.168.12.65
[2024-08-03 03:17:51] default-new-nginx-db6899df-m5tv8-192.168.5.131
[2024-08-03 03:17:52] default-new-nginx-db6899df-m5tv8-192.168.5.131
[2024-08-03 03:17:53] default-new-nginx-db6899df-d5nl8-192.168.12.65
```

当然，此时我们的nginx服务是只能在我们的k8s集群内部访问的，因为我们nginx的服务类型为ClusterIP，我们可以通过`kubectl edit`命令来修改svc的配置文件，将"ClusterIP"修改为“NodePort”来提供外部访问，也可以直接使用命令修改

```shell
# 使用命令直接修改svc的类型来提供外部访问
# kubectl patch svc new-nginx -p '{"spec":{"type":"NodePort"}}'
service/new-nginx patched
```

此时我们再次查看svc服务，发现new-nginx svc服务的80端口已经映射为宿主机的30788的端口，我们可以直接通过访问k8s集群中的任意节点的IP+30788端口来访问容器内的nginx服务

```shell
# kubectl  get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   172.17.0.1      <none>        443/TCP        2d
new-nginx    NodePort    172.17.73.244   <none>        80:30788/TCP   114s

[root@k8s-master-1 ~]# kubectl get node -o wide
NAME           STATUS   ROLES                  AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                 CONTAINER-RUNTIME
k8s-master-1   Ready    control-plane,master   2d    v1.20.0   192.168.100.31   <none>        CentOS Linux 7 (Core)   3.10.0-1160.119.1.el7.x86_64   docker://20.10.14
k8s-master-2   Ready    control-plane,master   2d    v1.20.0   192.168.100.32   <none>        CentOS Linux 7 (Core)   3.10.0-1160.119.1.el7.x86_64   docker://20.10.14
k8s-master-3   Ready    control-plane,master   2d    v1.20.0   192.168.100.33   <none>        CentOS Linux 7 (Core)   3.10.0-1160.119.1.el7.x86_64   docker://20.10.14
k8s-node-1     Ready    <none>                 2d    v1.20.0   192.168.100.34   <none>        CentOS Linux 7 (Core)   3.10.0-1160.119.1.el7.x86_64   docker://20.10.14
k8s-node-2     Ready    <none>                 2d    v1.20.0   192.168.100.35   <none>        CentOS Linux 7 (Core)   3.10.0-1160.119.1.el7.x86_64   docker://20.10.14
[root@k8s-master-1 ~]# curl 192.168.100.31:30788
[2024-08-03 03:28:05] default-new-nginx-db6899df-m5tv8-192.168.5.131
[root@k8s-master-1 ~]# curl 192.168.100.32:30788
[2024-08-03 03:28:08] default-new-nginx-db6899df-m5tv8-192.168.5.131
[root@k8s-master-1 ~]# curl 192.168.100.33:30788
[2024-08-03 03:28:10] default-new-nginx-db6899df-d5nl8-192.168.12.65
[root@k8s-master-1 ~]# curl 192.168.100.34:30788
[2024-08-03 03:28:13] default-new-nginx-db6899df-m5tv8-192.168.5.131
[root@k8s-master-1 ~]# curl 192.168.100.35:30788
[2024-08-03 03:28:15] default-new-nginx-db6899df-d5nl8-192.168.12.65
```

我们这里也来分析下这个svc的yaml配置

```yaml
# cat svc.yaml
apiVersion: v1       # <<<<<<  v1是Service的apiVersion
kind: Service        # <<<<<<  指明当前资源的类型为Service
metadata:
  creationTimestamp: null
  labels:
    app: nginx
  name: nginx       # <<<<<<  Service的名字为nginx
spec:
  ports:
  - port: 80        # <<<<<<  将Service的80端口映射到Pod的80端口，使用TCP协议
    protocol: TCP
    targetPort: 80
      selector:
    app: nginx     # <<<<<<  selector指明挑选那些label为run:nginx的Pod作为Service的后端
    status:
      loadBalancer: {}
```

我们来看下这个nginx的svc描述

```yaml
# kubectl describe svc nginx 
Name:                     nginx
Namespace:                default
Labels:                   app=nginx
Annotations:              <none>
Selector:                 app=nginx
Type:                     NodePort
IP:                       10.68.18.121
Port:                     <unset>  80/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  20651/TCP
Endpoints:                172.20.139.72:80,172.20.217.72:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
```

我们可以看到在Endpoints列出了2个pod的IP和端口，pod的ip是在容器中配置的，那么这里Service cluster IP又是在哪里配置的呢？cluster ip又是自律映射到pod ip上的呢？

看下本地网卡，会有一个ipvs的虚拟网卡

```shell
# ip addr
······
9: kube-ipvs0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default
    link/ether 9a:5d:d5:31:fc:10 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.10/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 172.17.183.211/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever 
    inet 172.17.73.244/32 scope global kube-ipvs0    # <-------- SVC的IP配置在这里
       valid_lft forever preferred_lft forever
    inet 172.17.149.167/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 172.17.0.1/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
······
```

来看下lvs的虚拟服务器列表

```shell
# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
······        
TCP  172.17.73.244:80 rr          #<-----------   SVC转发Pod的明细在这里
  -> 192.168.5.131:80             Masq    1      0          0
  -> 192.168.12.65:80             Masq    1      0          0    
······
```

除了直接用cluster ip，以及上面说到的NodePort模式来访问Service，我们还可以用K8s的DNS来访问

```shell
# kubectl -n kube-system get deployment,pod|grep dns
deployment.apps/coredns                   2/2     2            2           2d5h
pod/coredns-54d67798b7-4kbfn                  1/1     Running   1          2d5h
pod/coredns-54d67798b7-vsb8d                  1/1     Running   1          2d5h

# coredns是一个DNS服务器，每当有新的Service被创建的时候，coredns就会添加该Service的DNS记录，然后我们通过serviceName.namespaceName就可以来访问到对应的pod了，下面来演示下：
```

我们起一个临时pod（--rm代表等我退出这个pod后，它会被自动删除，当作一个临时pod在用）

```shell
[root@k8s-master-1 ~]# kubectl run -it --rm alpine --image=swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/alpine:3.20.1 -- sh
If you don't see a command prompt, try pressing enter.
/ # wget new-nginx.default
Connecting to new-nginx.default (172.17.73.244:80)
saving to 'index.html'
index.html           100% |*******************************************************************|    69  0:00:00 ETA
'index.html' saved
/ # cat index.html
[2024-08-03 08:15:25] default-new-nginx-db6899df-m5tv8-192.168.5.131
```

### service生产小技巧 通过svc来访问非K8s上的服务

上面我们提到了创建service后，会自动创建对应的endpoint，这里面的关键在于 selector: app: nginx 基于lables标签选择了一组存在这个标签的pod，然而在我们创建svc时，如果没有定义这个selector，那么系统是不会自动创建endpoint的，我们可不可以手动来创建这个endpoint呢？答案是可以的，在生产中，我们可以通过创建不带selector的Service，然后创建同样名称的endpoint，来关联K8s集群以外的服务，这样可以直接复用K8s上的ingress，来访问K8s集群以外的服务，省去了自己搭建前面Nginx代理服务器的麻烦

我们在k8s集群以外的节点上用python运行一个简易web服务器

```shell
[root@k8s-vip ~]# python2 -m SimpleHTTPServer 9999
Serving HTTP on 0.0.0.0 port 9999 ...
```

然后在k8s集群中配置svc服务

```yaml
# mysvc.yaml
apiVersion: v1
kind: Service
metadata:
  name: mysvc
  namespace: default
spec:
  type: ClusterIP
  ports:
  - port: 80
    protocol: TCP
---
apiVersion: v1
kind: Endpoints
metadata:
  name: mysvc
  namespace: default
subsets:
- addresses:
  - ip: 192.168.100.41    # 集群以外的服务器地址
    nodeName: serving-http
  ports:
  - port: 9999
    protocol: TCP
```

开始创建并测试

```shell 
[root@k8s-master-1 ~]# kubectl  apply -f mysvc.yaml
service/mysvc created
endpoints/mysvc created

[root@k8s-master-1 ~]# kubectl get svc,endpoints |grep mysvc
service/mysvc        ClusterIP   172.17.149.167   <none>        80/TCP         2m53s
endpoints/mysvc        192.168.100.41:9999 
25s

[root@k8s-master-1 ~]# curl 172.17.149.167
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN"><html>
<title>Directory listing for /</title>
<body>
<h2>Directory listing for /</h2>
<hr>
<ul>
<li><a href=".bash_history">.bash_history</a>
<li><a href=".bash_logout">.bash_logout</a>
<li><a href=".bash_profile">.bash_profile</a>
<li><a href=".bashrc">.bashrc</a>
<li><a href=".cshrc">.cshrc</a>
<li><a href=".pki/">.pki/</a>
<li><a href=".tcshrc">.tcshrc</a>
<li><a href=".viminfo">.viminfo</a>
<li><a href="anaconda-ks.cfg">anaconda-ks.cfg</a>
</ul>
<hr>
</body>
</html>

# 我们回到部署了简易web服务器的节点上，可以看到有一条刚才的访问日志打印出来了
[root@k8s-vip ~]# python2 -m SimpleHTTPServer 9999
Serving HTTP on 0.0.0.0 port 9999 ...
192.168.100.31 - - [03/Aug/2024 16:41:22] "GET / HTTP/1.1" 200 -
```

### 外部网络如何访问到Service呢？

```yaml
# 我们看下先创建的nginx service的yaml配置
# kubectl get svc new-nginx -o yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2024-08-03T03:00:32Z"
  labels:
    app: new-nginx
  managedFields:                #在新版的K8s运行的资源配置里面，会输出这么多的配置信息，这里我们可以不用管它，实际我们在创建时，这些都是忽略的
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:labels:
          .: {}
          f:app: {}
      f:spec:
        f:ports:
          .: {}
          k:{"port":80,"protocol":"TCP"}:
            .: {}
            f:port: {}
            f:protocol: {}
            f:targetPort: {}
        f:selector:
          .: {}
          f:app: {}
        f:sessionAffinity: {}
    manager: kubectl-expose
    operation: Update
    time: "2024-08-03T03:00:32Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:spec:
        f:externalTrafficPolicy: {}
        f:type: {}
    manager: kubectl-patch
    operation: Update
    time: "2024-08-03T03:02:19Z"
  name: new-nginx
  namespace: default
  resourceVersion: "33053"
  uid: fc1985ea-bd10-465a-aee2-2e88f7e13a25
spec:
  clusterIP: 172.17.73.244
  clusterIPs:
  - 172.17.73.244
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 30788      # 我们看下这里，它定义的一个nodePort配置，并分配了30788端口，因为我们先前创建时并没有指定这个配置，所以它是随机生成的
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: new-nginx
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
```

NodePort端口会在所在K8s的node节点上都生成一个同样的端口，这就使我们无论所以哪个node的ip接端口都能方便的访问到Service了，但在实际生产中，这个NodePort不建议经常使用，因为它会造成node上端口管理混乱

```shell
# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  127.0.0.1:30788 rr
  -> 192.168.5.131:80             Masq    1      0          0
  -> 192.168.12.65:80             Masq    1      0          0
······
TCP  172.17.0.1:30788 rr
  -> 192.168.5.131:80             Masq    1      0          0
  -> 192.168.12.65:80             Masq    1      0          0
······
TCP  192.168.27.128:30788 rr
  -> 192.168.5.131:80             Masq    1      0          0
  -> 192.168.12.65:80             Masq    1      0          0
TCP  192.168.100.30:30788 rr
  -> 192.168.5.131:80             Masq    1      0          0
  -> 192.168.12.65:80             Masq    1      0          0
TCP  192.168.100.31:30788 rr
  -> 192.168.5.131:80             Masq    1      0          0
  -> 192.168.12.65:80             Masq    1      0          0
······
```

### 生产中Service的调优

| 节点         | IP             |
| ------------ | -------------- |
| k8s-master-1 | 192.168.100.31 |
| k8s-master-2 | 192.168.100.32 |
| k8s-master-3 | 192.168.100.33 |
| k8s-node-1   | 192.168.100.34 |
| k8s-node-2   | 192.168.100.35 |

先把nginx的pod数量调整为1，方便待会观察

```
# kubectl scale deployment new-nginx --replicas=1
```

看下这个nginx的pod运行情况，-o wide显示更详细的信息，这里可以看到这个pod运行在k8s-node-2上面

```shell
[root@k8s-master-1 ~]# kubectl  get pod -o wide
NAME                       READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
new-nginx-db6899df-d5nl8   2/2     Running   0          7h11m   192.168.12.65   k8s-node-2   <none>           <none>
```

我们先直接通过pod运行的node的IP来访问测试

```shell
[root@k8s-master-1 ~]# curl k8s-node-2:30788
[2024-08-03 10:10:41] default-new-nginx-db6899df-d5nl8-192.168.12.65
```

我们查看nginx的日志，可以看到日志显示这条请求的来源IP是192.168.100.35，而不是k8s-master-1的IP 192.168.100.31

```shell
[root@k8s-master-1 ~]# kubectl logs --tail=1 new-nginx-db6899df-d5nl8 -c new-nginx
192.168.100.35 - - [03/Aug/2024:18:10:41 +0800] "GET / HTTP/1.1" 200 69 "-" "curl/7.29.0" "-"
```

我们再来通过k8s-master-1的IP 192.168.100.31来访问，可以看到显示的来源IP非node节点的 

```shell
[root@k8s-master-1 ~]# curl k8s-master-1:30788
[2024-08-03 10:16:21] default-new-nginx-db6899df-d5nl8-192.168.12.65
[root@k8s-master-1 ~]# kubectl logs --tail=1 new-nginx-db6899df-d5nl8 -c new-nginx
192.168.27.128 - - [03/Aug/2024:18:16:21 +0800] "GET / HTTP/1.1" 200 69 "-" "curl/7.29.0" "-"
```

其实这就是一个虚拟网卡转发的

```shell
[root@k8s-master-1 ~]# ip a|grep -wC1 192.168.27.128
    link/ipip 0.0.0.0 brd 0.0.0.0
    inet 192.168.27.128/32 scope global tunl0
       valid_lft forever preferred_lft forever
```

可以看下lvs的虚拟服务器列表，正好是转到我们要访问的pod上的

```shell
[root@k8s-master-1 ~]# ipvsadm -ln|grep -A1 192.168.27.128
TCP  192.168.27.128:30788 rr
  -> 192.168.12.65:80             Masq    1      0          0
```

从中我们可以了解到详细处理流程：

```
* 客户端发送数据包 192.168.100.31:30788
* 192.168.100.31 用自己的IP地址替换数据包中的源IP地址（SNAT）
* 192.168.100.31 使用 pod IP 替换数据包上的目标 IP
* 数据包路由到 192.168.100.35 ，然后路由到 endpoint 
* pod的回复被路由回 192.168.100.31
* pod的回复被发送回客户端
                                client
                                  | ^
                                  | |
                                  v |
|----------------| <------ |----------------|
| 192.168.100.35 |   SNAT  | 192.168.100.31 |
|----------------| ------> |----------------|
    | ^
    | |
    v |
 endpoint 
```

为避免这种情况， Kubernetes 具有保留客户端IP 的功能。设置 service.spec.externalTrafficPolicy 为 Local 会将请求代理到本地端点，不将流量转发到其他节点，从而保留原始IP地址。如果没有本地端点，则丢弃发送到节点的数据包，因此您可以在任何数据包处理规则中依赖正确的客户端IP。

```shell
[root@k8s-master-1 ~]# kubectl patch svc new-nginx -p '{"spec":{"externalTrafficPolicy":"Local"}}'
service/new-nginx patched
```

现在通过非pod所在node节点的IP来访问是不通了，通过所在node的IP发起请求正常

```shell
[root@k8s-master-1 ~]# curl k8s-master-1:30788
curl: (7) Failed connect to k8s-master-1:30788; 拒绝连接
[root@k8s-master-1 ~]# curl k8s-node-2:30788
[2024-08-03 10:35:51] default-new-nginx-db6899df-d5nl8-192.168.12.65
```

可以看到日志显示的来源IP就是k8s-master-1的IP地址，这才是我们想要的结果

```shell
[root@k8s-master-1 ~]# kubectl logs --tail=1 new-nginx-db6899df-d5nl8 -c new-nginx
192.168.100.31 - - [03/Aug/2024:18:35:51 +0800] "GET / HTTP/1.1" 200 69 "-" "curl/7.29.0" "-"
```

不过这样会带来个什么问题呢，如果一旦pod发生重启飘移到了另一台node节点上，而你用的IP还是k8s-node-2的话就会访问不到服务了，解决方法可以用`nodeSelector`来将服务的pod固定在哪几台node上运行，这样ip还是在我们控制的范围了

去掉这个优化配置也很简单

```shell
# kubectl patch svc nginx -p '{"spec":{"externalTrafficPolicy":""}}' 
```

